[
  {
    "slug": "adversarial_interoperability",
    "data": {
      "title": "Notes on Adversarial Interoperability",
      "description": "Summarizing thoughts from Seth Godin and Cory Doctorow on Interoperability",
      "tag_list": [
        "reflections",
        "tech",
        "strategy"
      ]
    },
    "content": "\nI just caught up on Seth Godin's podcast on [Adversarial Interoperability](https://play.acast.com/s/akimbo/adversarialinteroperability), and it reminded me of a [great YC podcast episode with Cory Doctorow](https://www.ycombinator.com/library/6c-cory-doctorow-on-adversarial-interoperability) on the same topic. Cory himself has written reams of blogposts on [how Adversarial Interoperability has been applied repeatedly over tech history](https://www.eff.org/deeplinks/2019/10/adversarial-interoperability). \n\nThe way to frame this is that there are two types of actors: \n\n- a (bigger) **core platform**, sometimes controlled by a single company, or sometimes a public commons\n- a group of (smaller) **third parties**, which want to connect to or use the core platform for their own benefit and the benefit of users\n\nBased on these basic desires, we have three types of interop:\n\n- **Cooperative Interoperability** is when the platform explicitly works with third parties to enable them, mostly because the platform sees the benefits of adding capability without paying to develop it (in fact, it could be their primary revenue stream, as with Stripe or anything else in [the API economy](https://a16z.com/tag/the-api-economy/)). We may be familiar with these as an \"API marketplace\" or \"plugin ecosystem\", but also this happens on a \"spec\" level with major languages, protocols, and file formats.\n- **Adversarial Interoperability** is when the platform does NOT explicitly want third parties to connect for whatever reason, whether it is about control of data or of revenues or of content available on a platform, BUT, crucially, third parties are **still able to interop with it anyway**. [Plaid, the fintech unicorn](https://qz.com/1784765/the-seeds-of-visas-5-3-billion-acquisition-of-plaid-were-planted-more-than-a-year-ago/) still mostly operates via [screen-scraping](https://searchdatacenter.techtarget.com/definition/screen-scraping), as do [Mint and Quicken](https://www.scrapehero.com/mint-com-and-quicken-data-scraping-blocked-by-bank-websites/).\n- **Indifferent Interoperability** is when the platform doesn't care, e.g. a fridge maker doesn't care what magnets you put on the door. Usually platforms here claim they are \"unopinionated\", but I don't think this attitude lasts forever.\n\nThe star of the show is of course Adversarial Interoperability. It means that if users are not being served well, someone else (often a technically proficient user themselves) can come along and make some sort of hack to fix that problem (analogous to the [Right to Repair](https://thecounter.org/right-to-repair-elizabeth-warren-john-deere/)). Open Source is itself an inherent promise that adversarial interoperability will always be possible - anyone can fork a project if their priorities diverge from the maintainers.\n\nYou can make a lot of money via Adversarial Interoperability - and this has at least some legal protection thanks to [hiQ vs LinkedIn](https://www.eff.org/deeplinks/2019/09/victory-ruling-hiq-v-linkedin-protects-scraping-public-data), a landmark court ruling that even made it *illegal* for platforms to *try* to prevent screen scrapers from getting already-public data. In fact, one could argue that every new social network should be bootstrapped off existing ones, since it's legal anyway.\n\nIn tech, progress is fastest with cooperative interop, as all interests are aligned. But progress is also *possible* with indifferent and adversarial interop. \n\nThe problem arises when technology **prevents** adversarial interop, which helps monopolistic platforms keep a stranglehold on their users. This is most prevalent in closed ecosystems like with Apple's iOS.\n\nI think this is an incredibly important idea in [tech strategy](https://www.swyx.io/writing/dev-guide-to-tech-strategy/) - I've forgotten it before, so I wanted to take this occasion to note it down."
  },
  {
    "slug": "namespacing-sites",
    "data": {
      "title": "Against Namespacing Personal Sites",
      "description": "I care a lot about creating Cool URIs so I have been paralyzed more than I should be about what I'm calling \"URL Architecture\". In true fashion, I'm blogging about it.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nI'm about to rewrite my personal site (for hopefully the last time for a long while). I have been paralyzed more than I should be about what I'm calling \"URL Architecture\". Because I care about [Cool URIs](https://www.w3.org/Provider/Style/URI), this is ideally not a reversible decision.\n\nAfter some agonizing I think I have decided on a URL Architecture I like. In true fashion, I'm blogging about it.\n\n\"URL Architecture\" (I don't know the proper word for this, if there is one pls correct me) is how you set up your URLs for each page of your site. For example, `netlify.com` has these:\n\n- `netlify.com`: generic landing page\n- `netlify.com/blog`: index of blogs\n- `netlify.com/blog/year/month/day/slug`: individual posts\n- other pages\n\n\nI call that `/blog` a **namespace**. The reason you do this is simple: it GREATLY reduces the risk of clashing with any other page you could possibly put on your site. It is \"future proof\".\n\n> Note: There is some debate about whether or not putting year/month/day in the URL impacts SEO. Some think it doesn't matter, some think it's a mild negative, some like it because you avoid clashes at the *slug* level.\n\nPersonal sites of frequent speakers often opt for TWO namespaces: `/writing` and `/speaking`. My current site (as of Sept 2020) uses this, but I have grown to dislike it: I will often speak about what I write, and write about related things I speak about. Why separate them? It is more effort to make my talks discoverable to my readers and vice versa.\n\nThe other thing that bothers me about my current setup is that it makes for ugly URLs. [My most popular essay ever](https://www.swyx.io/writing/learn-in-public) regularly gets linked to in Slacks, Discords, YouTube chats, Tweets and presentation slides. It weighs in at 43 characters all told.\n\nIf I shortened it to the shortcode that I set up for myself, `swyx.io/LIP`, it is only 11 characters. This looks much better.\n\nI've noticed that a lot of prolific bloggers don't namespace:\n\n- CSS Tricks: https://css-tricks.com/how-css-perspective-works/\n- Derek Sivers: https://sive.rs/cons\n- Paul Graham: http://www.paulgraham.com/good.html\n\nThese people have blogged for *decades* and somehow managed to find their way around possible URL clashes. Why can't I?"
  },
  {
    "slug": "svelte_tailwind_setup",
    "data": {
      "title": "How to Set Up Svelte with Tailwind CSS",
      "description": "A quick reference for myself on how to set up Svelte with Tailwind CSS",
      "tag_list": [
        "svelte",
        "tailwindcss",
        "tech"
      ]
    },
    "content": "\n> IMPORTANT UPDATE: Chris has shown a much easier way to do this than I originally outlined, so I have replaced my original notes with notes from [his sample repo](https://github.com/chrisdhanaraj/test-svelte-tailwind)\n\nOn [the latest Toolsday](https://spec.fm/podcasts/toolsday/o2XubpwD), [Chris Dhanaraj](https://twitter.com/chrisdhanaraj) said he had trouble finding documentation for adding [Tailwind](https://tailwindcss.com/) to [Svelte](https://svelte.dev/). \n\nToday I also needed to add Tailwind to a Svelte project, so I am writing this as a reference for myself. Setting up PostCSS with Svelte is something I have documented on [the new Svelte Society site](https://sveltesociety.dev/recipes/build-setup/using-postcss-with-svelte), but of course it could be better and more specifically tailored to Tailwind (which after all is \"just\" a PostCSS plugin).\n\nSo I am writing this for him and for me.\n\n> A quick aside on **WHY Use Tailwind with Svelte**, since Svelte offers scoped CSS by default: Tailwind offers a nicely constrained \"design system\" so you don't overuse [Magic Numbers](https://css-tricks.com/magic-numbers-in-css) and it's easy to add responsive styling with [Tailwind breakpoints](https://tailwindcss.com/docs/breakpoints). Because Tailwind has the developer experience of \"inline styles\", I also find it easier to delete and move HTML around without having to go back for the styling. I also like [not having to name classes](https://www.swyx.io/writing/how-to-name-things/#not-naming-things-aug-2019-edit).\n\n## 3 Steps\n\n I will assume you have a standard existing Svelte or Sapper project with no PostCSS/Tailwind set up. I'll also add in `autoprefixer` and [`postcss-nesting`](https://tailwindcss.com/docs/using-with-preprocessors#nesting) since I like to work with those, but of course feel free to remove as needed.\n\n### Step 1: Install deps\n\n```bash\nnpm install -D svelte-preprocess tailwindcss autoprefixer postcss-nesting\n```\n\n\n### Step 2: Setup Config Files\n\nAdd a `tailwind.config.js` file at the project root:\n\n```js\n// tailwind.config.js\nconst production = !process.env.ROLLUP_WATCH; // or some other env var like NODE_ENV\nmodule.exports = {\n  future: { // for tailwind 2.0 compat\n    purgeLayersByDefault: true, \n    removeDeprecatedGapUtilities: true,\n  },\n  purge: {\n    content: [\n      \"./src/**/*.svelte\",\n      // may also want to include base index.html\n    ], \n    enabled: production // disable purge in dev\n  },\n};\n```\n\nAnd now set it up inside of your Svelte bundler config as well:\n\n```js\nimport sveltePreprocess from \"svelte-preprocess\";\n\nconst production = !process.env.ROLLUP_WATCH;\n\nexport default {\n  plugins: [\n    svelte({\n      // etc...\n      preprocess: sveltePreprocess({\n        // https://github.com/kaisermann/svelte-preprocess/#user-content-options\n        sourceMap: !production,\n        postcss: {\n          plugins: [\n             require(\"tailwindcss\"), \n             require(\"autoprefixer\"),\n             require(\"postcss-nesting\")\n          ],\n        },\n      }),\n    }),\n  ]\n}\n```\n\n\n### Step 3: Add the Tailwind includes to your Svelte App\n\nTypically a Svelte app will have a way to inject css already, so all we do is piggyback onto that. You'll want to put these includes at a reasonably high level, say `App.svelte` or `Layout.svelte` component that will be included in every page of your site.\n\n```html\n<style global lang=\"postcss\">\n\n  /* only apply purgecss on utilities, per Tailwind docs */\n  /* purgecss start ignore */\n  @tailwind base;\n  @tailwind components;\n  /* purgecss end ignore */\n \n  @tailwind utilities;\n\n</style>\n```\n\nAnd that's it!\n\n> Note: this section used to involve messing with `package.json` scripts to run `postcss-cli`, but Chris realized that you didn't need to do any of this since Svelte already has a way to inject CSS and `svelte-preprocess` already runs on every Svelte file.\n\nPlease see:\n\n- [Chris' sample Svelte repo](https://github.com/chrisdhanaraj/test-svelte-tailwind)\n- [Chris' Sapper side project](https://github.com/chrisdhanaraj/tier-list/)\n\nTo see this working in action.\n\n## Alternative Approaches\n\nThis method outlined above is simple to get running, but does end up running thousands of lines of Tailwind's CSS through the Svelte compiler. This may cause performance issues (primarily, every time you change the entry point file). Alternative approaches may be more appropriate depending on your preferences:\n\n- Jacob Babich: \"I'm moving to https://github.com/babichjacob/sapper-postcss-template/tree/parallel-global-css running the global css builder in parallel with a reimplementation of postcss-cli (just so I can have source maps controlled by a variable in rollup.config.js) but without getting that extreme you can just use [npm-run-all](http://npm.im/npm-run-all) with [postcss-cli](https://www.npmjs.com/package/postcss-cli)\"\n- dominikg: \"The easiest way to setup tailwind with svelte: `npx svite create -t postcss-tailwind my-svelte-tailwind-project`\"\n- https://github.com/sarioglu/sapper-tailwindcss-template\n- https://github.com/sarioglu/svelte-tailwindcss-template\n- https://codechips.me/sapper-with-postcss-and-tailwind/"
  },
  {
    "slug": "rob_hope_hot_tips",
    "data": {
      "title": "If You Sell Anything Online, This Book Will Make You Money",
      "description": "My glowing review of Rob Hope's Landing Page Hot Tips Book",
      "tag_list": [
        "reviews"
      ]
    },
    "content": "\n## The Why\n\nMakers love spending all their time making their core product, and not enough time thinking about how to market and sell. \"*Build it and they will come*\", right?\n\n**Wrong.**\n\nThe average landing page conversion rate is **2.35%**, meaning **over 97% of people never even see what you made**.\n\nWe've all been warned not to judge a book by it's cover, but let's face it - we all do it.\n\nOne of my biggest stress points in [launching my own book](https://www.swyx.io/writing/coding-career-launch/) was leaving the landing page to the final week before launch, and realizing that I was totally out of my depth. After a certain point, one additional hour spent on the nth proofreading of the book was nowhere near as valuable to me as spending it on *the landing page*. \n\n[A top quartile landing page converts 5.31%, and the top decile converts 11.45%.](https://www.wordstream.com/blog/ws/2014/03/17/what-is-a-good-conversion-rate) That means **you can 2x-5x your sales** just working on your landing page rather than your product!!! \n\n> And that's assuming you can even make an average-performing landing page, especially on your first try!\n\nSo hopefully you can see why I think landing pages are important. If you're thinking \"but I'm a developer, this isn't my job!\", I'd say you're *almost* right. You took on this job the day you decided to sell something - your book, your business, your app, your *self* - online.\n\nYou're also right in that almost nobody - not even many marketers I have worked with - specializes in landing pages. It's *almost* nobody's job.\n\nIt is Rob Hope's.\n\n## The Book\n\n[![https://onepagelove.com/wp-content/themes/onepagelove/frontend/img/pages/hottips/cover.png](https://onepagelove.com/wp-content/themes/onepagelove/frontend/img/pages/hottips/cover.png)](https://onepagelove.com/go/swyx)\n\nFor over 12 years, Rob has [made a career](https://robhope.com/) out of studying one page websites. One Page sites embrace their titular constraint - within one page, they must drive the visitor to purchase or sign up or whatever action it is designed to do.\n\nI've just finished going through Rob Hope's new [Landing Page Hot Tips](https://onepagelove.com/go/swyx) book. It distils the best of Rob's experience from curating [tens of thousands of one page sites](https://onepagelove.com/inspiration) and of course working on his own. You can see a short sample of all 100 tips from [his hugely popular tweet thread here](https://twitter.com/robhope/status/1265278107088347136?s=20) - Dev.to Embed: {% twitter 1265278107088347136 %}\n\nIt's a quick read - just 100 short tips (1-2 page each) on everything from Design to Development to Pricing to Testimonials. \n\n> Side note: It's very un-like my style - where I like to explore every idea in detail, spell out every sub point (My book is 100k words even after editing!), Rob clearly likes to trim all the fat and just leave you with the core essence. There's a place for both, but I suspect Rob's approach is by far the more popular!\n\nThe book goes into more detail on each item and offers further links and 200+ resources to use to help achieve these things. Each tip is also delightfully narrated for an audiobook version, [with sound effects (!)](https://twitter.com/robhope/status/1299361913751601152?s=20).\n\nPart of the value of the book comes from it being available in so many formats, from PDF to HTML to audio, so that you retain better by spaced repetition, but probably my favorite feature is the categorized checklists.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/08aafsw0f732arkbzohc.png)\n\nI will literally use these for every landing page I have to do for the rest of my life. That's worth the price of admission to me.\n\nRob didn't pay me to write this, I just think it's a great book and am writing it up as per my usual M.O. I did sign up for the affiliate thingy though - If you decide to get the book, I get a small kickback if you **[use my link](https://onepagelove.com/go/swyx)**, at no cost to you. It's $10 off for the month of September. You can also book a personal critique of your landing page, worth $179!\n\n> ‎”I fear not the man who has practiced 10,000 kicks once, but **I fear the man who has practiced one kick 10,000 times.**” - Bruce Lee"
  },
  {
    "slug": "create_luck",
    "data": {
      "title": "How to Create Luck",
      "description": "Your entire worldview changes when you realize you can *create luck*.",
      "tag_list": [
        "luck",
        "advice"
      ]
    },
    "content": "\nMy entire worldview changed when I realized that **luck can be created**. \n\nMore precisely, you can actively **create optimal conditions for lucky things to happen to you**. The more I looked into this, the more I realized that this is not only *not* a new insight, but successful people have studied this for *decades* and I am just late to the party.\n\nIn this post we'll briefly review the \"Literature of Luck\", and then I'll end with some personal thoughts on how it could be extended.\n\n## Binary Luck\n\nMost people have a binary view of luck:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/kgd335km9x3ujfmhxnz7.png)\n\nThis is true enough. Some people are born into privilege, some people just win some literal or figurative lottery or other.\n\nThe closing question of every episode of Guy Raz's [How I Built This](https://www.npr.org/podcasts/510313/how-i-built-this) podcast asks successful people: *\"How much of your success is due to skill, and how much is due to luck?\"*\n\nThose who believe in their own agency answer the former. Others - who've seen people smarter and harder working than them fail - answer the latter. Those who are politically correct cop-out with the half-and-half.\n\nIt can be comforting to subscribe to the binary luck model. If you just got a bad roll of the dice, there's nothing you could do. Your lack of success is not your fault. \n\nBut what if I told you **there are people who have skill at creating luck** for themselves?\n\n## Luck Surface Area\n\nJason Roberts coined the term \"[Luck Surface Area](https://www.codusoperandi.com/posts/increasing-your-luck-surface-area)\", and it was expanded by [Sean Murphy](https://www.skmurphy.com/blog/2019/04/03/increase-your-luck-surface-area-to-get-more-customers/) and popularized by [Patrick McKenzie](https://www.perell.com/podcast/patrick-mckenzie-internet-famous). \n\nI liken this model of luckiness to a \"catchment area\" ([a term from urban and hydrological geography](https://en.wikipedia.org/wiki/Catchment_area)). Luck is still randomly occurring, but you can position yourself in a way that captures more of it:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/ogbkk8weenqb9nltrl34.png)\n\nJason has a really simple model of how to grow your LSA - do more things, and tell more people about it. **Doing and Telling**. Already this embodies a more active attitude toward how you can orient your life for more positive random events. It's \"[Fixed vs Growth mindset](https://www.brainpickings.org/2014/01/29/carol-dweck-mindset/)\" adapted for luck.\n\n## Four Kinds of Luck\n\nA parallel, older school of thought dates back to James Austin in 1978 and was repopularized by [Marc Andreesen](https://pmarchive.com/luck_and_the_entrepreneur.html) in 2007, then [Naval and Nivi](https://twitter.com/nivi/status/1094940675353784320?lang=en) a decade later.\n\nHere, there are no helpful visuals. James Austin just gives a list of types, and Marc quotes verbatim. [Naval summarizes](https://twitter.com/naval/status/1093981014920052736) the 4 kinds of luck as such:\n\n> 1. Hope luck finds you.\n> \n> 2. Hustle until you stumble into it.\n> \n> 3. Prepare the mind and be sensitive to chances others miss.\n> \n> 4. Become the best at what you do. Refine what you do until this is true. Opportunity will seek you out. Luck becomes your destiny.\n\nI've quoted this many times to friends and always had trouble remembering what the 4 types are. I gave it some thought and visualized/organized it as such:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/5ycsicfgoxsvxoyys5ip.png)\n\n1. **🌱 Accidental Luck**: You have the same luck as a plant. A plant does not move. Whether or not a plant does well pretty much just depends on where it's seed lands. It's not very interesting since by definition you can't do anything about it, but ofc privilege plays a huge part.\n2. **🏃🏽‍♀️ Active Luck**: The luck you get from constantly moving around. There's no particular direction in mind, but you're more likely to find something good if you move around and explore instead of stay put and hope. You're more likely to roll a 6 if you roll more dice.\n    \n    >  \"I have never heard of anyone stumbling on something sitting down.\" - [Charles Kettering](https://due.com/blog/keep-going-charles-f-kettering/)\n    \n    > \"You don’t get extreme results without extreme actions.\" - [Derek Sivers](https://sive.rs/extremex)\n3. **💊 Prepared Luck**: The luck you get from noticing that something lucky has happened, that most would miss. The canonical story on this is Alexander Fleming's discovery of penicillin, which was a huge medical breakthrough. The discovery was a total accident (some mold happened to fall in the right spot + Fleming happened to see it + he had a similar experience that was a nonevent 9 years ago), but Fleming was not only \"uniquely equipped to observe it\" by his background, he took action to confirm the observation.\n    \n    > \"Luck is what happens when preparation meets opportunity.\" - Seneca\n    \n    > \"Chance favors the prepared mind.\" - Louis Pasteur\n    \n    > \"Richard Feynman was fond of giving the following advice on how to be a genius. You have to keep a dozen of your favorite problems constantly present in your mind, although by and large they will lay in a dormant state. Every time you hear or read a new trick or a new result, test it against each of your twelve problems to see whether it helps. Every once in a while there will be a hit, and people will say: “How did he do it? He must be a genius!”\" - [Gian-Carlo Rota](http://themattheweffect.org/tag/richard-feynman/)\n4. **🧲 Magnetic Luck**: *\"Chance IV comes to you, unsought, because of who you are and how you behave.\"* All sources call it \"individualized action\" but I've renamed it \"magnetic luck\" to emphasize the end result rather than how you get there. I'm quite familiar with this as [the \"Miner\" gear of my Learning Gears](https://www.swyx.io/writing/learning-gears/#miner) terminology (since updated to 4 gears in [my book](https://learninpublic.org/)). This was already a thing pre-Internet, but search and social media have given tremendous reach and influence to the oddballs and obsessives that become the spiritual leaders of every idea and purpose both big and niche.\n\nI find the 4 of these hard to remember, so I've organized them along two axes - **active vs passive**, and **general vs individual**. The first axis is the same realization as \"Luck Surface Area\" - you actually have the power to do things to create more luck than you were given.\n\nThe second axis is the insight - that there are forms of luck that apply to everyone, and there are forms of luck that are available only to someone in your unique position. There is a you-shaped hole in the universe and you can either passively occupy it or you can become a beacon for some idea or purpose.\n\n## Habits and Strategy\n\nI of course find the Four Kinds of Luck very appealing, since I've quoted it so much to friends that I'm writing this post at all. But upon closer reading I think there's a *slightly* different direction that is unaddressed by the Four Kinds (I originally thought this was embedded in the Four Kinds, only to discover that it wasn't in the source material and I had completely read my own thinking into it). \n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/poue5zg9homy1eo4ml05.png)\n\nThis model differs in two ways. It treats both axes as a spectrum rather than a 2x2. It also focuses more on **actions** (you can take) rather than **classification** (which are a little more abstract). I've also swapped out \"Active vs Passive\" for \"Active Habits\" and \"Individual vs General\" for \"Good Strategy\". So my version is more about HOW you get more lucky. It's not very actionable to be \"more magnetic\", but you can Claim a Domain or Hustle and have a good sense of what those things entail.\n\nI've written about a few of these ideas in prior posts so I won't elaborate:\n\n- [Claiming a Domain, Personal Branding](https://www.swyx.io/writing/marketing-yourself/)\n- \"Copywork\" I wrote about in the \"Clone Open Source Apps\" chapter of the book\n\nI think **Exploring** and **Prospecting** are worth elaborating here.\n\n**Exploring** (and to a lesser extent Grinding and Copywork) are **Actively Habitual, non Strategic** activities. But this doesn't mean it is bad. I think this is what you do when you take in general life and career advice and apply them to yourself. You know there are a list of **Principles** which are just generally good things to do in life, and trust that you will do well if you do those things (e.g. [Learn in Public](https://www.swyx.io/writing/learn-in-public/)). Keep doing the \"right\" things, and \"trust the process\".\n\n**Prospecting** is a term I've [borrowed from the oil exploration industry](https://en.wikipedia.org/wiki/Prospecting). **Prospecting is a highly Active habit, and highly Strategic.** These days, when you look for oil, there is a whole science to handicapping whether or not a plot of land is likely to have oil. You don't know it for a fact, all you're doing is estimating probabilities. Not to get too tautological, but you will be luckier if you can consistently assess and move towards areas where you are more likely to be \"lucky\". It might look like luck to others, but the motion and intention you invested to get yourself in a position to be lucky was far from random. I've also written many times about how I think strategic [manoeuvring for \"Megatrends\"](https://www.google.com/search?&q=swyx.io+%22megatrends%22&oq=swyx.io+%22megatrends%22) is a good idea.\n\n## In Summary\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/5075sq79y4pstxpp4pdk.png)\n\nThis is the state of my thinking on luck right now. That last bit you just read is pretty fresh, I might come back and change this in a couple years when I have refined my thoughts. I welcome any and all feedback.\n\nBut overall, the message I really want to leave you with is this: **you can create luck**. That's it. I don't care how you do it, what mental model you use, who you quote. I just care *that* you do it. **Go make yourself more lucky.**\n\n> Ordinarily I'd wish you good luck, but now you have something better than a mere wish 😂"
  },
  {
    "slug": "karabiner_lindquist",
    "data": {
      "title": "Putting your Keyboard on Steroids with Karabiner Elements",
      "description": "I did a livestream with John Lindquist from Egghead.io today, and he blew my mind on how much mileage you can get out of your keyboard!",
      "tag_list": [
        "automation"
      ]
    },
    "content": "\nToday [I joined John Lindquist's Twitch stream to learn Karabiner Elements](https://www.twitch.tv/videos/723328200)! \n\nMy interest stems from 2 directions:\n\n- [my recent RSI explorations](https://dev.to/swyx/notes-on-rsi-for-developers-who-don-t-have-it-yet-239n)\n- a general interest in productivity and automation - I have heard a great deal from folks like Wes Bos how much TextExpander and Alfred help them in productivity, and I suspect I am not taking as full advantage of these tools as I should be.\n\nFriends like [Brandon Bayer](https://twitter.com/flybayer/status/1288610907367514113) use Karabiner to prevent bad (for RSI) key combinations, and both John and [Vadim](https://twitter.com/swyx/status/1274054230911803392) use it to do \"Home Row Computing\".\n\n## What is Karabiner Elements?\n\n[On its website](https://karabiner-elements.pqrs.org/), it describes itself as \"A powerful and stable keyboard customizer for macOS\". It is [free and open source](https://github.com/pqrs-org/Karabiner-Elements). \n\nI have also heard it called a \"key remapper\", but I don't think that describes the full range of what it does, because \"map from A to B\" is just the bare minimum of the possibility.\n\n## Setting up Karabiner and Goku\n\nAfter [downloading](https://karabiner-elements.pqrs.org/), the next move was to set up [Goku](https://github.com/yqrashawn/GokuRakuJoudo). Karabiner has a verbose JSON DSL - Goku lets you write a terser Clojure based EDN format that compiles to that JSON.\n\n```bash\nbrew install yqrashawn/goku/goku\ncd /Users/yourusername/.config\ntouch karabiner/karabiner.json\ntouch karabiner.edn\n# prepopulate the edn with something from https://github.com/yqrashawn/GokuRakuJoudo/blob/master/tutorial.md\ngoku\n```\n\nThis will give you something to start with.\n\n## The EDN language\n\nEDN is a dialect of Clojure. This is basic EDN:\n\n```clojure\n{:main [\n  {:des \"hello world\"\n   ; comments use semicolons\n   :rules [\n      [:a :b] ; map a key to b key\n   ]}\n]}\n```\n\nWe didnt find the EDN VSCode Extension formatter to be very good, so we just manually formatted the file ourselves.\n\nThe array syntax is good for setting up multiple rules:\n\n```clojure\n{:main [\n  {:des \"mapping some keys to the other\"\n   :rules [\n      [:a :b] \n      [:c :d] \n      [:e :f] \n      [:g :h] \n   ]}\n  {:des \"combination keys\"\n   :rules [\n      ; map \"shift+spacebar\" to \" = \"\n      [{:key :spacebar :modi :left_shift} [:spacebar :equal_sign :spacebar]]\n   ]}\n]}\n```\n\nYou can create layers to put Karabiner into modes:\n\n```clojure\n{\n     :layers {\n        ; implement caps lock mode\n        :caps_mode {:key :caps_lock :alone {:key :escape}}}\n        ; implement vs code mode\n        :applications {\n            :code [\"com.microsoft.VSCode\"]\n        }    \n     :main [\n    {:des \"capslock layer\"\n    :rules [\n        :caps_mode\n            ; VIM MODE - hold caps and AJKL\n            ; home row computing\n            [:##h :left_arrow] ; even with f, still do left arrow\n            [:##k :up_arrow]\n            [:##j :down_arrow]\n            [:##l :right_arrow]\n            [:##f :left_option]\n            [:##d :left_shift]\n            ]}\n    {:des \"these only work when you are inside vscode\"\n    :rules [ \n        :code\n            [:p :m] ; remap p to m INSIDE VSCODE\n    ]}\n     ]\n}\n```\n\nYou can even enable a [multitouch extension](https://karabiner-elements.pqrs.org/docs/manual/misc/multitouch-extension/) to switch modes based on how many fingers you have on the touchpad!\n\n```clojure\n{:des \"trackpad2\"\n:rules [ \n    ; 2 fingers down\n    [:condi [\"multitouch_extension_finger_count_total\" 2]]\n        [:f :button2] ; \n        [:v [:button1 :!Cv]] ; go around and paste\n        ; idea: g for cmd + click\n]}\n```\n\n## More\n\nTo see an expert's edn file, see:\n\n- https://github.com/nikitavoloboev/dotfiles/blob/master/karabiner/karabiner.edn\n- search github: https://github.com/search?l=&q=filename%3Akarabiner.edn&type=Code\n\nother ideas for things you can do:\n\n- https://stevelosh.com/blog/2012/10/a-modern-space-cadet/#s15-better-shifting\n- http://vadimpleshkov.me/notes/all/remapping-arrows/\n- [ensure consistent keyboard shortcuts in different apps](https://twitter.com/johnlindquist/status/1298317208435585024?s=20)\n\n"
  },
  {
    "slug": "amplify-flutter",
    "data": {
      "title": "First Look at AWS Amplify Flutter (Developer Preview)",
      "description": "AWS Amplify launched a Flutter Integration, so I thought I would record a quick video and blogpost",
      "tag_list": [
        "aws",
        "flutter"
      ]
    },
    "content": "\n[Flutter](https://flutter.dev/) is Google’s UI toolkit for building beautiful, natively compiled applications for mobile, web, and desktop from a single codebase, and is one of the fastest growing mobile frameworks in 2020. [AWS Amplify recently announced Flutter support](https://aws.amazon.com/blogs/mobile/announcing-aws-amplify-flutter-developer-preview/) in Developer Preview, meaning it's not officially launched yet, but we can start trying it out now.\n\nHere's my log from getting a simple Photo-Sharing app going with Authentication and Storage (example is in Android, but should work for iOS and web too).\n\n## Explained In 100 Seconds\n\n- YouTube: https://youtu.be/0NgR-Qa5GPE\n- Twitter: https://twitter.com/swyx/status/1298349861696282625?s=20\n- Dev.to Embed: {% youtube 0NgR-Qa5GPE %}\n\n## Start a new Flutter App\n\nAssuming you've [set up all the prerequisites for Flutter](https://flutter.dev/setup) (including setting up an [Android Virtual Device](https://developer.android.com/studio/run/managing-avds) and [accepting the Android SDK licenses](https://stackoverflow.com/questions/39760172/you-have-not-accepted-the-license-agreements-of-the-following-sdk-components), you can head to **File -> New -> New Flutter Project** and pick the basic Flutter Application starter.\n\nWe'll need to add the new Amplify flutter libraries, so be sure to add them in `pubspec.yaml` and `Pub get` to download:\n\n```yaml\ndependencies:\n  flutter:\n    sdk: flutter\n\n\n  # new\n  amplify_core: '<1.0.0'\n  amplify_auth_cognito: '<1.0.0'\n  amplify_storage_s3: '<1.0.0'\n\n  # we happen to also use this in our demo\n  file_picker: ^1.13.3\n\n  ## this is also available if you want analytics but not covered in this post\n  # amplify_analytics_pinpoint: '<1.0.0'\n```\n\n## Initialize an Amplify project\n\nNote: If you're reading this in late August 2020, be aware that you'll need a special version of the CLI because it is still in developer preview:\n\n```bash\nnpm i -g @aws-amplify/cli@4.26.1-flutter-preview.0\n```\n\nWe then follow the standard Amplify setup instructions (assuming [CLI is installed and configured](https://docs.amplify.aws/lib/project-setup/prereq/q/platform/flutter)). \n\n```bash\n# command\namplify init\n\n# prompts\n? Enter a name for the environment\n    `dev`\n? Choose your default editor:\n    `IntelliJ IDEA`\n? Choose the type of app that you're building: \n    'flutter'\n⚠️  Flutter project support in the Amplify CLI is in DEVELOPER PREVIEW.\nOnly the following categories are supported:\n * Auth\n * Analytics\n * Storage\n? Where do you want to store your configuration file? \n    ./lib/\n? Do you want to use an AWS profile?\n    `Yes`\n? Please choose the profile you want to use\n    `default`\n```\n\nNotice that you will see a configuration file created in `./lib/` called `amplifyconfiguration.dart`. This is a static file you can import that holds all your public facing Amplify data, but most people will only use it in `amplify.configure(amplifyconfig)`.\n\nWe're going to want to set up storage for our photo app, which requires authentication. Fortunately the CLI sets up both at once for us:\n\n\n```bash\n# command\namplify add storage\n\n# prompts\n? Please select from one of the below mentioned services:\n    `Content (Images, audio, video, etc.)`\n? You need to add auth (Amazon Cognito) to your project in order to add storage for user files. Do you want to add auth now?\n    `Yes`\n? Do you want to use the default authentication and security configuration?\n    `Default configuration`\n? How do you want users to be able to sign in?\n    `Username`\n? Do you want to configure advanced settings?\n    `No, I am done.`\n? Please provide a friendly name for your resource that will be used to label this category in the project:\n    `S3friendlyName`\n? Please provide bucket name:\n    `storagebucketname`\n? Who should have access:\n    `Auth and guest users`\n? What kind of access do you want for Authenticated users?\n    `create/update, read, delete`\n? What kind of access do you want for Guest users?\n    `create/update, read, delete`\n? Do you want to add a Lambda Trigger for your S3 Bucket?\n    `No`\n```\n\nWhen you've set up whatever else you might need, you can push to the cloud to kick off the provisioning:\n\n```bash\namplify push --y\n```\n\nThis process takes a while to provision all the resources for you, so while that's going we can set up our code first.\n\n## Diving into Flutter code\n\nInside of `./lib/main.dart`, we can import everything we will need:\n\n```dart\nimport 'package:amplify_auth_cognito/amplify_auth_cognito.dart';\nimport 'package:amplify_core/amplify_core.dart';\nimport 'package:amplify_storage_s3/amplify_storage_s3.dart';\nimport 'amplifyconfiguration.dart';\n// etc\n```\n\nThen inside of our main page we can write the Amplify config code:\n\n```dart\n// inside of some class for the page\n  Amplify amplify = Amplify();\n  bool _isAmplifyConfigured = false;\n\n  @override\n  initState() {\n    super.initState();\n    _initAmplifyFlutter();\n  }\n\n  void _initAmplifyFlutter() async {\n    AmplifyAuthCognito auth = AmplifyAuthCognito();\n    AmplifyStorageS3 storage = AmplifyStorageS3();\n    amplify.addPlugin(\n        authPlugins: [auth],\n        storagePlugins: [storage],\n    );\n    await amplify.configure(amplifyconfig);\n\n    setState(() {\n      _isAmplifyConfigured = true;\n    });\n  }\n```\n\nThis way we are able to add a little loading screen based on `_isAmplifyConfigured` if we wish.\n\nFor the rest of the app, you will have to write up the Flutter UI experience yourself, but the hard part is over from writing Amplify code.\n\n### Using Amplify Auth in a Widget\n\nCall the appropriate functions ([docs here](https://docs.amplify.aws/lib/auth/signin/q/platform/flutter)) as needed. Here's how we sign a user in:\n\n```dart\nimport 'package:amplify_auth_cognito/amplify_auth_cognito.dart';\nimport 'package:amplify_core/amplify_core.dart';\nimport 'package:flutter/material.dart';\n\nclass SignInView extends StatefulWidget {\n  @override\n  _SignInViewState createState() => _SignInViewState();\n}\n\nclass _SignInViewState extends State<SignInView> {\n  final usernameController = TextEditingController();\n  final passwordController = TextEditingController();\n\n  String _signUpError = \"\";\n  List<String> _signUpExceptions = [];\n\n  @override\n  void initState() {\n    super.initState();\n  }\n\n  void _signIn() async {\n    // Sign out before in case a user is already signed in\n    // If a user is already signed in - Amplify.Auth.signIn will throw an exception\n    try {\n      await Amplify.Auth.signOut();\n    } on AuthError catch (e) {\n      print(e);\n    }\n\n    try {\n      SignInResult res = await Amplify.Auth.signIn(\n          username: usernameController.text.trim(),\n          password: passwordController.text.trim());\n      Navigator.pop(context, true);\n    } on AuthError catch (e) {\n      setState(() {\n        _signUpError = e.cause;\n        _signUpExceptions.clear();\n        e.exceptionList.forEach((el) {\n          _signUpExceptions.add(el.exception);\n        });\n      });\n    }\n  }\n\n// etc for the Widget build\n```\n\n### Using Amplify Storage in a Widget\n\n\nCall the appropriate functions ([docs here](https://docs.amplify.aws/lib/storage/overview/q/platform/flutter)) as needed. Here's how we upload a new picture:\n\n```dart\nimport 'package:flutter/material.dart';\nimport 'dart:io';\nimport 'package:file_picker/file_picker.dart';\nimport 'package:amplify_core/amplify_core.dart';\nimport 'package:amplify_storage_s3/amplify_storage_s3.dart';\n\nclass ImageUploader extends StatelessWidget {\n  void _upload(BuildContext context) async {\n    try {\n      print('In upload');\n      // Uploading the file with options\n      File local = await FilePicker.getFile(type: FileType.image);\n      local.existsSync();\n      final key = new DateTime.now().toString();\n      Map<String, String> metadata = <String, String>{};\n      metadata['name'] = 'filename';\n      metadata['desc'] = 'A test file';\n      S3UploadFileOptions options = S3UploadFileOptions(\n          accessLevel: StorageAccessLevel.guest, metadata: metadata);\n      UploadFileResult result = await Amplify.Storage.uploadFile(\n          key: key, local: local, options: options);\n\n      print('File uploaded.  Key: ' + result.key);\n      Navigator.pop(context, result.key);\n    } catch (e) {\n      print('UploadFile Err: ' + e.toString());\n    }\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return Column(children: [\n      RaisedButton(\n        child: Text(\"Upload Image\"),\n        onPressed: () {\n          _upload(context);\n        },\n      )\n    ]);\n  }\n}\n```\n\n## Full Demo App\n\nIf you would like to see a demo app with all this setup, head to the `amplify-flutter` example repo: https://github.com/aws-amplify/amplify-flutter/tree/master/example\n\n(and give it a ⭐ - it really helps!)\n\n## What's Next?\n\nBeing a new SDK, the Flutter Developer Preview is definitely lacking feature parity with the JS/Android/iOS SDK's. Here's what's coming in the next few months:\n\n- API (REST/GraphQL): The API category provides an interface for retrieving and persisting your data, using GraphQL backed by AWS AppSync or REST APIs and handlers using Amazon API Gateway and AWS Lambda.\n- DataStore: Amplify DataStore provides a programming model for leveraging shared and distributed data without writing additional code for offline and online scenarios, which makes working with distributed, cross-user data just as simple as working with local-only data.\n- Predictions: The Predictions category enables AI/ML use cases on the frontend such as translating text, converting text to speech, and recognizing text, labels and faces in images.\n- Auth: We will extend the Auth category with social sign in with Web UI\n- Storage: We will extend the Storage category with the ability to track Upload and Download Progress\n- Escape Hatches: We will add support for ‘escape hatches’, enabling developers to more easily use the low-level generated iOS and Android AWS Mobile SDKs for additional use cases."
  },
  {
    "slug": "master_to_main",
    "data": {
      "title": "Cheatsheet for moving from Master to Main",
      "description": "Notes I have collected on how to renaming the Git default/primary branch",
      "tag_list": [
        "tech"
      ]
    },
    "content": "\n*[Share on Twitter](https://twitter.com/swyx/status/1296829223051526152?s=20)*\n\nFor my own reference, and anyone else interested in moving primary git branch from `master` to `main`. \n\n> I'm not interested in discussing [reasons to do this](https://www.hanselman.com/blog/EasilyRenameYourGitDefaultBranchFromMasterToMain.aspx) here, it has been rehashed thousands of times already.\n\nTL;DR: \n\n![Cheatsheet for Master to Main](https://dev-to-uploads.s3.amazonaws.com/i/6wqq892i53i8fn19kcg3.png)\n\n## Move Existing Projects\n\n### 1. Rename branches\n\n```bash\ngit branch -m master main # history unchanged\ngit push origin HEAD\ngit branch -u origin/main main\n```\n\n### 2. retarget existing PRs \n\nYou can use https://github.com/ethomson/retarget_prs to do it.\n\n### 3. set default branch\n\nMake sure you've pushed your `main` branch, then head to `https://github.com/USERNAME/REPONAME/settings/branches` - [docs here](https://docs.github.com/en/github/administering-a-repository/setting-the-default-branch)\n\n### 4. update local clones (if applicable)\n\nThis is handy if you are working on local forks of OSS - Thanks to https://twitter.com/xunit/status/1269881005877256192\n\n```bash\n$ git branch --unset-upstream\n$ git branch -u origin/main\n```\n\n## Set Default For New Projects\n\nYou can set new projects created on your machine to start with `main` branch as well.\n\n```bash\n## Git 2.28+\ngit config --global init.defaultBranch main\n## Git 2.27-\ngit config --global alias.new '!git init && git symbolic-ref HEAD refs/heads/main'\n```\n\nIf you're on a Mac like me, you can `brew upgrade git` or [download Git](https://git-scm.com/downloads) to update the version. Recent versions also include [sparse-checkout](https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/), in case you needed more incentive to upgrade.\n\nThis is for new projects *on your local machine* - unfortunately GitHub hasn't made a new setting for setting the default `main` branch for new repos created on GitHub yet.\n\n> Edit: now it has! head to https://github.com/settings/repositories to set it\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/rs27j4f09wle6cn6h1a3.png)\n\n## Set Bash Aliases\n\nLastly, you can setup [bash aliases](https://opensource.com/article/19/7/bash-aliases) that tries `main` first and then `master` so you get to use the same alias no matter what you work with:\n\n```bash\nalias gpom=\"git push origin main 2> /dev/null || git push origin master\"\n```\n\n## GitHub's Plans\n\nIf the GitHub process seem rather manual, don't worry. I have hundreds of repos on GitHub. GitHub has plans to release automated tooling to help you manage this.\n\nPlans: https://github.com/github/renaming\n\n- ✅ Jul 17 - redirects of deleted branches\n- ✅ Aug 1 - [GitHub Pages work from any branch](https://github.blog/changelog/2020-07-31-build-and-deploy-github-pages-from-any-branch-beta/)\n- ✅ Aug 26 - [Be able to change default branch in Github](https://github.blog/changelog/2020-08-26-set-the-default-branch-for-newly-created-repositories/) \n- ✅ Oct 1 - Default branch AUTOMATICALLY changes to `main` on GitHub\n- Summer - Configurable default for new repos\n- EoY 2020 - Retargeting PRs/Releases/Pages for existing repos\n"
  },
  {
    "slug": "prerecording_talks",
    "data": {
      "title": "Prerecording Talks for Online Conferences",
      "description": "Tips for Speakers Prerecording Talks for Online Conferences",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\n\nI've just given my 5th or 6th prerecorded talk this year, [at React Rally 2020](https://www.swyx.io/speaking/react-rally-metalanguage). Prerecorded talks are an increasingly popular way to do virtual conferences, because they reduce a host of conference organizer risks. However, they also present new challenges and opportunities to speakers.\n\nA lot of folks start off doing prerecorded talks exactly the same way they do in-person conference talks. This is fine, but misses the opportunity to fully take advantage of the medium.\n\nI'm no expert, but I realized I may have some ideas from my experiences that others may want to steal. Here they are.\n\n## Recording Yourself\n\n- **Write a script**. It's BETTER to have a script when prerecording. I rarely have a script when speaking live - it is more natural to be able to improvise and command attention when not scripted. But the prerecording restriction takes ALL the benefit out of speaking off script. Here, the expensive thing is now recording and editing. You should try as much as possible to reduce the amount of editing you need to do - and that means trying out words and phrases in writing first, instead of taking a million cuts and improvising. Writing a script may feel like extra work, but trust me that it saves rather than adds time.\n   - You can still improvise!! Just have a script first.\n   - **Front-load value**. People have a FAR shorter attention span online than in person. They could be deciding whether or not to listen to your 30 minute talk based on the first 30 seconds. They could be playing you in the background while working on something else. If you start out flat and monotone, I'm going to tune out. **Grab attention. Use pauses, videos, cliffhanger stories, whatever you can think of to get them to actually watch**.\n   - You can [set up a teleprompter for yourself](https://mobile.twitter.com/QuinnyPig/status/1291597180021559296) if you feel you must look at the camera at all times, but honestly nobody minds if you look away every now and then, especially when your face is not the focus of attention\n- **Good Lighting**: You want your face to be lighted up nicely. This doesn't mean you have to shine a [ring light](https://spectrum-brand.com/blogs/news/what-is-a-ring-light) directly in your eyes. I use a regular LED lamp and shine it at white paper taped on to the wall in front of my desk, so it bounces off the paper and *then* onto my face. Multiple lights will help you avoid shadows.\n- **Interesting background**: plain white is a little too plain. no Zoom backgrounds please. A good background tells a little about your personality, but isn't too distracting. Books, vases, plants, guitars, toys, paintings, shelves. You can take some inspiration from [Jason Lengstorf, Sarah Drasner](https://www.learnwithjason.dev/make-animations-feel-pro), [Michael Liendo](https://www.youtube.com/watch?v=ZQyiYifmwfg) or [Simona Cotin](https://www.youtube.com/watch?v=kVA6EfClvlg) or [Scott Tolinski](https://www.youtube.com/c/LevelUpTuts/videos).\n- **Camera**: Many laptops skimp on cameras, so the quality isn't that great. A lot of serious folks buy mirrorless DSLRs just for livestreaming, but if you are in the market for a cheaper option, I use a Logitech C9250-ish webcam. I elevate the camera with a few books so it is at eye level.\n- **Microphone**: A good microphone is important here - you can watch video with bad camera quality, but bad sound quality is unwatchable. Stick it right in front of your face. I use [the standard issue eggheadio mic](https://howtoegghead.com/instructor/screencasting/audio-equipment/), I've carried it across 4 continents.\n- **Misc**: Watch what you are wearing - you are home, but you don't have to dress too \"homely\". Don't wear the same color as your background. Try to lean forward to prevent double chin.\n\n\nCheck my [video on Growing a Meta-Language](https://www.youtube.com/watch?v=18F5v1diO_A). Dev.to Embed: {% youtube 18F5v1diO_A %}\n\n## Recording Your Screen\n\n- **MAKE. EVERYTHING. BIGGER.** Statistically, half your audience will be watching on their phones. Even if they watch on desktop, your video may be shrunk down for any number of reasons (livestream banners). Make everything as big as possible without having to scroll too much every second. It also just forces you to get rid of everything not immediately relevant on your screen. \n- **Clear Your Screen**: this is worth a standalone point. Remove everything not relevant to your talk. This includes stray browser tabs and chrome extensions. You can use a clean [Chrome Profile](https://www.bettercloud.com/monitor/the-academy/how-to-create-switch-profiles-in-chrome/) to do this (more from [Dave Ceddia](https://daveceddia.com/setup-chrome-for-screencasting/)). Hide the activity bar and file explorer in your IDE when not relevant. Hide the Mac OS menubar unless you actually need it. This isn't just about reducing clutter for aesthetic purposes - reducing points of variance can help you record video with time skips so it can be easier to edit seamlessly later.\n- **Make It Obvious**: If your software supports it (i use Screenflow) add an indicator for when you have clicked something on screen (usually concentric circles). [Make your mouse cursor bigger](https://www.lifewire.com/make-mac-mouse-pointer-bigger-2260808) so it is easier to track. Speak what you are typing and doing on screen if it is something you are trying teach. \"I cut this command here and paste it over there.\" \"I search and replace all instances of this variable.\"\n- Be wary of recording the keyboard sound when you type and speak at the same time. This is why using a separate mic helps reduce that noise. You can also choose to record your screen first with some narration, throw away the audio, then record voiceover without any typing.\n- Think about where you are placing your face later when you edit these things together. If you are doing anything in the bottom right, be aware that you may have to move your face awkwardly. Also good practice in general to leave a healthy margin in all directions and don't do anything too close to the corner of any side of the screen.\n- Slides can be nice, but you need slides less than in live talks. Since you are prerecording, you can easily \"live code\" and never be at any risk of mistakes. Likewise, you can bring people around on websites and apps to demonstrate what you are trying to say and what things will actually look like, instead of having each thing be merely a bullet point. **Show, don't tell**.\n\nCheck my [video on Vite + React for an example](https://www.youtube.com/watch?v=oJd9XzNIHzQ). Dev.to Embed: {% youtube oJd9XzNIHzQ %}\n\n## Editing Opportunities\n\n- I use [Screenflow](http://www.telestream.net/screenflow/store.asp). Free alternatives exist, like [Kdenlive](https://kdenlive.org/en/).\n- **Try different layouts**. Most people have their screen take up the full size of the video, and their camera/face floating/overlaid on the bottom right. It's fine, but it's boring. You can invert that - have your face/camera take the full video, and inset the screen. Alternatively, you can **move your face around** as a replacement of laser pointer, to specifically indicate what part of the screen you are talking about. Don't overdo it!\n- Transition scenes with quick fades instead of having hard cuts.\n- You can cut and speed up routine tasks in video so your audience don't have to wait for you, but you also don't have to take the cognitive loss of a jump cut.\n- You aren't limited to what's on your screen, or what's on your slides. You can superimpose even more content on your final video - more images to quickly reference what you are talking about, and even video and charts to illustrate your point. Don't throw too much or it will feel very chaotic, but adding good visuals atop audio can do a lot to break up the monotony of talking through a single slide for a whole minute. One every 10-20 seconds feels right.\n- Consider what the title frame/screenshot will be. This will be what most people see in YouTube before they decide to watch your video. make it good.\n- Since you're not live, take advantage of it to make your video AS CLEAR AS POSSIBLE. For example, if you have a 30 minute talk, with 3 parts of 10 minutes each, you could show a little overlay with text or diagrams as a visual reminder of where you are in the talk. This helps people follow along when you transition and change to the next point, as well as people on YouTube when they are scrubbing and skipping ahead.\n- Consider the final call to action - this is a VERY good chance to send people to related talks you have done and/or your personal blog/site/twitter for leaving comments on the talk.\n\nYou can see a very good example of editing in [Rich Harris' Svelte Society talk](https://www.youtube.com/watch?v=luM5uobewhA). Dev.to Embed: {% youtube luM5uobewhA %}\n\n## During the Talk\n\nFinally you can relax! You've done all the hard work! But this is the last benefit from going prerecorded - you can still talk while you talk!\n\n- Provide onscreen links in the conference chat so that people can click to follow through\n- Respond to questions as they come in\n- Discuss other things you didn't have time to cover but point people in the right direction if they\n- Director's commentary - talk about the behind the scenes of what's going on in the video and tell stories what you had to do to get there\n- I also share my script and slides for people who want to read along. It doesnt have to be 100% faithful\n- Heckle yourself - have some fun with it!\n\n---\n\n## Other resources\n\n- https://medium.com/shiftconf/how-to-prep-for-speaking-at-a-virtual-conference-2bb4ecfc0d30\n- Video setup: https://www.youtube.com/watch?v=WedG8LKO6ks\n- Lights: https://www.amazon.com/dp/B07K8GGX48/?coliid=I2Z6KLAW1XNOCD&colid=2VU9TEDIL0L6V&psc=1&ref_=lv_ov_lig_dp_it\n- use iphone for camera https://reincubate.com/camo/\n\n## Your Turn\n\n**What other great tips from prerecorded talks have you observed?**\n"
  },
  {
    "slug": "developer-exception",
    "data": {
      "title": "Developer Exception Engineering",
      "description": "It's time we look beyond the easy questions in developer experience, and start addressing the uncomfortable ones.",
      "tag_list": [
        "dx"
      ]
    },
    "content": "\nWe need to have some real talk around what we are calling Developer Experience (DX) at developer-focused companies. \n\nIt's not well defined, although [Chris Coyier has done a good survey](https://css-tricks.com/what-is-developer-experience-dx/) of what people think of when they hear the term, and others try to define [Developer Enablement](https://devbizops.substack.com/p/from-developer-experience-to-enablement). It's a differentiator among devtools, so of course it is professionalizing, and creeping into job titles (my last job title was \"Developer Experience Engineer\", whatever that actually means). I fully expect this to develop into a fullblown sub-industry, with conferences and thoughtleaders and gatekeepers and the like.\n\n## \"Developer Experience\"\n\nHere are some things that companies traditionally work on when they \"work on DX\":\n\n- **Replacing many with few**: Replacing many lines of code with few lines of code. Replacing many logos with one logo. Replacing many steps with one click (signup, deploy). [Generating code](https://www.swyx.io/writing/write-my-code-for-me/) so you don't have to handwrite it. Providing great value and plenty of functionality as a first-party, out of the box, or with zero config.\n- **Extensive Documentation**: Getting Started. Example Demos. Interactive Examples. Full API Docs. Guides and Recipes. Good Search. Versioning. (appropriate to [project maturity](https://www.swyx.io/writing/documentation-levels/))\n- **More tooling**: CLIs, Editor Extensions, Code Snippets, Playgrounds, Language Servers.\n\nI'm not at all saying these things aren't important. They're even hard to do well, and fully deserve specialists in their own right. These foundational pieces of developer experience should also be *fast* and intuitive to the point of [guessable](https://johno.com/guessable).\n\n## \"Developer Exceptions\"\n\nBut I'm also saying that developers, when they use our products, experience *many* other things which aren't traditionally the domain of \"DX people\", because they have to do with core engineering and product promises:\n\n- **Downtime**: When your service goes down, does your status page lie? Do you refrain from tweeting about it for fear of scaring off customers you don't yet have, rather than reassuring those you do? Do you post prompt, no-bullshit [post-mortems](https://github.com/danluu/post-mortems)? Do you provide good fallback options for when your service is down? Do you practice disaster recovery?\n- **Response times**: Are you not just meeting your SLAs, but actually clearly answering customer questions? What are you doing for users not yet covered by SLAs? For your open source footprint, do users have confidence that their issues will be addressed and appropriate PRs reviewed, or are you asking people to do free work for you that you then ignore?\n- **Missing/Incomplete Features**: No product launches feature complete. Nobody expects you to. **The true test is whether you address it up front or hide it like a dirty secret.** As developers explore your offering, they will find things they want, that you don't have, and will tell you about it. How long do you make developers dig to find known holes in your product? Do developers have confidence you will ship or reject these features promptly, or are they for a \"v2\" that will never come?\n- **Uncertainty over roadmap**: Do your most avid users know what's coming so they can plan around your plan? If that's too high a bar, do your *own employees* know what's coming so they can coordinate nicely? Do you have \"perma-beta\" products? How do you communicate when users ask if they should use \"orphan\" products? (don't be ashamed, everyone has them)\n- **Uncertainty over costs**: Is your pricing predictable or do your users need a spreadsheet to figure out what you are going to charge them? If charges are unexpectedly high, can developers use your software to figure out why or do they have to beg for help? Are good defaults in place to get advance warning? Have your employees ever paid for your own product (or had to justify doing so to their own employers)?\n- **Deprecation/Lock-in/Portability**: Do you [constantly deprecate APIs and products](https://medium.com/@steve.yegge/dear-google-cloud-your-deprecation-policy-is-killing-you-ee7525dc05dc), causing additional work for no gain? Some amount of lock-in is unavoidable, but are you conscious of how much proprietary API you are foisting on your user? Should your user want to leave someday for whatever reason, have you documented that and made that easy, or are they on their own? \n- **Debugging**: Are your errors informative or scary? Have you designed them to be searchable or do they only make sense to maintainers? When things go wrong, how quickly does your service surface common issues and offer resolution steps? What about enabling users to answer [questions they don't yet know they have](https://www.swyx.io/writing/frontend-observability/)? If developers are constantly making mistakes and \"[holding the phone wrong](https://www.engadget.com/2010-06-24-apple-responds-over-iphone-4-reception-issues-youre-holding-th.html)\", is it their fault or yours?\n- **Audit Logs & Access Control**: Many products start life in single-player mode, and then execute a very clumsy transition to multi-player when they start serving teams and enterprise. When something that shouldn't have been done has been done, do you offer trustworthy sources of truth, and ways to prevent repeats (or, of course, make them  impossible in the first place)?\n\n## Causes\n\nOf course, none of these are new problems, and well known to product management. The problem is mostly organizational - as \"developer advocacy\" as a discipline evolved from \"developer evangelism\" (going from a \"1 way street\" to a \"2 way street\"), it is now evolving to \"developer experience\". I see a troubling organizational limitation of DX folks, defining DX to the stuff that focuses on increasing \"top of funnel\" growth (reducing friction, growing awareness, an endless parade of copycat short term growth hacks) - and having comparatively a lot less impact on the \"bottom of funnel\" stuff (logo churn, satisfaction scores, account expansion/customer success). **There's little sense making more rain when faced with a leaky bucket**.\n\nConway's Law is an [eponymous law](https://www.swyx.io/writing/eponymous-laws/) that states that \"organizations design systems that mirror their own communication structure\". Steven Sinofsky's snappier definition is \"[don't ship your org chart](https://medium.com/@donorem/shipping-the-org-chart-3319181be9bd)\". We need to be careful about the consequences of letting Conway's Law apply to Developer Experience.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/87bsgzkj9jrftxhgxil9.png)\n\n## Engineering for Developer Exceptions\n\nTo be clear, I don't really know how to do this yet. I am just thinking out loud. But my intuition is that in order to design exceptional developer experiences, we should pay more attention to developer exceptions. As DX people, we've focused a lot on `try`, perhaps we should take a good look at `catch`.\n\nThe first thing to fix is organizational incentives. DX work must not just feel welcome, it must be demanded and good results rewarded. Nobody wants to feel like they are adding weight onto an already overloaded backlog. Most orgs are set up in a way that lack of feedback isn't noticed when it isn't given, and feedback when given feels like additional burden. This is, unsurprisingly, not conducive to feedback.\n\nThe second thing is to establish invariants around your core developer experience and automate monitoring and reporting of these to the fullest extent possible. [Tools that hold the line are incredibly powerful](https://twitter.com/sebmarkbage/status/1063585097545220096). To progress, you need to first stop regressing.\n\nMy last thought is around helping PMs and EMs create DX, rather than taking primary responsibility. If you're sending in PRs and design docs yourself, you are probably doing too much of someone else's job and will be ineffective and/or resented. Better to provide them the information they need to make decisions and be available for opinions and instant feedback since you represent the end user. It is common for there to be an infinite list of small things to do - and that is very hard to sell internally - so how can you bundle these up into thematic projects, motivate engineers, and carefully communicate progress to the wider world?\n\nIt's time we look beyond the easy questions in developer experience, and start addressing the uncomfortable ones.\n\n> Note: This is my own thinking sparked by a third party conversation, and I am not speaking on behalf of my current employer."
  },
  {
    "slug": "realtime-offline-first-chat-app-in-100-seconds-with-amplify-datastore-react-and-graphql-47d4",
    "data": {
      "title": "Realtime Offline-First Chat App in 100 Seconds with Amplify DataStore, React, and GraphQL",
      "description": "A quick demo of how easy it is to add realtime, offline persistence to an app with Amplify DataStore",
      "tag_list": [
        "aws",
        "react",
        "graphql"
      ]
    },
    "content": "\n[Amplify DataStore](https://docs.amplify.aws/lib/datastore/getting-started/q/platform/js#option-1-platform-integration) provides a persistent on-device storage repository for you to write, read, and observe changes to data if you are online or offline, and seamlessly sync to the cloud as well as across devices.\n\nIt is free, open source, and supported by the [AWS Amplify](https://amplify.aws/) team, and I wanted to show you how easy it is to use it to add realtime, offline first CRUD features to your app! We'll use React in this example, but you can easily use this guide for adding realtime, offline-first CRUD to an app built with any framework.\n\n## 100 Second Video Version\n\nYoutube: https://youtu.be/pSSfTWqSXbU\n\nDev.to Embed: {% youtube pSSfTWqSXbU %}\n\n## Text Based Version - 3 Steps\n\nThe stuff below is the script for the video above, so you can copy/paste!\n\n### Step 1: Setup the React Chat App\n\nAssuming you have [setup the Amplify CLI](https://docs.amplify.aws/start/getting-started/installation/q/integration/react), we're going to spin up a standard React app and install a special demo Chat component I've prepared under [the `react-demos` package](http://npm.im/react-demos):\n\n```bash\nnpx create react-app amplifychatapp\ncd amplifychatapp\nyarn add react-demos # or npm install react-demos\n```\n\nLet's try out this demo component to get familiar with it!\n\n```js\n// src/App.js\nimport React from 'react'\nimport { Chat, useChatLocalState } from 'react-demos'\n\nexport default function App() {\n  const {\n    currentUser,\n    sendMessage,\n    loginUser,\n    messages,\n    usersOnline,\n  } = useChatLocalState()\n  return (\n    <div>\n      <Chat\n        {...{\n          currentUser,\n          sendMessage,\n          loginUser,\n          messages,\n          usersOnline,\n        }}\n      />\n    </div>\n  )\n}\n```\n\nNow we can start up our app with `npm run start` and it works! This data isn't stored or shared though - when you reload that page or load it in an incognito browser, the messages start over from scratch. Not much of a chat app!\n\n### Step 2: Setup the Amplify DataStore\n\nWe'll init a new Amplify project and `amplify add api`, making sure to enable \"Conflict Resolution\" (which enables the Amplify DataStore):\n\n```bash\nyarn add aws-amplify @aws-amplify/datastore \n# or use npm install\n\namplify init \n# go with all default answers... and when you are done...\n\namplify add api\n? Please select from one of the below mentioned services: GraphQL\n? Provide API name: # Any Name Is Fine\n? Choose the default authorization type for the API API key\n? Enter a description for the API key: # Any Description Is Fine\n? After how many days from now the API key should expire (1-365): 7\n? Do you want to configure advanced settings for the GraphQL API Yes, I want to make some additional changes.\n? Configure additional auth types? No\n? Configure conflict detection? Yes # IMPORTANT\n? Select the default resolution strategy Auto Merge\n? Do you have an annotated GraphQL schema? No\n? Choose a schema template: Single object with fields (e.g., “Todo” with ID, name, description)\n# some instructions here...\n? Do you want to edit the schema now? Yes\n```\n\nThis will open up your editor where we can specify the GraphQL schema for the DataStore (it is exactly the same schema definition language as [GraphQL Transform for AWS AppSync](https://docs.amplify.aws/cli/graphql-transformer/overview)). We'll paste in this very simple schema:\n\n```graphql\n# /amplify/backend/api/YOURAPINAME/schema.graphql\ntype User @model {\n  id: ID!\n  name: String\n}\n\ntype Message @model {\n  id: ID!\n  user: String\n  text: String\n}\n```\n\nSave the file and `amplify push --y` to kick off provisioning the AWS backend!\n\nWhile that's going, we will run `amplify codegen models` to [generate the DataStore models](https://docs.amplify.aws/lib/datastore/getting-started/q/platform/js#code-generation-amplify-cli) we will use in our React app.\n\n### Step 3: Wire up DataStore with React\n\nNow let's put it into use:\n\n```js\n\nimport React from \"react\";\nimport { DataStore } from \"@aws-amplify/datastore\";\nimport { User, Message } from \"./models\";\nimport { Chat } from \"react-demos\";\nimport Amplify from 'aws-amplify';\nimport awsconfig from './aws-exports';\nAmplify.configure(awsconfig); // will not sync if you forget this\n\nfunction App() {\n  const [currentUser, setCurrentUser] = React.useState(null);\n  const [usersOnline, setUsersOnline] = React.useState([]);\n  const [messages, setMessages] = React.useState([]);\n\n  React.useEffect(() => {\n    fetchMessage();\n    DataStore.observe(Message).subscribe(fetchMessage);\n  }, []);\n  React.useEffect(() => {\n    fetchMessage();\n    DataStore.observe(User).subscribe(() => \n      DataStore.query(User).then(setUsersOnline)\n    );\n  }, []);\n  async function fetchMessage() {\n    const _Messages = await DataStore.query(Message);\n    setMessages(_Messages);\n  }\n\n  async function loginUser(name) {\n    const user = await DataStore.save(new User({ name }));\n    setCurrentUser(user);\n  }\n  async function sendMessage(text) {\n    await DataStore.save(\n      new Message({\n        user: currentUser.name,\n        text,\n      })\n    );\n  }\n\n  return (\n    <div>\n      <Chat\n        {...{\n          currentUser,\n          sendMessage,\n          loginUser,\n          messages,\n          usersOnline,\n        }}\n      />\n    </div>\n  );\n}\n\nexport default App;\n```\n\nAnd there you have it - a realtime, offline persisting chat app with Amplify DataStore!\n\n## Conclusion\n\nNow you've had a taste, be sure to [head to the docs](https://docs.amplify.aws/lib/datastore/getting-started/q/platform/js) to get a fuller understanding, or [watch Richard Threlkeld's Re:Invent 2019 talk where Amplify DataStore was first introduced!](https://www.youtube.com/watch?v=KcYl6_We0EU&feature=emb_title)\n\n> P.S. if you are worried about incurring AWS charges following this guide, just run `amplify delete` at the end to delete everything you just set up in one command! The Amplify DataStore itself carries no charge to use, but it uses [AWS AppSync for storage](https://aws.amazon.com/appsync/pricing/) so you should check that pricing as you plan for production usage.\n\n## Share\n\nLiked this format/tutorial/video? Got requests for more? Please comment and share it with a [Tweet](https://twitter.com/swyx/status/1293676684747526144?s=20) or [subscribe to my YouTube](https://youtube.com/swyxTV)!"
  },
  {
    "slug": "brief-guide-to-startup-fundraising-terminology-7k2",
    "data": {
      "title": "A Developer's Guide to Startup Fundraising",
      "description": "A Brief Guide to Startup Fundraising Terminology for Beginners",
      "tag_list": [
        "startup"
      ]
    },
    "content": "\nSomeone in the [Coding Career Community](https://www.learninpublic.org/#community) asked:\n\n> Is there a good guide out there to familiarize oneself with the structures and lingo of startups? I mean knowing about seed fundraising, equity rounds... don't be afraid to go really basic on it. Startups for Dummmies.\n\nI realize that the lingo can be pretty scary for new developers and the tech news media never really stops to explain some basics. So here is a quick brain dump of thoughts at least from a US-biased perspective.  This is a living document, please feel free to add in comments and I will edit.\n\n\n## Important Caveats and Disclaimers\n\nI'm just a developer with a former finance background and an interest in this stuff. But I'm no expert or VC. I have never personally raised any startup money. but I do keep tabs on the US startup ecosystem and have worked at a couple startups, most recently from [Series B to C](https://dev.to/swyx/working-at-a-startup-from-series-b-to-c-445p).\n\nJust sharing what I know, developer to developer, not trying to come across as an authoritative source or anything. \n\n## Why Fundraise At All\n\nWe should probably start with why you even want to raise funds, when most \"normal\" (non tech) businesses start with a bank loan or a \"rich uncle\".\n\nThis happens because of the economics of scale. If you're just starting a mom 'n pop shop, you don't need a lot to get started, and you can (hopefully) recoup what you invest in a short amount of time. \n\nBut if you're building a $1b factory to make $200 computer chips or hiring a bunch of $200,000/yr engineers to write $10/month software, you need a lot of money upfront, and patience to grow sales up to the point where your overall profits start to cover the upfront cost you incur.\n\nMany startups don't start with their full economics worked out yet. The assumption is that if you can get a bunch of users first, you can figure out how to monetize them later. So modern startups can often run with negligible revenue for years (think: GIPHY, Reddit, Facebook, but also open source developer tooling companies like Gatsby) while having a **burn rate** (contractually obliged spending, like employee payroll or office rental) of hundreds of thousands a month. This is especially important if the goal is to spread virally, or to form a sticky habit.\n\nOften, aggressive companies will even use the money they raise to subsidize their customers, just to grow the customer base and build the habit (think: Uber/Lyft, Postmates/Doordash). They will literally *make a loss on every transaction*, something you can only do if you have deep pockets. So you go *deeper* in the red the *faster* you grow. As you might imagine, this is high risk, high reward. Giving money to customers to get customers sounds like a terrible business, but it worked very well for Paypal.\n\n> Pop culture reference: You saw this happen on [HBO's Silicon Valley where Richard Hendricks \"bankrupts\" Sliceline](https://www.youtube.com/watch?v=Txl90NEl92U) in order to acquihire the company, by buying a bunch of pizzas from them. In reality this is very unlikely to happen due to the expense, but it's a TV show 🤷‍♂️\n\nMoney is often also bundled with advice and network. You may not NEED the money, but if you are building a social network, you might well want to raise money from Reid Hoffman or Peter Thiel just to get their advice. If you are selling to developer enterprises, you may want to raise money from Y Combinator just to get your foot in the door to sell to all Y Combinator alumni companies. Do note that the actual value of these non-monetary aspects of fundraising is heavily disputed. \n\n## Fundraising Economics\n\nThere are two ways of financing any business venture: **Equity and Debt**.\n\nDebt is what you might be familiar from taking on a mortgage for a house - I lend you a sum of money, and you agree to pay it back to me in future, with some amount of interest due every year for my trouble. If your business valuation grows, I take no benefit from that, I merely continue to receive my interest payments. If your business goes bankrupt, you don't owe me anything (more accurately, I take possession of your remaining assets) - thats the risk I took in exchange for those interest payments. Since most startups fail, most startups are not able to get lenders to take this risk. \"*Venture Debt*\" does exist though, for the small category of [startups who can raise it](https://wistia.com/learn/culture/taking-on-debt-to-grow-our-own-way).\n\nMost fundraising takes the form of sale of **Equity**. This is a straightforward exchange of cash for a partial ownership in the company. The math gets a little circular, because the value of the company goes up when the money goes in, which then affects what % of the company you actually own.\n\nSo the startup community has adopted a simple terminology. You have a **\"pre-money valuation\"** and a **\"post-money valuation\"**. The \"money\" here is the amount invested/raised. So say I give you a $45m pre money valuation and I invest $5m. Your post money valuation is $50m, and I now own a 10% stake in your company (5 divided by 50). In practice this lingo can be shortened in various ways: \"I raised 5 on 50\", \"I'll take 10% on a 50 post\", so on. The lingo differs but the math is the same.\n\nMost valuations quoted are post-money, because it is the bigger and easier number to do math with.\n\n## Important Nuances on Equity\n\nEarly stages of startups often have a hybrid between debt and equity. This is often used to deal with the fact that trying to place a valuation on an early stage company with no revenues is meaningless, but standard debt is unattractive for the investor, so we improve the upside for them by letting the debt convert into equity at some future point. This has a few names, and is known as **convertible debt** in the public markets, but in startups the popular version of this is the [Simple Agreement for Future Equity or SAFE](https://www.ycombinator.com/documents/), introduced by Y Combinator in 2013. SAFEs recognize the futility of placing a valuation number on an early stage, and only convert to equity in a future **priced round**. \n\n> Technically if you raise SAFEs and then never raise a priced round, your investors and employees never get anything back. [Toptal founder Taso du Val is accused of doing this](https://www.theinformation.com/articles/at-booming-toptal-no-stock-for-employees-or-investors). \n\nAnother way startup investors sweeten the deal is to include special terms in the contract. There are a wide range of these:\n\n- An option to invest further money in future funding rounds\n- Some sort of \"preference\" or \"preferred\" status in the investment, where, if the business gets sold, they get paid back first ahead of other investors\n- Anti-dilution terms to defend against **dilution** from future fundraising (I'll explain below)\n- Board seats (which lets investors have a say in company strategy and hire/fire senior management including founders - routine for lead investors of each stage to have)\n- and more I cant think of and don't know about (*what other major terms should i add?*)\n\n**Dilution** is what happens when startups issue new shares to raise subsequent rounds of funding and you aren't involved. This happens because nobody ACTUALLY sells \"10% of a company\". You create, out of thin air, 1 million shares, and give me 100,000 of them. Then, for the purpose of simple math, when you raise your next round, you create 1 million more shares, and I for whatever reason don't invest more money, my 10% goes down to 5% (because I now own 100,000 shares out of a shareholder base of 2 million shares).\n\nYou most famously saw dilution dramatized in [The Social Network](https://www.youtube.com/watch?v=xdiFzcpmmJc), when Eduardo Saverin was diluted out of his ownership of Facebook. This happens to cofounders, employees, and unsophisticated investors if they aren't careful. Therefore anti-dilution terms are used to protect investments. Some go overboard and become genuinely predatory, eating up more of the company when it is not doing well, like the dreaded **[full ratchet](https://www.cooleygo.com/glossary/full-ratchet/)**.\n\nFundraising Terms are deals with the devil. You of course get better valuation and more money with these terms, but they make it more complicated for you down the line as well as for everyone else on your **\"cap table\"** (the list of people who invested with you). If you wish to be acquired some day, you ideally want to have \"**clean terms**\". Of course this is the domain of lawyers and much negotiation.\n\nNow that you know what **dilution** is, you can also learn the difference between **primary** and **secondary** share sales. **Primary** fundraising happen when you create new shares out of thin air and sell them to investors. **Secondary** sales happen when you transfer ownership of existing shares to investors. Most fundraising is primary. However, you often see some secondary sales as well bundled alongside the primary sales, especially in later stages. This allows founders and employees to \"take some money off the table\", and is usually not a sign of lack of faith in the company. Instead, secondary sales are mostly simple pragmatism in that founders and early employees may sit on paper wealth worth millions which might go to 0 someday, so it is prudent to cash in some of it in. Basecamp founders Jason Fried and DHH are famously criticized for \"raising money\" from Jeff Bezos, but this [was a secondary sale](https://m.signalvnoise.com/the-deal-jeff-bezos-got-on-basecamp/) that gave them the financial security to build for the long term.\n\nAn extreme version of secondary sale is VC's buying out other VC's. Chris Sacca [became a billionaire by buying up 10% of Twitter](https://techcrunch.com/2011/02/27/jp-morgan-twitter-chris-sacca-10-percent-secondary/) from other shareholders who weren't as optimistic as he was. VC's have also been known to stalk departing employees to snap up their shares in a hot company. \n\nMost employees do not get outright grants of equity. They get **options** to buy equity. They also don't get these options right away - they **vest** over time to ensure that you can't just join for 1 month and leave with the whole amount (Founders vest too). If they leave the company, they get to \"buy in\" to the company they helped to build. If the company gets acquired, all employee options convert to shares. For more details on the employee perspective, see the [Holloway Guide to Equity Compensation](https://www.holloway.com/g/equity-compensation).\n\nIn the US, it is generally illegal to openly solicit funds for your startup. To protect the average Joe from losing money to some get-rich-quick scheme, you can only raise in private from accredited investors (read: rich people). Of course, this could also have the downside of reserving opportunities to get rich only to the already rich. Exceptions to this rule are opening up, between [the JOBS act](https://www.investopedia.com/terms/j/jumpstart-our-business-startups-act-jobs.asp) and [Reg 506(c) for rolling venture funds](https://techcrunch.com/2020/08/05/gumroad-founder-sahil-lavingia-launches-new-seed-fund-in-collaboration-with-angellist/).\n\n## Fundraising Stages\n\nIn the course of their life as a private startup, startups often raise anywhere from $5m to $5b, between their founding to their **Exit**.\n\nAn \"exit\" is VC lingo for either an Initial Public Offering or an Acquisition. In both cases, the VC gets paid back on their investment either in public company stock or in cash and are said to have \"exited\" their investment - although in reality many VC's often hold on to their stock after IPO, sometimes for years, because they continue to believe in the company as an investment.\n\nWhen you hear a company do a successful exit, it can be common to assume that the founders are billionaires. This is often far from the truth, though they certainly won't have to work again. Typical founder ownership percentages by exit range from 5-20%. It's much more likely that \nVC's have more ownership than founders and therefore make more from the exit (and are financially more incentivized to push for an earlier exit than founders). If that sounds like a bum deal, it can be. Someone running a $100m bootstrapped company can be wealthier than someone running a unicorn. So of course, putting off fundraising can be very beneficial for founders, at the expense of speed of growth. the Atlassian founders [owned 60% of the company](https://www.businessinsider.com.au/atlassian-files-for-ipo-2015-11) at IPO, but took 15 years to get there. \n\nMost professional VCs are not investing money out of their own pockets. VC firms take a partnership format and raise funds in vintages like wine. They raise money from institutions, which range from pension funds to university endowments to family offices (read: \"rich people\"), who form their **\"Limited Partners\"** (LP's). The principal VC investors are thus **\"General Partners\"** (GP's) of those partnerships. Now  you know what it means when VC's talk about their \"fiduciary duty to LPs\". The compensation model is usually some variant of \"2 and 20\" - speaking VERY VERY loosely, VC's earn 2% of funds invested per year, and 20% of realized profits over some pre-agreed hurdle on exit. This varies WILDLY, and I want to be extra clear that I have only very third-hand knowledge of the actual terms here.\n\nBut the point is this - **funds are raised for a specific purpose to serve a specific clientele with desired risk profile and investment mandate.** Some LPs are comfortable with the risks of early stage investment. Some only want to invest in proven later stage companies. Money is NOT the same shade of green everywhere. It is completely reasonable and natural to have different profiles of investors along the lifetime of a journey. Further, these investors expect a return on their investment within a reasonable timeframe, say, 10 years (the commonly cited amount of time between starting and exiting a \"unicorn\" startup - though this has gotten longer recently as more and more financing moves to private markets).\n\nThere are no hard and fast rules with fundraising stages, but there is an informal norm with many exceptions. As a rule of thumb, 10-30% of post money equity is sold at every stage, and you need to reach some proof points before you get the next stage. The other way to think about stages is that given your projected **burn rate** (contractually obliged spending, like employee payroll or office rental), you want to raise about 2 years worth of funds at each stage (burn typically does a quantum step up at every stage, you see a flurry of hiring in order to meet aggressive growth targets).\n\nHere is a very rough, illustrative guide:\n\n- **Angel/Pre-seed round**: the \"first money in\" to any company. Usually these are former entrepreneurs looking to \"give back\" to the next generation of entrepreneurs. There are legends and self made billionaires in this space alone - [Chris Sacca angel-invested in Uber, Kickstarter, Twitter, Instagram, and Facebook and returned 250x his investors' money](https://observer.com/2017/08/the-strange-question-chris-sacca-used-to-build-the-greatest-angel-portfolio-ever-investing-venture-capital/). Jason Calacanis has written a [notable book](https://www.goodreads.com/book/show/32600757-angel) on this and Naval Ravikant started [AngelList](http://angel.co/) to help angel investors find you. The size of this round is very small, from 50k to 500k. You're not expected to have much here beyond an idea and a passionate pitch.\n- **Seed round**: the first \"institutional\" money in from **incubators** and **seed stage VCs**. THe amount raised is anywhere from $1.5m to $5m.  Some \"celebrity investors\" can come in for just tiny checks of $5-10k, just lending their prestige to the startup. (\"we are backed by the founders of X, Y, and Z\"). Here you may have a working proof of concept, or a [minimal viable product](https://en.wikipedia.org/wiki/Minimum_viable_product). Here is extensive detail on [a real life seed round from Freshpaint](https://www.freshpaint.io/blog/anatomy-of-a-seed-round-during-covid-19), right down to check size of each investor. \n- **Series A**: this is where things get serious and you start to be on the \"VC clock\". [It is possible](https://sahillavingia.com/reflecting), but very rare, to get off the VC track once you are on. You have a working product, a small base of loyal users, and a plan to get to Series B in the next 2-3 years. Your mandate is to grow this userbase and potentially build out your platform. Yous might also start to monetize, but it is not super important right now. Amount raised from $10-$25m, with exceptions on either end (this goes for everything in this post). Here's [Standard Treasury's Series A Deck](http://blog.zactownsend.com/standard-treasurys-series-a-pitch-deck).\n- **Series B**: this is where you start to really pour fuel onto the fire. By this stage you might be regarded to have [\"product-market fit\"](https://en.wikipedia.org/wiki/Product/market_fit), a clear business model, and a clear story around how 1 additional dollar of funds raised turns into 5 or more dollars of customer lifetime value. **Cash is the limiting business factor.** Amount raised $15-$50m. Check out [LinkedIn's Series B Pitch Deck](https://www.reidhoffman.org/linkedin-pitch-to-greylock/). It has been called [the hardest round to raise](http://tomtunguz.com/hardest-round-raise-2017/).\n- **Series C**: Your business model is working! You've built out your 2nd or 3rd product and they're also doing well! You've reached the proof points needed to unlock the MASSIVE growth stage!  Amount raised $30-$100m. [Here's Front's Series C Pitch Deck](https://medium.com/@collinmathilde/front-series-c-deck-11773b30b272).\n- Round about here you hopefully become a **unicorn** (valued at $1b or more)\n- **Series D-G**: You are on the path to becoming a **decacorn** (10b or more) and/or you are burning a lot of money and don't want to IPO to get it.\n\nHere's is a list of some popular names of investors that specialize in each stage: https://www.fundz.net/what-is-series-a-funding-series-b-funding-and-more\n\nThese numbers are merely illustrative and there are lots of great reasons why an individual company might not meet those situations. After all, the nomenclature of \"Series A B or C\" only depends on the order of fundraises. Jason Calacanis has coined the term of **\"Pegasus\"** for companies that normally would've raised a round at a particular stage, but don't because they didn't need the money, therefore they \"fly\" over a particular stage. Webflow is one example of a Pegasus that [raised an abnormal $72m \"Series A\"](https://news.crunchbase.com/news/webflow-raises-72m-series-a-continuing-the-trend-of-outsized-early-stage-rounds/) (not at all a standard Series A by any metric).\n\nOf course there are other reasons to reach abnormal valuation at any given stage - serial founders with proven track records, or just abnormal business results, like with [Rippling's $145m Series B](https://www.rippling.com/blog/entrepreneurship/pitching-vcs-heres-the-deck-we-used-to-raise-145m/).\n\nThe most aggressive VCs and high potential startups do **\"pre-emptive\" rounds** - meaning they get the money they would have received for a stage, despite not having yet met the traditional metrics needed for that stage. Pre-emptive A's and B's are common.\n\nOn the other end of the spectrum, if startups need a little more time to hit their metrics, they often get intermediate rounds of funding. This can be called a number of things, from \"bridge\" to \"extension\"\n\nGiven how every case is special, funding dynamics are always very difficult to generalize. You can find collated data in [Crunchbase](https://en.wikipedia.org/wiki/Crunchbase) and [Pitchbook](https://pitchbook.com/). They also change over time. The amounts and valuations of each stage has almost doubled over time compared to where they were 10 years ago.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/u3a0e9h31x7b5bt0vqsh.png)\n\nIt will likely shrink and grow together with the economy. The exits of one generation of entrepreneurs also frees up money to invest in the next, so investing can happen in waves.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/2f9vjk0bflp3gaiblieu.png)\n\nIf you are wondering how fundraising has been affected by Covid, [here's some data for you](https://tomtunguz.com/fundraising-market-corona-two-quarters-in/) - investment sizes have gone UP, numbers have gone DOWN. Massive concentration in companies that are doing well.\n\n## To be continued?\n\nPhew that was a lot. Let me know if there are still unanswered questions and I'd be happy to add to this."
  },
  {
    "slug": "5-q-a-s-on-writing-and-selling-my-first-book-18d1",
    "data": {
      "title": "5 Q&A's on Writing and Selling My First Book",
      "description": "I answered some questions from someone planning to write their first book",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nFrom [April to July 2020](https://www.swyx.io/writing/coding-career-launch) I made >$45k writing and selling [the Coding Career Handbook](https://www.learninpublic.org/) and the related Community and Creator packages to over 1k individual developers, teams, and bootcamps.\n\n> Before you get too excited, know that it was over 600 hours of work, and probably took me 2-3 years of relationship building, career experience, and blogging before that to build up to this. I was exhausted afterward. I will NOT be able to repeat this strategy or topic again for a while, this was at best a once-every-couple-years effort.\n\nToday I got some questions from [Jess Eddy](https://twitter.com/jesseddy/status/1292969688083755009?s=20), who is planning to write her first book (also a career advice book, but different industry). \n\nI'm answering in public!\n\n---\n\n## 1. Did you test the contents of your book while writing it?\n\nYes! It served two purposes - marketing the book as well as getting feedback from readers.\n\nMy first attempt at this went EXTRAORDINARILY well. I wrote and published [How To Market Yourself](https://twitter.com/swyx/status/1249793388037025797?s=20) 3 days after deciding to write the book. This was explicitly marked as a chapter in the book.\n\nI knew it would be a popular topic, but there was some risk in that, if the chapter turned out bad, I would lose credibility (if I can't market a blogpost, can't market a book, how can I teach others to market themselves?). I took the risk.\n\nFortunately, it did not fail. It is probably my 3rd or 4th most popular piece of writing ever. It also immediately got me booked on 3 different podcasts ([like this](https://www.productionreadypod.com/episodes/marketing-yourself-your-products-by-learning-in-public-w-shawn-wang) and [this](https://www.youtube.com/watch?v=QFHO2-8fGtM) and [this](https://aquestionofcode.com/71-how-should-developers-market-themselves-shawn-wang/)). So publishing the chapter helped market the book and helped me produce even more free collateral for the site! Wonderful stuff.\n\nI also livestreamed 5 hours of this writing process. After launch, I took down the livestream and made the video part of my \"Creators package\" (which helps teach others to \"get paid to learn in public\", aka create their first products). Might as well reuse valuable work, eh?\n\nIn case you think I know what I'm doing, my second and third attempts did not fare as well. \n\nThe second attempt was my Intro to Tech Strategy chapter - first released as a [screenshot](https://twitter.com/swyx/status/1250837183264522242?s=20), and then [as a blogpost](https://twitter.com/swyx/status/1253009793448460288?s=20). I think the topic choice was a mismatch between how important I *know* it is to developers, but how much developers are inherently interested in it. Fortunately, I believed in this topic strongly enough to not care that it didn't do well. I also got some valuable feedback in a couple of interactions.\n\nThe third attempt [did not do much better](https://twitter.com/Coding_Career/status/1258070398605774848). I knew \"Junior Developer vs Senior Developer\" was a hot topic. I wrote up a whole chapter, but only published some core quotes. I chose a different format here - tweet thread from book account, not main account. It has a smaller following of course. I'm not sure how I could have done this better.\n\nI also livestreamed writing my chapter on Why You Should Write. Very meta, but it [didn't really do well](https://twitter.com/swyx/status/1259186402907967488?s=20).\n\nat this point i just abandoned public testing and just went heads down writing until the book was done, then when i was 99% done I started sending out to early reviewers for both testimonials and feedback on things that could be problematic (HUGELY important for a soft skills book like this one with a massive attack surface I can be criticized on)\n\nThroughout the writing process I kept tweeting behind the scenes from both my personal account and my book account. I also presold the book, so I was able to send periodic updates to presale buyers, which serves as a form of marketing.\n\n---\n\n## 2. What publishing methods have been most fruitful for you (Kindle, Gumroad, etc.)?\n\nThere are two questions here. Publishing format and publishing platform.\n\nI think the format you pick will depend on the kind of experience you aim to deliver. If you have a graphics/design heavy book, for example, then go PDF-only. If you have a text-heavy book like mine, then be aware that the market is [exactly split a third each between EPUB, MOBI and PDF](https://twitter.com/swyx/status/1265301477049790464?s=20), and your publishing tool will ideally want to support all of them (this is annoyingly hard to achieve AND have other requirements met like having autogenerated Table of Contents - which the professional writing app Ulysses has had \"on the roadmap\" [for 7 straight years](https://twitter.com/ulyssesapp/status/1252593635125792772?s=20)). I used Michael Hartl's [Softcover](https://www.softcover.io/) which does the job, but requires you to learn some LaTeX to typeset to acceptable quality. (more on typesetting in a future post, lmk what qtns you have here)\n\nChoice of Platform is a smaller field of choices.\n\n- Kindle Direct Publishing takes a HUGE cut. They take 30% if you charge under $9.99, and 70% if you want a higher priced book. You also of course don't have the option to sell packages.\n- Gumroad is well known in this space, mainly bc of the founder's marketign ability. They charge 3.5% + 30c with a $100/yr Creator account. They have a trusted brand and can take PayPal, which is good in some circles. They also serve as a \"Merchant of Record\" which helps US/Canada residents not worry about state/VAT tax nonsense. Their landing page is spartan and serviceable, but buggy (not terminal). And their couponing/affiliate system is easy to use. Analytics are good. Their \"marketplace\" can bring enough sales to cover the cost of a Creator account.\n- Podia is the other big player here. Where Gumroad focuses on ebooks, Podia is more generalist on ebooks + video courses + webinars + email marketing. They don't charge transaction fees, but cost $390-790/yr depending on plan. I've made back the $790 on affiliates alone. Do the math as to how much you expect to sell vs the feature set. Their landing page is better designed but still kinda ugly. Analytics nonexistent. And their external embed widget is buggy. However, their support for memberships/video/webinars/email made me switch from Gumroad to them. They also offer live chat with your customers, which people ACTUALLY use, surprisingly.\n- both founders of Gumroad and Podia are cocky and annoying and sometimes problematic. idk if that matters to you. it's pissed me off sometimes.\n- Quite honestly, you can't go wrong picking either Gumroad or Podia. both aren't perfect but they do similar jobs in slightly different ways. If you end up using Podia you can toss me a few coins using my podia referral https://www.podia.com/?via=shawn-wang at no cost to you.\n- Paddle is another big player but mostly seems to be used by Europeans. They can sell ebooks but also do packaged software (e.g. little utilities)\n- Other smaller players are out there, like Sellfy, Happeno, Payhip and the newly launched Converkit Commerce.  No exp with them.\n\n---\n\n## 3. How did you figure out pricing and did it change over the course of time?\n\nI honestly wasn't very scientific about pricing.\n\nI started out wanting to presell, so I picked a $19 price point on the day of the presale and said it was 50% off, making my \"actual\" price $39. This was back when I intended to make it a 2 week writing effort.\n\nAs I wrote and wrote and my scope grew, I decided I wanted to write a bigger book. So as 2 weeks became 2 months I kept selling it at $19, but I knew I'd be asking more at the end.\n\nIn the leadup to launch I kind of reverse engineered everything from $39. I knew I wanted the launch price of the book to be $39. Assuming a launch discount of $20, that means full price of the book is $59. from there, I then priced the Community package at $99 (just under buying another book, with the promise that you get more out of it if you join the community. Along with an audiobook version, which people are used to paying extra for and did take a ton of extra work) and the Creators package at $249 (with extra workshops and the livestream). I set these with a quick call for advice from [Joel Hooks of Egghead](https://twitter.com/jhooks/status/1278357145411399680?s=20).\n\nThe 20% launch sale lasted through all of July. Now the sale has expired, people are still buying $200-300 of book daily. I don't know how much more I would get if I had priced the book at a more normal $39 instead of $59. But I'm happy making $6-9k a month while this lasts.\n\nIf the goal was to maximize revenue, I might do more pricing experiments. But instead, I wanted to write an aspirational book and I stand by the value of the book. A higher list price also leaves me room to do sales in future.\n\nNeither Podia or Gumroad have capability to adjust prices for purchasing power, so I don't explicitly offer it, but I have reserved some coupons for countries with obviously lower income.\n\n---\n\n## 4. How do you market your book?\n\nheheh. See #1.\n\nI haven't done much marketing post-launch. The occasional tweet, that's about it. I've offered an affiliate program to friends so that has helped some but I think mostly it's word of mouth from here on out.\n\nI'm thinking of trying out [Acadium](https://app.acadium.com/). I don't really know. seems like a lot of work, and I have a day job. Ideally I'd like to pair up with a marketing partner and split things 50-50.\n\n---\n\n## 5. Is there anything else you think is worth sharing?\n\nAdvice from Robert Kiyosaki: \"They're called best SELLING authors, not best WRITING authors.\" You may feel like writing is the most important thing you can do as an author, but it's probably equal parts selling as it is writing. I'm still learning this. a HUGE part of it is the landing page for the book pretty much sets the expectation for the user driving their purchase decision. Have a look at [Rob Hope's teardown of mine](https://dev.to/swyx/8-landing-page-tips-from-rob-hope-338l) and [Mahmoud Abdelwahab](https://twitter.com/thisismahmoud_/status/1286683701322055680?s=20)'s refactor of my shitty initial page. Landing page is really that impt. Check Rob Hope's [epic thread of advice](https://twitter.com/robhope/status/1265278107088347136) and [use my link](https://t.co/6PElS7Iaa9?amp=1) if you want his book. If you are great at designing your own, you can even do [$40k presale like Emma](https://compiled.blog/blog/how-i-made-40000-dollars-on-a-book).\n\nIf you have no audience and no authority you can bootstrap it by interviewing a bunch of folks like [Philip Kiely](https://philipkiely.com/wfsd/) did. I don't know how the commercial terms for these things work and suspect it worked for him bc he is a college kid. But doing interviews are an easy content+marketing package.\n\nDon't forget to make a team licence. I've made about 2k like this.\n\nDon't worry about DRM/piracy.\n\nFor specific launch sequences and to study comparable launches, please check out my **[launch cheatsheet](https://github.com/sw-yx/launch-cheatsheet/blob/master/README.md)**. Contributions VERY welcome, including of your own reflections.\n\n**It doesn't end with the launch, and it doesn't end with the book either.**\n\nYou can keep putting out new versions, continue to add value to the higher tiers.  And when you launch your NEXT thing, be it a course or book, you can bundle this book together with it to add extra value. When you launch you will feel like this is the most precious thing in the world, because you worked so hard on it, but with the passing of time it'll be emotionally easier to give the book away for other gains.\n\nIf you have a plan to \"win\" whether or not the book sells well, whether it becomes:\n\n- part of your overall personal brand (*\"hey, she wrote the book on X, she must know what she's talking about even if I havent read the book\"*), \n- or your future course offerings (*\"i'll even throw in a free copy of X valued at $99!\"*)\n\nThen you cannot fail.\n\nGood luck and enjoy the ride! "
  },
  {
    "slug": "notes-on-growing-a-language-by-guy-steele-5501",
    "data": {
      "title": "Notes on \"Growing a Language\"",
      "description": "Revisiting one of my favorite talks ever, on designing programming languages for user extensibility and collaboration",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nOne of the talks in my Lindy Library (from the [Lindy effect](https://en.wikipedia.org/wiki/Lindy_effect) which I discussed in [my book](https://learninpublic.org/)) is \"Growing a Language\", by Guy Steele:\n\n- [Youtube Link](https://www.youtube.com/watch?v=_ahvzDzKdB0)\n- Dev.to Embed: {% youtube _ahvzDzKdB0 %}\n- [The full text of the talk with some diagrams is here](https://www.cs.virginia.edu/~evans/cs655/readings/steele.pdf)\n\nIf you care enough about programming languages (and every framework, every library has aspects of a language), you come to care intensely about programming language **design**. This is one of the few accessible, entertaining talks that tackles this head on.\n\nI'm watching it again in preparation for an upcoming talk so here are my public notes.\n\n## Core Points\n\n- You should design languages to start small and then grow.\n- You should design languages so that users help you grow them easily.\n- The solution to the Cathedral vs Bazaar is let everyone play but have a BDFL decide what to take in/out. Have a Shopping Mall of good ideas.\n- It is good for you and your users, to give them a chance to buy in and pitch in.\n- If you design a small number of useful patterns, you can say no to a lot more things that not everybody uses, while letting them define things they will use.\n- Generic Types and Operator Overloading are 2 such patterns that let users solve all sorts of problems.\n- Going meta: We should now think of a language design as a pattern for language designs, a tool for making more tools of the same kind.\n- Use simple words in real life too.\n\n## Raw Notes\n\n> ⚠️ Note - a lot of these are lifted verbatim from the transcript as I am just doing a first pass of notes right now.\n\n- A language that is too small is hard to use. You can't say much at all until you take the time to define at least a few new terms.\n- If you want to get far at all with a small language, you must first add to the small language to make a language that is more large.\n- **I need to design a language that can grow.** I need to plan ways in which it might grow—but I need, too, to leave some choices so that other persons can make those choices at a later time. \n- We have known for some time that huge programs can not be coded from scratch all at once. There has to be a plan for growth. What we must think on now is the fact that languages have now reached that large size where they can not be designed all at once, much less built all at once.\n- At times we think of C as a small language designed from whole cloth. But it grew out of a smaller language called B, and has since grown to be a larger language called C plus plus. **A language as large as C plus plus could not have spread so wide if it had been foisted on the world all at once.**\n- [Worse is Better](https://en.wikipedia.org/wiki/Worse_is_better): The gist of it is that the best way to get a language used by many persons is not to design and build 'The Right Thing,' because that will take too long. In a race, a small language with warts will beat a well designed language because users will not wait for the right thing; they will use the language that is quick and cheap, and put up with the warts. Once a small language fills a niche, it is hard to take its place.\n- Languages must do a lot more things than they used to:\n\n    > They need to paint bits, lines, and boxes on the screen in hues bright and wild; they need to talk to printers and servers through the net; they need to load code on the fly; they need their programs to work with other code they don’t trust; they need to run code in many threads and on many machines; they need to deal with text and sayings in all the world’s languages. A small programming language just won’t cut it.\n\n- So a small language can not do the job right and a large language takes too long to get off the ground. Are we doomed to use small languages with many warts because that is the sole kind of design that can make it in the world?\n- No. Start small, then grow.\n- If one person does all the work, then growth will be slow. But if one lets the users help do the work, growth can be quick. If many persons work side by side, and the best work is added with care and good taste, a great deal can be added in a short time.\n- Let the user define new words in a way that they look like primitives. In this way the user can build a larger language to meet his needs. (Hooks pass this test). \n- In APL, new words don't look like primitives. To add to APL, it takes a lot of work - this has stopped users from helping to grow the language. To adopt userland code into the language takes a total rewrite.\n- In Lisp, new words defined by the user look like primitives and, what is more, all primitives look like words defined by the user. In other words, if a user has good taste in defining new words, what comes out is a larger language that has no seams. The designer in charge can grow the language with close to no work on his part, just by choosing with care from the work of many users.\n- So Lisp grew much faster than APL did, because many users could try things out and put their best code out there for other users to use and to add to the language.\n- **A main goal in designing a language should be to plan for growth.** The language must start small, and the language must grow as the set of users grows.\n\n    > I, as a language designer helping out with the Java programming language, need to ask not 'Should the Java programming language grow?' but 'How should the Java programming language grow?'\n\n- if the goal is to be quick and yet to do good work, one mode may be better by far than all other modes.\n- There are two kinds of growth in a language. One can change the vocabulary, or one can change the rules that say what a string of words means.\n- A library is a vocabulary designed to be added to a programming language to make its vocabulary larger. A true library does not change the rules of meaning for the language; it just adds new words. The key point is that the new words defined by a library should look just like the primitives of the language.\n- It may be good as well to have a way to add to the rules of meaning for a language.\n\nCathedral vs Bazaar \n\n- There are 2 ways to do the growing - One person-or a small group-to be in charge and to take in, test, judge, and add the work done by other persons. The other way is to just put all the source code out there, once things start to work, and let each person do as he wills. To have a person in charge can slow things down, but to have no one in charge makes it harder to add up the work of many persons.\n- The way that I think is faster and better than all others does both. Put the source code out there and let all persons play with it. Have a person in charge who is a quick judge of good work and who will take it in and shove it back out fast. You don’t have to use what he ships, and you don’t have to give back your work, but he gives all persons a fast way to spread new code to those who want it.\n- As for the role of programmer in charge, Eric Raymond says that it is fine to come up with good thoughts, but much better to know them when you see them in the works of other persons.\n- The key point is that in the bazaar style of building a program or designing a language or what you will, the plan can change in real time to meet the needs of those who are working on it.\n- This tends to make users stay with it as time goes by; they will take joy in working hard and helping out if they know that their wants and needs have some weight and their hard work can change the plan for the better.\n\nHIGH POINT OF THE TALK: The Importance of Patterns\n\n- Christopher Alexander says: Master plans have two additional unhealthy characteristics. To begin with, the existence of a master plan alienates the users. After all, the very existence of a master plan means, by definition, that the members of the community can have little impact on the future shape of their community, because most of the important decisions have already been made. In a sense, under a master plan people are living with a frozen future, able to affect only relatively trivial details. When people lose the sense of responsibility for the environment they live in, and realize that they are merely cogs in someone else’s machine, how can they feel any sense of identification with the community, or any sense of purpose there?\n- Does this mean, then, that it is of no use to design? Not at all. But in stead of designing a thing, you need to design a way of doing.\n- A pattern is a plan that has some number of parts and shows you how each part turns a face to the other parts, how each joins with the others or stands off, how each part does what it does and how the other parts aid it or drag it down, and how all the parts may be grasped as a whole and made to serve as one thing, for some higher goal or as part of a larger pattern. A pattern should give hints or clues as to when and where it is best put to use. What is more, some of the parts of a pattern may be holes, or slots, in which other things may be placed later.\n- A generic type is a map from one or more types to types. Put another way, a generic type is a pattern for building types.\n- A word is said to be overloaded if it is made to mean two or more things and the hearer has to choose the meaning based on the rest of what is said. For example, using the rules defined near the start of this talk, a verb form such as 'painted' might be a past tense or a past participle, and it is up to you, the hearer, to make the call as to which I mean when I say it. Just as a user can code methods in just the same way as methods that are built in, the user ought to have a way to define operators for user defined classes that can be used in just the same way as operators that are built in.\n- If we grow the language in these few ways, then I think we will not need to grow it in a hundred other ways; the users can take on the rest of the task. (Examples with number types - cannot add all of them at once even tho each individually may have a good usecase)\n- Some parts of the programming vocabulary are fit for all programmers to use, but other parts are just for their own niches. It would not be fair to weigh down all programmers with the need to have or to learn all the words for every niche used.\n- We should not make the Java programming language a cathedral, but a plain bazaar might be too loose. What we need is more like a shopping mall, where there are not quite as many choices but most of the goods are well designed and sellers stand up and back what they sell.\n- When a language gives you the right tools, such classes can be coded by a few and then put up as libraries for other users to use, or not, as they choose; but they don’t have to be built in as part of the base language. If you give a person a fish, he can eat for a day. If you teach a person to fish, he can eat his whole life long. If you give a person tools, he can make a fishing pole—and lots of other tools! In this way he can help other persons to catch fish.\n\nGoing Meta\n\n- Meta means that you step back from your own place. What you used to do is now what you see. What you were is now what you act on. Verbs turn to nouns. What you used to think of as a pattern is now treated as a thing to put in the slot of an other pattern.\n- We should now think of a language design as a pattern for language designs, a tool for making more tools of the same kind. A language design can no longer be a thing. It must be a pattern—a pattern for growth—a pattern for growing the pattern for defining the patterns that programmers can use for their real work and their main goal.\n- In the course of giving this talk, because I started with a small language, I have had to define fifty or more new words or phrases and sixteen names of persons or things; and I laid out six rules for making new words from old ones.\n- It should give no one pause to note that the writing of a program a million lines of code in length might need many, many hundreds of new words—that is to say, a new language built up on the base language.\n- Language design is not at all the same kind of work it was thirty years ago, or twenty years ago. Back then, you could set out to design a whole language and then build it by your own self, or with a small team, because it was small and because what you would then do with it was small. Now programs are big messes with many needs. A small language won’t do the job.\n- It would be great if there were some small programming language that felt large, the way Basic English is small but feels large in some ways. But I don’t know how to do it and I have good cause to doubt that it can be done at all. \n- So I think the sole way to win is to plan for growth.\n- Parts of the language must be designed to help the task of growth- of growing the language.\n\nPlan for Warts\n\n- You may find that you need to add warts as part of the design, so that you can get it out the door fast, with the goal of taking out the warts at a later time.\n- With care, one can design a wart so that it will not be too hard to take out or patch up later on.\n- Some warts are not bad things you put in, but good things you leave out. Have a plan to add those good things at a later time, if you should choose to do so, and make sure that other parts of your design don’t cut you off from adding those good things when the time is right.\n\nNatural Languages\n\n- If we add hundreds of new things to the Java brand programming language, we will have a huge language, but it will take a long time to get there. But if we add a few things— just the right things- generic types, operator overloading, user defined types of light weight, for use as numbers and small vectors and such— and some other things- that are designed to let users make and add things for their own use, I think we can go a long way, and much faster. We need to put tools for language growth in the hands of the users.\n- I hope that we can, in this way or some other way, design a programming language where we don’t seem to spend most of our time talking and writing in words of just one syllable.\n- (In designing this talk) there was this choice for each new word: is it worth the work to define it, or should I just stick with the words I have?\n- I learned in my youth, from the books of such great teachers of writing as Strunk and White, that it is better to choose short words when I can. I should not choose long, hard words just to make other persons think that I know a lot. I should try to make my thoughts clear; if they are clear and right, then other persons can judge my work as it ought to be judged.\n- Short words work well, if we choose them well. \n- All in all, I think it might be a good thing if those who rule our lives—those in high places who do the work of state, those who judge what we do, and most of all those who make the laws—were made to define their terms and to say all else that they say in words of one syllable. For I have found that this mode of speech makes it hard to hedge. It takes work, and great care, and some skill, to find just the right way to say what you want to say, but in the end you seem to have no choice but to talk straight. If you do not veer wide of the truth, you are forced to hit it dead on."
  },
  {
    "slug": "4-things-i-learned-from-mastering-mongoose-js-25e",
    "data": {
      "title": "4 Things I Learned from Mastering Mongoose.js",
      "description": "A quick book review of the new Mongoose.js book from Val Karpov",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nI first encountered MongoDB as part of the FreeCodeCamp curriculum, then as part of the MeteorJS stack, finally attending a talk at my bootcamp given by it's lead developer, Val Karpov (who, by the way, also coined [the MEAN stack](https://www.mongodb.com/blog/post/the-mean-stack-mongodb-expressjs-angularjs-and)!). \n\nMost resources, including my bootcamp, kind of treat Mongoose as a thin interface to MongoDB, which is a shame given how powerful it really is and how central your mastery of your database stack can be for an app's speed and scale. So it was of course exciting to hear that Val has finally written the definitive guide on Mongoose: [Mastering Mongoose](https://masteringjs.io/ebooks/mastering-mongoose)!\n\nI'm going to jot down 4 things I learned. **Note** - I haven't used Mongoose in about 3 years, so this is mostly very introductory level.\n\n## 1. Change Tracking for Minimal Updates\n\nWhen you load a document from the database using a query and then modify it after, change tracking means Mongoose can determine the minimal update to send to MongoDB and avoid wasting network bandwidth.\n\n```js\n// Mongoose loads the document from MongoDB and then _hydrates_ it\n// into a full Mongoose document.\nconst doc = await MyModel.findOne();\ndoc.name; // \"Jean Valjean\"\ndoc.name = 'Monsieur Leblanc';\ndoc.modifiedPaths(); // ['name']\n// `save()` only sends updated paths to MongoDB. Mongoose doesn't\n// send `age`.\nawait doc.save();\n```\n\nThis is very nice out of the box for performance and I don't have to do a thing!\n\n## 2. Multiple Connections\n\nMost apps only need one connection to MongoDB. However, Mongoose supports multiple connections:\n\n```js\nconst mongoose = require('mongoose');\nconst conn1 = mongoose.createConnection('mongodb://localhost:27017/db1',\n { useNewUrlParser: true });\nconst conn2 = mongoose.createConnection('mongodb://localhost:27017/db2',\n { useNewUrlParser: true });\n// Will store data in the 'db1' database's 'tests' collection\nconst Model1 = conn1.model('Test', mongoose.Schema({ name: String }));\n// Will store data in the 'db2' database's 'tests' collection\nconst Model2 = conn2.model('Test', mongoose.Schema({ name: String }));\n```\n\nThis is helpful when:\n\n- Your app needs to access data stored in multiple databases\n- Your app has some slow operations and you don't want them to cause performance issues on fast queries. A MongoDB server can only execute a single operation on a given socket at a\ntime, and the number of concurrent operations is limited by the `poolSize` of sockets. So to deal with slow connections, you can increase `poolSize` past the default of 5 - however, too many connections can hit OS-level performance issues and limits! Val uses `poolSize = 10` for his production apps. Beyond that, you can't add more sockets. So it's better to just put slow operations on a separate connection.\n\n## 3. Mongoose uses its own Middleware\n\nIn Mongoose, middleware lets you attach your own custom logic to built-in Mongoose functions. You can run `pre` and `post` any function:\n\n- Document Middleware\n  - `validate()`\n  - `save()`\n  - `remove()`\n  - `updateOne()`\n  - `deleteOne()`\n- Model Middleware\n  - `insertMany()`\n- Aggregation Middleware\n  - `aggregate()` - you can do fancy stuff here with modifying the [aggregation pipeline](https://docs.mongodb.com/manual/core/aggregation-pipeline/)\n- Query Middleware\n  - `find()`\n  - `count()`\n  - `countDocuments()`\n  - `deleteOne()`\n  - `deleteMany()`\n  - `distinct()`\n  - `estimatedDocumentCount()`\n  - `find()`\n  - `findOne()`\n  - `findOneAndDelete()`\n  - `findOneAndRemove()`\n  - `findOneAndReplace()`\n  - `findOneAndUpdate()`\n  - `remove()`\n  - `replaceOne()`\n  - `update()`\n  - `updateMany()`\n  - `updateOne()`\n\nThis is a powerful pluggable system that reminds me of similar things in the [npm scripts](https://docs.npmjs.com/misc/scripts) and [Netlify Build](https://github.com/netlify/build) system. \n\nMongoose uses its own system internally - Mongoose attaches a `pre('save')` middleware to all models that calls\n`validate()`. That means `save()` triggers `validate()` middleware, which is why `save()` works the way it does.\n\n\n## 4. Principle of Least Cardinality\n\nWith relational databases it is best to normalise data per schema. The third normal form is something like:\n\n>  \"Every non-key must provide a fact about the key, the whole key, and nothing but the key.\"\n\nYou can do this in Mongoose with `populate()`, but that goes against the grain of how NoSQL schemas should be setup. The common recommendation is to `denormalize` -  a document should store all the properties you want to query by (\"The Princple of Denormalization\"), and, data that is referenced together, belongs together (\"The Principle of Data Locality\"). However, taken literally, this can lead to monster documents that can take forever to load.\n\nThe Principle of Least Cardinality states:\n\n> Store relationships in a way that minimizes the size of individual documents.\n\nThis helps make the correct tradeoff as far as Val is concerned. The other very good reason to do this is the fact that MongoDB limits documents to 16MB in size, of course -  but primarily, the Principle of Least Cardinality is about conserving bandwidth when loading documents.\n\n## Conclusion\n\nThe book offers 4 sample apps built with Websockets, React, and Vue, and even ends with a great discussion about recommended app structure at the directory and app level! These polishing touches and the accessible and well thought through examples on every page make this a great reference point for anyone using Mongoose.js."
  },
  {
    "slug": "8-landing-page-tips-from-rob-hope-338l",
    "data": {
      "title": "8 Landing Page Tips from Rob Hope",
      "description": "I submitted my book's landing page for Rob Hope's review and he obliged! Brutal feedback incoming.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nI submitted [my book's landing page](https://learninpublic.org/) for [Rob Hope's review](https://twitter.com/swyx/status/1284487504670806018) and he obliged! Brutal feedback incoming. I respect Rob's opinion mainly because this is [literally his thing](http://onepagelove.com/) and he is writing THE BOOK on this (if you do get it, [my affiliate link is here](https://gumroad.com/l/hottips?offer_code=swyx)!) .\n\n[As a first time maker](https://www.swyx.io/writing/launching-coding-career) I have a lot to learn, so I expect that there will be a lot of issues. Rob gave me a 5 min review which was just PACKED with insight and I am excited to internalize this and improve.\n\n## 1. Erroring Checkout\n\nIf you try to buy on the page in Incognito mode, it errors because Podia (the backend I use)'s widget uses cookies, and cross-site cookies are blocked by default in Chrome.\n\n![image](https://user-images.githubusercontent.com/6764957/88324929-fcc6f400-cd56-11ea-948e-a855205cbdf3.png)\n\nThe only way to fix this is to move off Podia, which I'm not very keen on, or to drop the Podia widget altogether and just directly link to the Podia sales page (slightly worse experience for the majority of visitors, in exchange for better experience for the small percentage that uses Incognito). \n\n## 2. Communicating Launch Sale\n\n![image](https://user-images.githubusercontent.com/6764957/88325284-7a8aff80-cd57-11ea-8cbc-3858a84ef5f4.png)\n\nI launched on Jul 1 and ended launch sales on Jul 14. After some people DMed me saying they missed the launch sale, I held out for a while and eventually relented by extending it to Aug 1. I didn't spend a lot of time on this but basically the objection is that the high number of hours hurts my credibility. \n\nI definitely don't intend to do a permasale as that is unethical, however, I do want to figure out the best price that the market pays for my book while still having most customers feel like they got value for money (I care about total revenue more than I care about per-unit prices - I tried to market like an upmarket brand that deserves a premium for a while, but unfortunately I don't think I know how to do that well).\n\nRecommmendation: go with days instead of long countdown timer.\n\n## 3. Duck Branding\n\nUse it more! everyone likes it!\n\nHis recommendations:\n\n- put duck near book reviews\n- and at footer\n- cheeky angle\n\nAdds personality!\n\n![image](https://user-images.githubusercontent.com/6764957/88326452-a35fc480-cd58-11ea-89e6-cc9f819fd269.png)\n\nMore Yellow Page inspo on his site: https://onepagelove.com/tag/yellow-color\n\n## 4. Hero Section \n\nBook Cover Too Small. Squinting to read the subtitle. Make it bigger.\n\n\"The Missing Manual\" text is more appealing than \"Build an Exceptional Career\".\n\nMove Login to top of the page, See Sample instead -> don't put you on Newsletter at all - send sample with friendly email.\n\nRemove Product Hunt widget.\n\nMake entire Hero section yellow?\n\nAdd 1 testimonial here.\n\n![image](https://user-images.githubusercontent.com/6764957/88344541-63a7d580-cd76-11ea-8125-bed042417511.png)\n\n\n\n## 5. What's in the book? section\n\nText hierarchy is \"out\" - headers should be darker than body text, I have the inverse when it comes to my bolded text.\n\nMake sample chapter links into buttons, more \"actionable\".\n\n![image](https://user-images.githubusercontent.com/6764957/88343291-ab792d80-cd73-11ea-802d-9614503a88a0.png)\n\n## 6. Book Reviews \n\nToo many!\n\nNeed to be more specific. Address doubts and features. 4-6 slam dunk ones.\n\nBring 1 testimonial up under the Hero section.\n\n![image](https://user-images.githubusercontent.com/6764957/88343759-b2547000-cd74-11ea-824d-d1b2367f00a3.png)\n\n## 7. Pricing section\n\ncreate left-to-right flow\n\n![image](https://user-images.githubusercontent.com/6764957/88343891-02333700-cd75-11ea-94b0-18745249587b.png)\n\n\n## 8. About the Author\n\nSAY WHY YOU WROTE THE BOOK\n\n![image](https://user-images.githubusercontent.com/6764957/88343990-3c9cd400-cd75-11ea-829b-286563e51448.png)\n\n---\n\nlots to chew on, as you can see! I figured I would transcribe the lessons here so I can save them for my future self. I'll be working on making recommended changes as I go. You're welcome to give more critiques of my page too!\n\n\nIf you enjoyed this you should probably buy his book prelaunch like I already have - [my affiliate link here](https://gumroad.com/a/942240883). :)"
  },
  {
    "slug": "8-q-a-s-for-pre-bootcamp-students-in-2020-1phh",
    "data": {
      "title": "8 Q&A's for Bootcamp Students in 2020",
      "description": "I did a Q&A for Fullstack Academy Bootcamp Prep students - copying out my answers here!",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nThis week I did a Q&A for Fullstack Academy's (FSA) [Bootcamp Prep](https://www.fullstackacademy.com/bootcamp-prep) students - copying out my answers here! This was [the bootcamp I went through 3 years ago](https://medium.com/hackernoon/no-zero-days-my-path-from-code-newbie-to-full-stack-developer-in-12-months-214122a8948f) so I'm always happy to give some time back to people going through the same career change I did.\n\n\n## Table of Contents\n\n- [What made you want to start writing a book about coding careers?](#what-made-you-want-to-start-writing-a-book-about-coding-careers)\n- [What is something you wish you knew during the bootcamp and right after graduation (while looking for a first job)?](#what-is-something-you-wish-you-knew-during-the-bootcamp-and-right-after-graduation-while-looking-for-a-first-job)\n- [Are there any podcasts/coding talks that you recommend listening to for beginners?](#are-there-any-podcastscoding-talks-that-you-recommend-listening-to-for-beginners)\n- [What was your (career) background before joining FSA? what made you choose FSA?](#what-was-your-career-background-before-joining-fsa-what-made-you-choose-fsa)\n- [In addition to the coursework at FSA, did you have to learn anything else (programming language, framework etc) to secure your first job at a programmer?](#in-addition-to-the-coursework-at-fsa--did-you-have-to-learn-anything-else--programming-language--framework-etc--to-secure-your-first-job-at-a-programmer)\n- [I understand you graduated from FSA in 2017? In last 3 years with increase in bootcamps and bootcamp grads, do you think industry's outlook towards bootcamps have changed?](#i-understand-you-graduated-from-fsa-in-2017-in-last-3-years-with-increase-in-bootcamps-and-bootcamp-grads-do-you-think-industrys-outlook-towards-bootcamps-have-changed)\n- [I was wondering what drew you in to apply for a job at AWS, what certs did you get and/or study tools did you use before applying, and how was the interview process?](#i-was-wondering-what-drew-you-in-to-apply-for-a-job-at-aws-what-certs-did-you-get-andor-study-tools-did-you-use-before-applying-and-how-was-the-interview-process)\n- [I really do want to work remotely, do you have any insights / recommendations for that route ?](#i-really-do-want-to-work-remotely-do-you-have-any-insights-recommendations-for-that-route-)\n\n---\n\n## Q&A\n\n### What made you want to start writing [a book about coding careers](https://learninpublic.org/)?\n\nIt was a couple of factors - i had 2 months off between jobs before i was going to join AWS, and then my most successful writing to date had been a career advice essay, so I went where my readers pointed me to go!\n\n---\n\n### What is something you wish you knew during the bootcamp and right after graduation (while looking for a first job)?\n\n- take it easy on algorithms, even Google doesnt have time to test you on writing heap sorts in their technical interview. Most of Cracking the Coding Career is not tested (at least at our level)\n- social pressure is helpful when jobhunting - there is a huge “Cliff” after graudation when people tend to slack off and veg out and dont know how to get into the grind of the job search. organize weekly standups with friends and report what you did and what you will do.\n- read technical books like YDKJS and go thru workshops like frontendmasters/eggheadio. solidify your fundamentals, there was a lot of rushing during the bootcamp\n- more: https://github.com/sw-yx/ama/issues/1\n\n---\n\n### Are there any podcasts/coding talks that you recommend listening to for beginners?\n\nYes: https://www.swyx.io/writing/fave-podcasts/#tech-webdevjs\n\nIn particular, check out the BaseCS and CodeNewbie podcasts - just go thru the back catalog! lots of good stuff! for career transition stories i liked the Freecodecamp and Breaking Into Startups podcasts as well.\n\n---\n\n### What was your (career) background before joining FSA? what made you choose FSA?\n\nI was in finance before FSA. I chose FSA by applying to a bunch of bootcamps (flatiron, hackreactor, GA) and picking the one with the hardest entrance exam. theory is, if the exam is hard, everyone i study with will also be high quality. also i think “exam” is the wrong word. more like a short technical interview.\n\n---\n\n### In addition to the coursework at FSA, did you have to learn anything else (programming language, framework etc) to secure your first job at a programmer?\n\nNo. plain and simple. i only needed to know html/js/css. i got my job from the very first person i sat down with at FSA’s hiring day. probably got lucky lol\n\n---\n\n### I understand you graduated from FSA in 2017? In last 3 years with increase in bootcamps and bootcamp grads, do you think industry's outlook towards bootcamps have changed?\n\nNo the industry’s outlook hasnt changed. if anything, its better, bc many bootcamp grads are sr devs now. i would say that the covid recession has made the post bootcamp job search a bit harder. but i still see jr devs getting first jobs all the time so its not 0.\n\n---\n\n### I was wondering what drew you in to apply for a job at AWS, what certs did you get and/or study tools did you use before applying, and how was the interview process?\n\nI applied for AWS because i knew my now-manager for 2 years and i wanted to experience the BigCo life. I didn’t get any certs, at all. interview process was mostly alright, except the final onsite day! a lot of tough questions. if you get there i’ll be happy to coach you thru it!\n\nfor more on my AWS story https://www.swyx.io/writing/hello-aws/#personal-note\n\n---\n\n### I really do want to work remotely, do you have any insights / recommendations for that route ?\n\nYes - contribute to open source, write a lot. when you work remotely, people have to trust that you can be self motivated (more than the average in-office worker) and that you can communicate very well (since you have to replace hallway/watercooler/casual meeting chats with slack/github/internal memos)\n\nIn particular contributing to open source demonstrates ability to take issues, clarify requirements, and execute independently, in a code setting.\n\nWhen you apply for these kinds of jobs with proof that you can already work well under a simulated “remote” setting then its much easier to say yes!\n\nas an example - this is something i just did for work, today https://github.com/aws-amplify/amplify-js/issues/6369. it has no difference at all from normal open source work, except that i get paid for it lol\n\n## End\n\nThat's it - if you're currently considering or going through a Bootcamp, feel free to leave further questions here or [@ me on Twitter](https://twitter.com/swyx) or [buy my book :)](https://learninpublic.org/). Good luck, this is a scary journey but you can do it!"
  },
  {
    "slug": "unofficial-vs-code-snippets-for-aws-amplify-1959",
    "data": {
      "title": "Unofficial VS Code Snippets for AWS Amplify",
      "description": "making my own vs code snippets helpers for working with AWS Amplify",
      "tag_list": [
        "aws",
        "vscode"
      ]
    },
    "content": "\nAWS Amplify is a [full stack serverless](https://www.oreilly.com/library/view/full-stack-serverless/9781492059882/) framework that helps frontend/mobile developers rapidly build with scalable AWS components. In the month-plus since [joining AWS](https://www.swyx.io/writing/hello-aws/) I have found myself needing to look up and copy and paste a bunch of code snippets from [the docs](https://docs.amplify.aws/cli). This is usually a sign that [there's something to automate](https://xkcd.com/1205/).\n\nAmplify must accommodate multiple frontend frameworks and platforms so we can't lean on convention to save code - so the next best thing is to write snippets! I use VS Code so I wrote this set of snippets for myself. This is my first time making a snippets extension (so I needed to read [the extension docs](https://code.visualstudio.com/api/working-with-extensions/publishing-extension) first), but it wasn't too hard to figure it out, especially when prototyping snippets with [easy-snippet, recommended by Scott Tolinski](https://twitter.com/stolinski/status/1285605384191913991?s=20).\n\n## The Snippets Extension\n\nYou can install it by looking for \"Amplify VSCode Snippets\" in the VS Code extensions tab, or [click here](https://marketplace.visualstudio.com/items?itemName=sw-yx.amplify-vscode-snippets).\n\n\n![image](https://user-images.githubusercontent.com/6764957/88311465-7b199b00-cd43-11ea-8623-e56769e24197.png)\n\n\nYou can look for a gif demo in the [GitHub repo](https://github.com/sw-yx/amplify-vscode-snippets) - it's 10mb so I'm not including it here for your bandwidth's sake.\n\n> Note: There is an [older extension](https://marketplace.visualstudio.com/items?itemName=aws-amplify.aws-amplify-vscode) where you can [find more snippets here](https://github.com/aws-amplify/amplify-js/wiki/VS-Code-Snippet-Extension#full-code-block-snippet-documentation).\n\n## Results\n\nWith these snippets, I was able to [livecode 2 CRUD apps in 1 hour](https://www.youtube.com/watch?v=0Js5O5a4bQ4) in the recent ThisDotLabs JavaScript marathon! I'm still not too practiced yet, so it's not something I'm that proud of, but I feel a lot more productive already with them! \n\nYou're welcome to [propose snippets or fork it for your own use](https://github.com/sw-yx/amplify-vscode-snippets)!"
  },
  {
    "slug": "release-automation",
    "data": {
      "title": "Semi-Automatic npm and GitHub Releases with `gh-release` and `auto-changelog`",
      "description": "A snippet I use all the time",
      "tag_list": [
        "javascript"
      ]
    },
    "content": "\nThis is a blogpost I've [been sitting on for over 1 yr](https://mobile.twitter.com/swyx/status/1118966159641067521) because I haven't had the time to really study all the approaches and decide on the best one.\n\nKent C. Dodds is notable for [setting up fully automated releases](https://egghead.io/lessons/javascript-automating-releases-with-semantic-release) for all his OSS libraries. I think that is good for high volumes of releases, but it does involve extra setup steps that may not be worth it for smaller, more casual libraries.\n\nI picked up this semi-automatic method from [Bret Comnes](http://twitter.com/bcomnes) at Netlify and I quite like it. Writing it down since I refer to it approximately once a month.\n\n## Steps\n\nGiven a library you want to automate releases of:\n\n```bash\nnpm i -D auto-changelog gh-release\n```\n\nadd npm scripts:\n\n```js\n{\n  \"scripts\": {\n     \"version\": \"auto-changelog -p --template keepachangelog && git add CHANGELOG.md\",\n     \"prepublishOnly\": \"git push && git push --tags && gh-release\"\n  }\n}\n```\n\nNow whenever you want to cut a new release:\n\n-  you run `npm version` (I have a shortcut for `npm version patch`) to bump the package version and generate the CHANGELOG.\n- You have the time to inspect and modify the changelog, then you can `npm publish` to push this new release to GitHub as well as to npm. \n\n## Why\n\nUsing GitHub releases is a nice best practice for people watching your repo but also if you have any downloadable artifacts people can grab it right off your GitHub.\n\nYou retain manual control of your CHANGELOG and your publish process, while automating the boring parts of pushing releases and generating the base changelog. Also no extra npm tokens or infrastructure need be setup to do this, it's all done on your own machine.\n\n## More advice\n\nI advise adding a `prepublish` script to run builds and tests as well - I have been caught a couple times publishing bad versions because I forgot these in some packages.\n\nThis is **NOT** the last word on how best to automate releases - this is just what works for me now, but I have a LONG list of other methods to investigate but not enough time to try them all out right now.\n\nRelated writing from me: [Best Practice Open Source Repo Setup](https://www.swyx.io/writing/oss-repo-setup/)\n\nHere's a list of release tech I have yet to try out but made notes of:\n\n- https://github.com/semantic-release/semantic-release \n- https://github.com/atlassian/changesets/ \n- https://github.com/release-drafter/release-drafter \n- https://github.com/intuit/auto (Orta writeup here: https://artsy.github.io/blog/2019/01/03/label-based-prs/)\n- https://github.com/algolia/shipjs\n- https://www.npmjs.com/package/release-it \n- lerna-changelog \n- https://github.com/beyonk-adventures/svelte-mapbox/tree/master/.github/workflows \n- https://github.com/infinitered/open-source/blob/master/Continuous-Deployment-Setup-NPM.md\n- Github actions for nightly deploys by Orta https://github.com/microsoft/TypeScript-Website/issues/130#issuecomment-663471663-permalink\n- more tips here: https://mobile.twitter.com/swyx/status/1118966159641067521\n- linting: https://blog.codonomics.com/2020/03/3-musketeers-eslint-husky-lint-staged.html"
  },
  {
    "slug": "coding-career-launch",
    "data": {
      "title": "Lessons and Regrets from My $25000 Launch",
      "description": "Reflections on the Coding Career book launch",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nICYMI, I launched [my first book](https://dev.to/swyx/launching-the-coding-career-handbook-3f43) yesterday! So now for what has become tradition - passing on lessons and regrets from new maker to aspiring makers! \n\nAs with basically everything I do, I am not an expert at this, I literally just did my first launch. So caveat reador! 😅\n\n> Note: you can see the fuller timeline of this project via [IndieHackers](https://www.indiehackers.com/product/cracking-the-coding-career/) and [Twitter](https://twitter.com/coding_career)\n\n> Note from 1 month later: Followup post published - 5 Q&A's https://dev.to/swyx/5-q-a-s-on-writing-and-selling-my-first-book-18d1\n\n## PART ONE - Reader Submitted Questions\n\nBefore writing this I asked what you wanted to know. Here are your answers:\n\n- **Sales**: $12k presales, $13k on launch day. ~600 units total, across all tiers, for an ASP of $42 (I priced 3 tiers: $39, $79, $199 at launch). Given that I sold ~200 units for $19, the higher tiers definitely helped move up the ASP, at the cost of promising future work from me. Separate post on that coming.\n- **Email List**: I didnt have a separate email list, and I didn't really push hard for my list before this. I just sent to my pre existing blog reader mailing list of about 800.\n- **Hours spent**: holy shit i didn't count haha. I almost don't wanna know. I probably worked on this 8-12 hours a day, almost daily, from April 11 to Jun 1. Let's call that 500 hours. Then from Jun 1 to Jul 1 I took about 2-3 weeks off and spent more time on my AWS job. so call that about 100 hours for a **total of 600 hours or a $42 hourly rate**. This hourly rate is, of course, inaccurate bc it will rise from here on out. (Decoupling input from output is the point of [Productizing Yourself](https://nav.al/productize-yourself), something I discuss in the book)\n- **Word Count**: 101,475 words, 584,875 characters. (automatically counted, so just call it 100k words)\n- **External Links**: 1419 external links to blogposts and talks and other resources in the book. I put the best of literally everything i've ever read or seen related to careers in here, it is meant for you to go down a rabbit hole.\n- **Getting Help with the Landing Page**: I can't take credit for this one - [Mahmoud volunteered the entire design](https://twitter.com/thisismahmoud_/status/1278406532267466752)!\n- **Doing the Audiobook**: Yes I am narrating. I pretty much just followed the top few Google results for \"how to audiobook\" - use Audacity (and [basic filters to fix quality](https://www.instructables.com/id/How-to-Improve-Vocal-Quality-in-Audacity/)), and record each chapter separately (so you have the option to stitch them together in future). I opted not to use any intro/outro music, nor use different voices for quoting. People like authors reading their own books. If you are selling via Amazon, you may want to keep 1:1 to your book to support [WhisperSync](https://www.amazon.com/gp/feature.html?ie=UTF8&docId=1000827761). However, I took advantage of the audio medium to offer extra commentary, and do verbal descriptions of diagrams which are probably pretty entertaining if I go back and listen to them again.\n- **Staying motivated and keeping yourself accountable**: Work in public. hehe, what else would you expect me to say?? I do think the emotional journey is something I need to share - I did have high highs and low lows - I explain more below.  But mostly it was highs. As for the lows - \"**if you are going through hell - keep going**.\"\n- **Balancing the launch with the new job**: Explained below.\n- **Podia vs Gumroad**: oof. tough. It's not clean cut - both have redeeming qualities. Gumroad has better customer acceptance, UX, payment options, discovery marketplace, and dashboard. Podia has better emailing, take rate, affiliate/coupon system, membership/webinar support, and a no-code frontend with good custom domain support (i had trouble with Gumroad's). I would still recommend Gumroad for people with smaller projects, but if you're looking to make >$40k you may wish to [try Podia](http://www.podia.com/?via=shawn-wang). Both are Good Enough that you can't go wrong picking either.\n- **Distribution channels**: HN is best for dev audience, but be prepared for harsh scrutiny from people who dont care who you are and have zero incentive to be nice to you and are suspicious of every person who speaks up in your defense. #5 on HN for 3 hours gets you about 20k hits. Twitter is second for traffic, but better for endorsements. I got about 2k hits. Product Hunt (PH) is third, maybe 200. I have 3k followers on Dev.to, but didnt get many hits, and I dont think my long form nonbeginner content is very popular on this platform. Reddit nonexistent (for me - for something like The Good Parts of AWS it is perfect for the AWS subreddit). \n- **Emotional journey of the whole process**: described below. Basically - April good, May goodish, early June sucked, late June was great!\n- **Getting Reviews**: I had prior relationships with most of my reviewers, but Scott Hanselman was almost cold (we had interacted once before, over some Twitter drama). There were others who I considered acquaintances but still felt like a HUGE ask to get them to review - I'd say half came through, the other half just ignored or just said no. **I wrote personalized requests for every single person I asked** - the colder, the more elaborate. This involved referencing their earlier podcasts and talks so that I could highlight how my book reflects things they already believe in. I didn't publish every review I got on my site, mostly bc I realized it wouldn't add anything to do too many. I'd love to thank them and acknowledge them in some way though, or have a dynamic way of displaying them based on the audience. (#retargeting #adtech #privacybegone?)\n\n### Launch Checklist\n\nAssorted small things that didnt fit in my narrative below but that i followed:\n\n- [remember og:image and metadata for site](https://www.swyx.io/writing/jamstack-og-images/)\n- [Friends don't let friends leave service workers on for launch day](https://twitter.com/swyx/status/1265324631101026312?s=20)! cachebusting is a pain!\n- But still run lighthouse anyway for easy perf/a11y wins. I enjoyed implementing the [lazyload](https://web.dev/codelab-use-lazysizes-to-lazyload-images/) script, highly recommend it.\n- give affiliate codes to friends\n- setup Teams product - let people buy in bulk, takes minutes but generates a few hundred extra in revenue\n- dry run the site - buy my own book\n- set up attribution analytics - where the traffic comes from, and what affiliate codes are being used\n\nI have more tips on my [Launch Cheatsheet](https://github.com/sw-yx/launch-cheatsheet/).\n\n### Regrets\n\n- Do NOT change domain providers/DNS settings 48 hours before launch. Have that all sorted WELL before \n- Do NOT say thanks to friends who comment on HN posts in support of you or your work. it looks coordinated/premeditated even when it is not. HN is paranoid - but it makes sense - this isn't Linkedin or PH. Play by their rules on their turf.\n- I should have set up extra tracking for clickthrough rates and frontend failure rates. It's not too late for me to implement [Frontend Observability](https://www.swyx.io/writing/frontend-observability/) - but i had no time to futz around with this before my launch\n- Get professional design for the book and for the site. Don't DIY. Randall Kanna uses [Design Pickle](https://designpickle.com/) and her books look gorgeous. I had to have [Mahmoud volunteer to help me ship a total redesign](https://twitter.com/Coding_Career/status/1278456010370244610?s=20) *during launch day* for it to get somewhere acceptable, and I definitely lost sales because my original site looked shitty and unprofessional.\n- **Hiring help: don't be hands off.** You need to be very clear what you want and to check in constantly. I was too hands off, and I didn't scope the project I hired for well. Fiverr sucked for me - both people I hired delivered extremely late - 1 not at all, the other was disappointing and unusable. But others have had success.\n- Audiobook - [CHECK YOUR POPS](https://www.neumann.com/homestudio/en/how-to-protect-your-microphone-against-pops). I had hours of work ruined because I spoke right into the mic instead of aside the mic. I don't like pop filters - I don't think they do enough - I just speak sideways to the mic. \n\n\n----\n\n## PART TWO - CHRONOLOGICAL\n\n\nNow for the long form narrative.\n\nGrab a drink, this is a long one!\n\n## April\n\nBeyond being an [Egghead.io Instructor](https://egghead.io/instructors/shawn-wang?af=95qfq1) I had never previously made money on any side project ever. \n\nI started writing on **Apr 11** after seeing this tweet from Daniel Vassallo:\n\n- [Twitter link](https://twitter.com/dvassallo/status/1248482980885983233?s=20)\n- DEV embed: {% twitter 1248482980885983233 %}\n\nI didn't know exactly what to write but I had a few ideas, so I let my audience choose:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/3c8yu4y5mjbrn4ysilco.png)\n\nDespite the polls saying I should write the React+TS book, the replies and DMs I got told me to do the nontechnical book. I figured replies and DMs were a better indicator of interest than poll taps 🤷‍♂️. Also I felt it'd be more challenging and interesting. (I wasn't wrong on that one...)\n\nI spent the first day *not* writing - I decided I wanted to presell the book, so I stressed out over picking a name, picture, and writing marketing copy. By the end of the first day I had [my marketing page up on Gumroad](https://gumroad.com/products/bAZJq/) and announced [the book on DEV](https://dev.to/swyx/i-m-writing-a-book-45a8) and [Twitter](https://mobile.twitter.com/swyx/status/1248664386262032384) (DEV embed {% twitter 1248664386262032384 %}). For v0.0.1 of the book, I just made an empty PDF promising it soonish:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/zsyfh08ggyjihbd4rima.png)\n\n(yes, thats a different, and worse, title. I would spend about 4-5 hours researching titles alone on that first day before settling on \"Cracking the Coding Career\")\n\nWhen setting goals I estimated it like such: If I took 2 weeks to complete it and I valued my time at $200k/yr, I would need about $7.7k to break even. This made sense because I had [just left Netlify](https://dev.to/swyx/farewell-netlify-1alo) and was expecting to join AWS at the end of April (I get more than that, but I felt $200k was a good annualized rate). So I set my goal at $8k for the project.\n\nI set the price at the top end of the range for ebooks, $39, but set the presale at half off, $19. As Daniel likes to say:\n\n> I know everyone likes to say \"charge more\", but I'd rather leave some money on the table than risk having a lot of people think that they overpaid.\n\nThis rang true to what I wanted to do. I want to play long term games - If my first launch has happy customers, it will make my second launch more successful, and so on. I don't need to maximize money short term - just charging *something* is \"Good Enough\".\n\nThe first day brought in about $2k on Gumroad, and on the second day I got another $1k from a friend buying in bulk to support my work (thanks Zell!!!). Together with a few stragglers, **I sold an empty PDF for about $4k in my first week**. Half of my goal!\n\nThat's the wrong way to look at it. That $4k wasn't mine - it was in my Paypal account, sure, but I hadn't earned it yet. In accounting terms the $4k cash would be classed as an asset, but I would also have an equal size liability - the promise to deliver the project to all these people!\n\nI started writing the book, and along the way kept adding topics I thought I wanted to talk about. I had a lot of fun - for example, I planned to release sample chapters like [How To Market Yourself](https://twitter.com/swyx/status/1249793388037025797?s=20) (it has been edited and updated since for the final book version), which was an early hit (netting me three [podcast](https://www.softwaresessions.com/episodes/learning-in-public/) [interviews](https://www.youtube.com/watch?v=QFHO2-8fGtM) [off the bat](https://www.youtube.com/watch?v=bcca0VCJe9Q)). \n\nHowever I realized that I was spending too much time editing articles released for free and not enough time writing paid articles. My [Tech Strategy](https://twitter.com/swyx/status/1253009793448460288?s=20) chapter release was a flop. \n\nEventually I ran out of time on the 2 weeks - on Apr 24 this is the Table of Contents I ended up with:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/tjitmm9dhnves9mq1pvu.png)\n\nI had run into the classic developer problem - scope creep. I started wanting to do a 2 week project, and ended with 50 chapters' worth of ideas! Reminds me of this tweet:\n\n> \"About 2.5 years ago, I estimated that my book on software estimation would be finished within 6 months. It is still nowhere near finished.\" - [Neil Killick](https://twitter.com/neil_killick/status/1261535908953600000?s=20)\n\nYeah. I definitely do not want this thing hanging around my neck for 2.5 years (i have seen it happen to other authors I was helping).\n\nSo Daniel advised me to drastically cut scope or release in parts. I did neither as I really did not feel comfortable charging for what I had made yet. So I asked for a 3 week extension. It also turned out that my AWS job would be delayed due to covid and visa issues, so it wasn't like I had anything better to do anyway. I ended up promising a new ship date of June 1st. Having delayed once already, I was less confident in myself, so I only announced this to presale buyers.\n\nI did, however ship v0.1.0 of the book, so that at least early buyers wouldn't feel like I had taken their money and ran off:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/0a6c4nt06wuzab4sejmh.png)\n\nYes, I had only 60 pages written in 2 weeks. And a lot more I wanted to cover. The first version was shipped using LeanPub - my goal was to support EPUB+PDF+MOBI with autogenerated Table of Contents out of the box, and LeanPub appeared to do that best with tight git integration. I tried some professional writing tools like Scrivener and Ulysses, but none of them supported that feature set (Ulysess ALMOST had it, but no Table of Contents! wtf! if you @ them they will tell you its on the roadmap, which they have been [saying for 7 years](https://twitter.com/swyx/status/1252315148062044162?s=20). (Remind me never to mislead users with technically-true replies like this.)\n\n> btw - [i did a poll later](https://twitter.com/swyx/status/1265301477049790464?s=20) and my instincts proved right - my audience is split 1/3 for each of the 3 big formats!\n\nI celebrated my birthday by writing all day and all night. Pro tip: don't take on a massive book writing challenge just before your birthday! lol\n\n## May\n\nWithin the extra 3 weeks I had all chapters planned and maybe about 60% written. \n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/o3leb1sbwj3277fqauwz.png)\n\nI fell into a good rhythm with the writing process - brainstorm during the day, especially while on my now-daily quarantine run/walks, note it down on my phone, and then at night I would turn it into long form prose. [More on my writing process here](https://www.swyx.io/writing/writing-mise-en-place/) as well as [my Reverse 9-5 schedule](https://dev.to/swyx/working-the-reverse-9-to-5-23li). I also found LeanPub unreliable, and dumped it for [Softcover](https://softcover.io), a nice Rails project which Michael Hartl uses to make [the Rails Tutorial](https://railstutorial.org). Good enough for him, good enough for me. I had to learn basic LaTeX though - [Overleaf](https://www.overleaf.com/learn/latex/Typesetting_quotations) and [the TeX Stack Exchange](https://tex.stackexchange.com/) were good for this. I did have a backup - [Dr Axel Rauschmayer](https://twitter.com/rauschma/status/1253767027761786880?s=20) recommended Pandoc and [Alex DeBrie](https://www.dynamodbbook.com/) recommended Asciidoc, they would have been my very next stops if Softcover had not worked out.\n\nNormal (unemployed) life for me went on while I did this. It was important for me that I keep up my other community responsibilities - I wrote unrelated pieces like [Cloud Distros](https://www.swyx.io/writing/cloud-distros), [the Third Age of JS](https://www.swyx.io/writing/js-third-age) and [My Life as a Con Man](https://www.swyx.io/writing/con-man-life), just to keep my readers warm instead of disappearing completely. I also continued [giving conference talks on Svelte and React](https://www.swyx.io/speaking/) - 6 events in April, 5 events in May. Yes, continued uncertainty about coronavirus and my job was stressful. I honestly wasn't even sure if my job would stick around for me, though I was secure enough that I knew I could probably find another job if I needed to. But I also was staring down the face of possibly having to become Very Serious about becoming a Full Time Author/Educator.\n\nThings picked up closer to the end of May. I decided I wanted to try out things I had always heard of but never had the chance to try. If I **treated the presales like a Kickstarter**, that meant that I suddenly had a $4k budget to spend on marketing and book quality! so:\n\n- I spent about $800 hiring people off Fiverr and my personal network to produce derivative works that I could sell as upgrades. None really worked out in the end.\n- [I spent](https://twitter.com/Coding_Career/status/1262897941464793089?s=20) about $2k getting the book professionally edited from Wordy.com, which Daniel used. They mostly corrected punctuation and casing, which I didn't feel added much value. They also charge by word, which was bad because I intentionally went for long form instead of concise. I eventually calculated that it would cost [between 10-15k](https://twitter.com/Coding_Career/status/1263492259401527301?s=20) to get the whole book edited. I ended up just pointing them at only the top 5 essays, and fortunately [Nat Sharpe and Joe Previte](https://twitter.com/Coding_Career/status/1265017707541311491?s=20) stepped in to help out with remaining edits, which was great because they actually cared about the subject matter. Greg Las also volunteered a few hours with me to [fix my horrible typesetting](https://twitter.com/Coding_Career/status/1263902045502705664?s=20).\n- I spent about $800 on [an annual Podia subscription](http://www.podia.com/?via=shawn-wang) as it seemed the better all-in-one platform with more stuff I could use in future like memberships and webinars.\n\nWith my target launch 2 weeks away, I [started a daily vlog to document my countdown to launch](https://twitter.com/Coding_Career/status/1262519814284664833?s=20). I figured working in public would help build interest and anticipation. I was feeling confident about my launch, so I also announced that [I would donate 25% of book sales to FreeCodeCamp](https://twitter.com/swyx/status/1263841899225350144?s=20). I didn't set any end date, but internally I hoped to match [my donation of $10k](https://twitter.com/swyx/status/1186320264969240579?s=20) last year which would imply at least $40k in sales. I started studying other launches and (of course) [put them into a cheatsheet](https://twitter.com/swyx/status/1262306425998467073?s=20) and figured this number was reasonable.\n\n> **In case you are keeping score - my target sales went from $8k to $40k!**\n\nWith 1 week to go and the book 99% done, I started asking friends for reviews and testimonials. Their honest and direct feedback **strongly** helped make the book better, in particular Jeff Escalante, Robin Wieruch, Samantha Bretous, and a few more mentioned in the acknowledgements. A few reviewers even went as far as a public tweet, but this one probably took the cake:\n\n> \"I’m reading through @swyx’s @Coding_Career and I’m genuinely impressed. It’s very dense but reads very lightly. Lots of useful context for someone coming into the industry.\" - *[Dan Abramov](https://twitter.com/dan_abramov/status/1265463366694440960?s=20)* \n\nDEV Embed: {% twitter 1265463366694440960 %}\n\nGeorge Floyd's murder happened on May 25. It took a while to trickle into my sphere, but by May 31 it was abundantly clear that launching on June 1st would be extremely tone-deaf and damaging. So with admittedly some reluctance, I called off the launch with 1 day to go, killing all the momentum I had been building up.\n\n## June\n\nI originally delayed things to June 8, but quickly pushed it further to July 1st. This was probably my lowest point, where I was even questioning the value of working on something like a tech career advice book when so much more serious issues were going on in the world. [Gayle McDowell](https://twitter.com/gayle), author of Cracking the Coding Interview, got in touch and said that my title was too close to hers and that infringed her trademark, so *I didn't even have a book name left*.\n\nIn other news, I [finally started at AWS](https://twitter.com/swyx/status/1266351781170171904) though! Book stuff took a back seat as I started work and onboarding, and continued giving more talks in a personal capacity. I also continued updating [my more-or-less weekly mailing list](https://tinyletter.com/swyx) with updates, to keep things warm despite not having anything to launch.\n\nEvery time I sent anything I'd get a wave of unsubscribes, which hurt slightly but I knew it was the right thing to do. People on my list were there because they liked my blog, and I never really put much effort into the basic stuff everyone says to do, like offer a [lead magnet](https://optinmonster.com/9-lead-magnets-to-increase-subscribers), so I only had about 800 subscribers despite [having about 1k unique visitors/day since blogging every day in January](https://twitter.com/swyx/status/1220372896448700418?s=200).\n\nThis period was a weird one - I was technically done with the book, but of course couldn't resist continuing to edit and adding more content. I did take about 2-3 weeks mostly off.\n\nSince I had spent all of April and May just writing, I hadn't really figured out the finer details of shipping. So I figured that I could spend June doing non-book stuff. [Joel Hooks](https://twitter.com/jhooks/status/1278357145411399680?s=20) is always going on about Ryan Delk's pricing formula (3 tiers: 1x, 2.2x, 5x) so I figured I should make extra stuff for people who wanted to pay me more. I wasn't happy with the Podia storefront, so [I setup a standalone page with Svelte and Begin](https://twitter.com/Coding_Career/status/1268647974541770753) and announced an audiobook version. For the middle tier I figured that I could sell community access (where I could put my moderating skills to work) and for the highest tier I could offer behind the scenes stuff and live workshops.\n\nRyan is silent on how to price for team/bulk purchases. I borrowed from [Philip Kiely](https://philipkiely.com/wfsd/) but kept it to teams of 5 incase team sizes got out of control - I mostly went for 2.5x the single unit price, minus  a few bucks, just to make it very clear it was good value if you were even *thinking* about doing a teams thing over a single unit thing. Again, not caring about whether I am maximizing short term money helped a lot here, I basically spent no time agonizing over this.\n\nCloser to the launch, things picked up again. I decided on a new name - The Coding Career Handbook, but bought a future proof domain - [LearninPublic.org](https://learninpublic.org/), where the book lives now and I have the option to expand into a bigger series and community.\n\nI wasted time working on my custom site design ([with my shitty but barely passable design skills](https://twitter.com/Coding_Career/status/1276721038743662592?s=20)) and worked on [recording the Audiobook](https://twitter.com/Coding_Career/status/1277042994554605568?s=20). In retrospect the site design was a total waste of time and **I should have paid a real designer money** to help me out with this. I nearly went too far cloning the [Refactoring UI](https://refactoringui.com/) design, but fortunately I checked in with Adam Wathan and he said that wasn't cool to do (I had NO idea that was a no-no - open source software doesnt translate to openly copyable design).\n\n## The Launch\n\nThis being the third intended launch date and having had time to sort out finer details, I felt a little bit better about this one. This is the rough plan I followed.\n\nI had been promising my mailing list to have the best, most significant discount. The day before the launch, I set up a 50% coupon and sent it out together with all my launch details, pretty much as [Adam spelled out in his talk](https://www.youtube.com/watch?v=ajrDxZRpP9M). This not only lets you get in some early sales (deeply discounted to reward your loyal readers ofc) but it gives your fans a way to support you by telling them to look out for your posts on the various social sites. I also upgraded all 200+ presale buyers to the middle, community tier, giving them $99 of value for the original $19 that they put down. The principle I follow here is to just **Spark Joy** in everyone who believed in me early on, so that they will help me spread via Word of Mouth and/or be a future customer because they trust me to take care of them no matter what.\n\nCloser to the launch I had a nightmare freakout. I had domain issues with Begin and Google Domains and decided to swap over to AWS Route 53 with less than 24 hours to go before launch - putting some risk that I might not have a functioning site for people to visit. Fortunately it passed in a few hours when it turned out that I had done almost everything correctly. However I did end up having to ship without having my naked domain setup - which I got a few comments about but couldn't do anything about. It resolved itself after 1 day.\n\nThe morning of the launch, this was the sequence I followed:\n\n- midnight-ish PST - [post on Product Hunt](https://www.producthunt.com/posts/the-coding-career-handbook)\n- 8amish EST - HN: https://news.ycombinator.com/item?id=23700486\n- 9amish EST - Twitter: https://twitter.com/swyx/status/1278305371610771456?s=20\n- 9amish EST - Dev.to: https://dev.to/swyx/launching-the-coding-career-handbook-3f43\n- I also tried Reddit but correctly anticipated that they would hate it. i also [updated Indie Hackers](https://www.indiehackers.com/product/cracking-the-coding-career/crossed-12k-in-sales-prelaunch--MB8Cbet3cBMMR-PDHNb) but it isnt a significant launch platform.\n\nI also [livestreamed the entire 2 hour EST launch process](https://www.youtube.com/watch?v=zRzYv9ZBoHI&feature=youtu.be): {% youtube zRzYv9ZBoHI %} - this would have been a MUCH bigger part of my launch plan if my original 14 day countdown momentum hadn't been broken, but I didn't feel too confident about this one so I also did not promote it as much. The original idea was that my launching would be like a TV show with a \"finale\" being a \"launch party\" where people could ask me anything - in reality I found the daily vlogging to be hard enough to ship as it was on top of the actual work I had to do. I think, with a longer timeline than 2-3 months, or a separate video production team, this would be a viable strategy, but it wasn't realistic for a solo maker. Lesson learned.\n\nProduct Hunt did well for a platform I have zero affinity for. My more clued-in friends definitely helped out, primarily [Monica](https://www.producthunt.com/@monicalent) [Robin](https://www.producthunt.com/@rwieruch) and [Joe](https://www.producthunt.com/@jsjoeio).\n\nTwitter was home turf, so I knew I'd be fine there. Still, crafting the announcement tweet was tricky. I had of course studied everyone's tweets, but stayed closest to [Alex DeBrie's](https://twitter.com/alexbdebrie/status/1247535794216001544?s=20) because it felt most like me. I got 144k impressions and 2.5k clickthroughs, with retweets from mentors and now friends like Sarah Drasner, Chris Coyier, Kent C Dodds, Axel Rauschmayer, Sacha Grief ([whom I interviewed WAY back](https://softwareengineeringdaily.com/2017/08/09/state-of-javascript-with-sacha-greif/) when I was just starting out in 2017, Jem Young and even my former bootcamp teacher [Cassio Zen](https://twitter.com/cassiozen/status/1278360592730988546?s=20)!! 😬\n\nHonestly the amount of support from friends I admire and have interacted with pretty much 99% via Twitter truly overwhelmed me. You are never poor if you have friends like these. **Friends are a form of true wealth**. I could lose my job tomorrow, my bank account could be wiped out, I could be marooned halfway around the world from my normal location. My friends would help sort me out somehow. I genuinely sleep better at night with this knowledge - a security I *never* had during my stressful finance days.\n\nHN was a wild card. I'd seen *plenty* of launches fail on HN. Mine fortunately went straight to the front page for a good 4 hours, and peaked at #5. Then dang killed it:\n\n![https://pbs.twimg.com/media/Eb29qGMU8AAeOIO?format=jpg&name=4096x4096](https://pbs.twimg.com/media/Eb29qGMU8AAeOIO?format=jpg&name=4096x4096)\n\nI thought it was from too many flags from people who didn't like that I was light on experience. It turned out that it was because **I had neglected to offer free chapters without an email signup**. This violates [the rules of Show HN](https://news.ycombinator.com/showhn.html). I had known that this was a good idea to do, but I was busy doing other stuff and I hadn't expected that this was a *disqualifying* thing or I wouldve prioritized it. I raced to fix it and put out 4 free chapters, and dang restored it, but by then the damage was done:\n\n![https://pbs.twimg.com/media/Eb7ZYvuVcAIdvxU?format=jpg&name=large](https://pbs.twimg.com/media/Eb7ZYvuVcAIdvxU?format=jpg&name=large)\n\nTo give you an idea of why this is important - per my privacy conserving handrolled shitty analytics, I got 10x the traffic from HN that I did from Twitter.\n\nThis is where I normally would post some chart porn, [like Emma did in her own recap post.](https://compiled.blog/blog/how-i-made-40000-dollars-on-a-book). Unfortunately Podia has zero charting available, compared to gumroad. I have all the raw data to construct the charts though, and may revisit this post in future to update for you. Anyway, here are my Podia numbers after about a day of sales:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/nl7vdf3dq3pdxzatdveb.png)\n\n**Together with my $8k non-Podia sales, that makes for a $25k+ launch.** Still under my $40-80k target, but this is still day 1.\n\nI'm not done. I see this as a LONG running thing. I want to publish a new version every year. I'm looking for and speaking with potential coauthors for future versions. I have been approached to make this in a fuller produced course. And of course more sales will come with word of mouth. In my view, **if my launch day is my biggest day, I am doing this wrong.**\n\nI'm not even done with launch week. I have been invited for more podcasts, but am focused on making existing customers happy. I screwed up the audio for the second half of my audiobook recording, and need to rerecord it. I also need to deliver the live workshops I have promised. And I also need to get back to work at AWS :)\n\nBut not before I ship One More Thing:\n\n![FreeCodeCamp donation](https://dev-to-uploads.s3.amazonaws.com/i/qg5rboa43vq6s87kl2pg.png)"
  },
  {
    "slug": "launching-coding-career",
    "data": {
      "title": "Launching the Coding Career Handbook!",
      "description": "I'm launching my career advice book today!",
      "tag_list": [
        "advice"
      ]
    },
    "content": "\nToday I'm excited to announce that [the Coding Career Handbook](https://www.learninpublic.org/?from=DEV.to) is now live!\n\n![Coding Career Site](https://dev-to-uploads.s3.amazonaws.com/i/vmna6l4qpmtuqti1nnr4.png)\n\n\nIt is packed with the best of my writing - 1/4 of the book is my best career advice blogposts like [Learn in Public](https://www.swyx.io/writing/learn-in-public/), rewritten and updated for 2020 - and the other 3/4 is never before seen stuff on Career/Business/Tech Strategy, and tactical advice on everything from Negotiation to Writing to Twitter!\n\nCheck out the Table of Contents here:\n\n[![Table of Contents](https://ph-files.imgix.net/0df61c25-ab38-4705-abac-e948130448e3.png?auto=format&auto=compress&codec=mozjpeg&cs=strip&w=602.82722513089&h=380&fit=max&dpr=2)](https://www.learninpublic.org/?from=DEV.to#TOC)\n\nThis is the follow up to my original announcement that [I am writing a book](https://dev.to/swyx/i-m-writing-a-book-45a8) - 2 months later than I originally expected, but I am so much prouder of this final output! The original announcement got $4k in presales, then [Dan Abramov's tweet](https://twitter.com/dan_abramov/status/1265463366694440960) doubled that, and I did another $3-4k in the leadup to the launch. This means that I far exceeded my original goal just in *presales* alone, hitting $11k before today!\n\nI hope to write up my writing process in a future post (pls ask me questions here so I can answer it in the post) but for now, I just wanted to share that the book is now live!"
  },
  {
    "slug": "amplify-console-branch-subdomains",
    "data": {
      "title": "Versioned Docs in 30 Seconds with Amplify Console's Branch Subdomains",
      "description": "Amplify Console just got the ability to create a custom subdomain for every new branch. This makes creating versioned docs a cinch!",
      "tag_list": [
        "aws",
        "inthirtyseconds"
      ]
    },
    "content": "\n> Amplify Console [just got the ability](https://aws.amazon.com/about-aws/whats-new/2020/06/amplify-console-adds-support-for-automatically-creating-deleting-custom-sub-domains-for-every-branch-deployment/) to create a custom subdomain for every new branch. This makes creating versioned docs a cinch! Let me try to explain it in 30 seconds.\n\nLet's say you have your docs hosted on an existing Amplify site ([get one here](https://docs.amplify.aws/start)) at `https://www.my-library.com/`. You can turn custom subdomain branch deployments (and automatic deletion) on in 3 clicks:\n\n![image](https://user-images.githubusercontent.com/6764957/85810347-5c98a200-b78d-11ea-9c75-3d75c470c34a.png)\n\nI have chosen to enable this feature just for branches that start with `v`, so `v1`, `v2.1`, `v2.0-alpha3`, etc all work, but a `feat/my-new-pr` or `patch-1` branch don't cause a build.\n\nNow, whenever I cut a new release, I can `git checkout -b v3` to create a new branch and push it up to GitHub. Amplify Console will see that and generate a permanent subdomain with that branch's name. Now I can keep developing on my main branch, but my docs for `v3` will always live at `https://v3.my-library.com/`! \n\n**That's it!!** Versioned docs are a highly demanded feature for open source libraries. Because this is based on git branches, I can move and delete files, and *even completely change site generators* without ever affecting my old docs. **Now you have no excuse not to have versioned docs!**\n\n---\n\nSee my example [11ty](http://11ty.io/) docs site here: \n\n- Main: https://main.d7jllnhqfi58b.amplifyapp.com/\n- v1.1: https://v1-1.d7jllnhqfi58b.amplifyapp.com/\n- v2: https://v2.d7jllnhqfi58b.amplifyapp.com/\n- v3: https://v3.d7jllnhqfi58b.amplifyapp.com/\n- Source: https://github.com/sw-yx/test-eleventy/\n\n\nYou can fork and deploy my example with this nifty button: \n[![amplifybutton](https://oneclick.amplifyapp.com/button.svg)](https://console.aws.amazon.com/amplify/home#/deploy?repo=https://github.com/sw-yx/test-eleventy)\n\n---\n\nP.S. Pro tip - you don't have to manually update the version numbers inside the docs yourself. Since I name the branches after the exact version number, I can make use of [Amplify Console's provided environment variables](https://docs.aws.amazon.com/amplify/latest/userguide/environment-variables.html#amplify-console-environment-variables) which give me the `AWS_BRANCH`, which I can use inside my docs site generator to always make sure that the docs reflect the accurate version!\n\nThis is how I set it up [in my demo project](https://github.com/sw-yx/test-eleventy/tree/main):\n\n![image](https://user-images.githubusercontent.com/6764957/85811429-b9e22280-b790-11ea-94bb-8c4787a4b47c.png)\n\nwhich becomes...\n\n![image](https://user-images.githubusercontent.com/6764957/85811265-36283600-b790-11ea-8832-1110e6d0d60e.png)\n\n\n"
  },
  {
    "slug": "rsi-tips",
    "data": {
      "title": "Notes on RSI for Developers Who Don't Have It (Yet)",
      "description": "I'm starting to feel some RSI in my left hand. It's a matter of time. I decided to collect some information about it to make improvements now rather than later.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\n## Basic Facts\n\n\n**What is RSI?** (from [Harvard RSI Action](http://www.rsi.deas.harvard.edu/what_is.html))\n\n> RSI stands for Repetitive Strain Injury. It includes a whole spectrum of conditions, from tendonitis of the hand or wrist to carpal tunnel syndrome to cubital tunnel syndrome. Basically, if your hands or wrists hurt or go numb or tingle, that may be RSI. If these symptoms are associated with repetitive tasks, such as typing at a computer, it is almost certainly RSI. (Note that such symptoms do sometimes go away within a few minutes, hours, days, or weeks.) Other symptoms include losing strength or coordination in your hands, or perhaps dropping things.\n>\n> Not everyone experiences all symptoms, and sometimes your symptoms may not occur until several hours or even days after the activity which causes them. (Have you ever hiked a long way and not been really sore until the next day or even two days later?) Some people, especially at UHS, call RSIs Cumulative Trauma Disorders (CTDs); it's the same thing.\n\n- RSI seems to have many causes. Everyone's RSI is different. The cause can be equipment, but contributing factors can be broader (overall body health like sleep/head/neck/back posture factors, psychological factors). Sometimes you might have RSI symptoms, but it might actually NOT be RSI!\n- Pronating wrists (turning face down) is bad - therefore the default mouse and keyboard alignment is bad! 😱\n- Tilting wrists up is bad - the normal up-sloping keyboard is bad. It should be exactly the opposite: ![image](https://user-images.githubusercontent.com/6764957/85174576-31ebac80-b2a8-11ea-913d-5efdfc4fb9e7.png)\n- Arm posture/lumbar/upper back support is important.\n- Constant pressure in one position is a cause - [vary it up](https://news.ycombinator.com/item?id=4482445) and take frequent breaks\n- Long key travel (up/down) is bad.\n- Long key travel (sideways) is bad therefore QWERTY is bad. \n- Multiple pressure at the same time is bad, therefore Ctrl+C/S/V with one hand is bad.\n- Sometimes the cause is **psychosomatic** - subconscious stress.\n- Exercise helps sometimes - pushups, pullups, dips, and situps. Not too much, though. Sleep too. But it doesn't always fix RSI.\n- It is never \"cured\", just managed. Know what triggers your RSI.\n\n\n## Basic/Cheap Recommendations\n\n- Don't use default Apple laptop keyboard\n- [Split Keyboards?](https://sea.pcmag.com/keyboards/37648/the-best-ergonomic-keyboards-for-2020)? Mechanical Keyboards? [Microsoft Sculpt Keyboard](https://www.amazon.com/dp/B00CYX26BC) or [Natural keyboard](https://www.microsoft.com/accessories/en-us/products/keyboards/natural-ergonomic-keyboard-4000/b2m-00012)\n- Map Capslock to Ctrl, or [turn on Sticky Keys](https://news.ycombinator.com/item?id=17133275). **Ctrl+C/V/S combinations are bad for hands.** You can do this via Settings > Keyboard > Modifier keys on a Mac.\n   > Note for me I think my problem is the left shift key. I am experimenting with mapping Capslock to Shift, or trying to use Right shift\n\n- Use right hand for modifier keys like ctrl, cmd, alt, opt keys\n    ![image](https://user-images.githubusercontent.com/6764957/85171614-ec2be580-b2a1-11ea-98ad-e3fb5dc4dbb4.png)\n- Have multiple keyboards, switch every 10 minutes (seriously!) or [twice a year](https://news.ycombinator.com/item?id=17134537)\n- [Hand Stretches](https://www.youtube.com/watch?v=TSrfB7JIzxY), [Nerve flossing](https://www.healthline.com/health/nerve-flossing#carpal-tunnel-syndrome),  [physical therapy stretches](https://www.youtube.com/watch?v=xAKOpnqvttI) and the prayer stretch:\n    ![image](https://pbs.twimg.com/media/Ea8F96bU0AIBq4t?format=jpg&name=900x900)\n- [Put your hands in warm water for 15 minutes](https://news.ycombinator.com/item?id=23577204) or icing: \n\n    > Mitigate inflammation via: Ice and Naproxen... icing is incredible. I've found that 5 minutes of ice will buy me 30 minutes of coding... nice perpetual motion machine when it comes to RSI. Naproxen is nice because it's like slow-release ibuprofen, so battles inflammation while you sleep\n- Mice: the [Evoluent VerticalMouse](https://evoluent.com/) is $90\n- [Hand Grip Exerciser](https://www.amazon.com/dp/B01AJ6HZLG?psc=1), [Finger stretchers](https://www.amazon.com/PROCIRCLE-Finger-Stretcher-Resistance-Assorted/dp/B00MA8M454/), [Gyroscopic exercisers](https://en.wikipedia.org/wiki/Gyroscopic_exercise_tool)\n- Tuck thumbs under fingers? [Book by concert pianist](http://www.kogosowski.com/product/prevent-rsi-ebook/)\n- Rest mouse arm on a raised surface (eg textbook) so your hand drops down rather than pushed up.\n- [Comfortbead Wrist Rest](https://www.amazon.com/Allsop-Comfortbead-Wrist-Rest-Keyboard/dp/B000XV16LS)\n- Proper Computer Chair: Feet touches the floor, good lumbar support, arm supports, locked seat back, upper back support. \n- Take breaks every 10-45mins - apparently you can [use Workrave](https://sourceforge.net/projects/workrave/) (or AntiRSI for Mac or [other software](https://henrikwarne.com/2012/02/26/mac-os-x-break-programs-review/)) to enforce breaks.\n- Cut all recreational computer use!\n\nMore: http://www.rsi.deas.harvard.edu/preventing.html\n\n## Expensive/Extreme Recommendations\n\n- Trackball mouse like [this](http://www.amazon.com/One-Finger-Mouse-USB-Black/dp/B000BSLTIS?ie=UTF8&tag=leoe-20&link_code=btl&camp=213689&creative=392969) or [this](http://www.amazon.com/Logitech-910-000806-Trackman-Marble-Mouse/dp/B001F42MKG?ie=UTF8&tag=leoe-20&link_code=btl&camp=213689&creative=392969) or [this](https://www.kensington.com/p/products/ergonomic-desk-accessorries/ergonomic-input-devices/slimblade-trackball/)\n- Super Ergonomic Keyboard: like the [Keyboardio Model 01](https://shop.keyboard.io/products/model-01-keyboard), or \"tented\" keyboards like the [ErgoDox](https://ergodox-ez.com/), or even more extreme, [Kinesis](https://www.amazon.sg/Kinesis-Advantage2-Ergonomic-Keyboard-KB600LFQ/dp/B07K1SMRGS) or the [SafeType vertical keyboard](https://www.allthingsergo.com/safetype-keyboard-review/). You can make the Model01 keyboard \"hang\" (thanks [Tre](https://twitter.com/TreTuna/status/1274124634762276864)): ![https://pbs.twimg.com/media/Ea6ZDK3U4AIwZXd?format=jpg&name=medium](https://pbs.twimg.com/media/Ea6ZDK3U4AIwZXd?format=jpg&name=medium)\n- [Articulating Keyboard Tray](https://www.amazon.com/Bush-Business-Furniture-Articulating-Keyboard/dp/B0000C0XP5) so you can tilt it back\n- Adjustable Height Desk\n- Aeron chairs are well regarded here ($300ish)\n- Alternative Keyboard mapping: Dvorak or colemac or QFMLWY or [carPalX](http://mkweb.bcgsc.ca/carpalx/). Fingers [travel 1/3 as much under Dvorak](https://news.ycombinator.com/item?id=12989700).\n- Wearing [Wrist braces](https://www.amazon.com/ACE-Wrist-Splint-Support-Medium/dp/B001ACU1MY?ie=UTF8&tag=leoe-20&link_code=btl&camp=213689&creative=392969) or [Soft braces](https://www.amazon.com/BRACE-RCA-Treatment-Support-Regular/dp/B00PFUPRQ2) even when not typing (eg [during sleep](https://news.ycombinator.com/item?id=12991157) - so you [don't curl your wrists during sleep](https://news.ycombinator.com/item?id=4481423))\n- Speech Recognition:  Dragon NaturallySpeaking, [Talon Voice](https://www.gcppodcast.com/post/episode-223-voice-coding-with-emily-shea-and-ryan-hileman/) for Voice Coding (more [here](https://github.com/melling/ErgonomicNotes/blob/master/programming_by_voice.md)).\n- Stenography: [Plover](https://www.youtube.com/watch?v=Wpv-Qb-dB6g) - 240wpm\n- Chondroitin / glucosamine / MSM to promote collagen repair and reduce swelling. Take for about 3 months.\n\n## Treatments\n\n- http://www.rsi.deas.harvard.edu/resources.html\n- Dr. John Sarno's Mind/Body approach https://www.amazon.com/Mindbody-Prescription-Healing-Body-Pain/dp/0446675156 ([endorsed](https://aaroniba.net/how-i-cured-my-rsi-pain)) and [the Divided Mind](http://www.amazon.com/Divided-Mind-Epidemic-Mindbody-Disorders/dp/0061174300/) ([endorsed](https://blog.evanweaver.com/2012/09/05/a-programmers-guide-to-healing-rsi/))\n- [Hand warmers could work!](https://codewithoutrules.com/2016/11/18/rsi-solution/) - [esports gamers use them](https://www.businessinsider.com/esports-hand-warmers-2015-8). [Arm warmers](https://www.rei.com/product/119280/pearl-izumi-elite-thermal-arm-warmers) too\n- https://www.amazon.com/Trigger-Point-Therapy-Workbook-Self-Treatment/dp/1572243759 ([multiple endorsements](https://news.ycombinator.com/item?id=12989707))\n- Heating Hands: https://www.amazon.com/UTK-Infrared-Electric-Therapy-19-Inch/dp/B013S7KGUU/\n\nIf really bad, [see a physiotherapist](https://news.ycombinator.com/item?id=1270047).\n\n## More Resources and Blogposts\n\n- Harvard RSI Action http://www.rsi.deas.harvard.edu/\n- More RSI notes people have tried https://github.com/melling/ErgonomicNotes\n- http://blog.chromarati.com/2010/04/rsi-my-problem-child-10-tips-for.html\n- https://news.ycombinator.com/item?id=12986759\n- https://codewithoutrules.com/2016/11/18/rsi-solution/\n- https://aaroniba.net/how-i-cured-my-rsi-pain\n- https://blog.evanweaver.com/2012/09/05/a-programmers-guide-to-healing-rsi/\n- http://podolsky.everybody.org/rsi/\n- https://jmulholland.com/rsi-as-a-developer/ (HN https://news.ycombinator.com/item?id=20847822)\n- https://news.ycombinator.com/item?id=1638567\n- (older book): https://www.amazon.com/Repetitive-Strain-Injury-Computer-Users/dp/0471595330/ref=pd_sim_b_3\n- more stretches ![image](https://user-images.githubusercontent.com/6764957/85393993-7ca74600-b580-11ea-8341-164aca2cafb0.png)\n\n> \"I've been battling RSI for 20+ years. Can recommend getting standard blood tests. I was extremely low in vitamin D. Once that was fixed, one whole layer of pain fell away.\" - [Matthew Taylor](https://twitter.com/NeedsMoreFunny/status/1300891828330229760?s=20)\n\n\n## My Context\n\n\nI'm starting to feel some [Repetitive Strain Injury ](https://www.healthline.com/health/repetitive-strain-injury) (RSI) in my left hand. A few weeks ago it was so intense I had to stop typing for a weekend. Fortunately I am right handed, so I could still do most other things well. I gave it some rest and then I made a full recovery. I'm feeling the beginnings of it again today. It's a matter of time until I get hit by it again and don't recover. \n\nI decided to collect some information about it to make improvements now rather than later. If I intend to write as much code and words as I do for the rest of my life I better do this right. People have been forced to quit programming over stuff like this.\n\nI'm not an expert here, just collecting notes and sharing it in public. I will update this over time as I learn more, **please comment or [@me](https://twitter.com/swyx) and add more info and I will curate**.\n"
  },
  {
    "slug": "hey-for-good",
    "data": {
      "title": "#HeyForGood",
      "description": "Recounting my process of how I started with 2 HEY email invites and ended up raising $2575 in Donations to Support Diversity in Tech!",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nYesterday I got [my HEY invite](https://techcrunch.com/2020/06/16/basecamp-launches-hey-a-hosted-email-service-for-neat-freaks/) as part of [Jason Fried's 1984 batch](https://twitter.com/jasonfried/status/1273344047034351616) of invites:\n\n![image](https://user-images.githubusercontent.com/6764957/85070794-05249000-b1e9-11ea-8442-9287b5483b8b.png)\n\nI of course went and snagged `swyx@hey.com` (*holla at ya boi!*) but then it informed me that my invite code was good for 2 other uses by friends and family!\n\nI hadn't known the invites were going to work like this, but I'd seen people [auctioning HEY invites](https://twitter.com/PierreDeWulf/status/1273633462700908544) for $50-$100 bucks. I reckoned my invites would probably be good for at least $10-$100 bucks since its basically just a spot in a waitlist. It'd be interesting to trade in this literal privilege to help out good causes – for me, I'm really interested in helping out POC in Tech and [helping people learn to code for free](https://www.swyx.io/writing/donating-to-freecodecamp/).\n\nSo I tweeted out a fun contest:\n\n[![image](https://user-images.githubusercontent.com/6764957/85071410-102bf000-b1ea-11ea-9556-778064ef305d.png)](https://twitter.com/swyx/status/1273349447716532224)\n\nNotice how I set expectations to give them some value and also added a time limit (so that I wouldn't go mad managing the silent auction).\n\nWithin minutes I had 4 bids come in ranging between $50-$100. People were both generous and wanted a HEY invite badly!\n\nHaving raised $275 that quickly it was clear that the final winning bid was going to be well above $100. So I wanted to set the next level in terms of expectations. [I thought it'd be nice](https://twitter.com/swyx/status/1273355134274162688) to raise $1984 for charity:\n\n![image](https://user-images.githubusercontent.com/6764957/85071699-79abfe80-b1ea-11ea-88d9-75aba29cd0cb.png)\n\nJust saying numbers sometimes speaks them into existence. This is also true for negotiations (a topic I cover in [my upcoming book](https://twitter.com/coding_career)) I went to bed and overnight I got 4 more bids bringing the total to about $1k. This is where I set the next level noting that [eBay bids went up to $355](https://twitter.com/swyx/status/1273635870227103752?s=20).\n\nWith an hour to go in the 24 hour period, I did a couple last calls. [Emma Bostian](https://twitter.com/EmmaBostian/status/1273698551835643904?s=20) had the great idea of offering invites to people of color for free, and two folks jumped in to offer their invites for this great cause! I tweeted them out and it was claimed in minutes. It makes sense, no point raising money for diversity in tech and then ending up with the HEY userbase being full of rich white/Asian guys.\n\nThe final hour got pretty nuts. Top bid was $300 by then. I basically had 3 guys who were willing to outbid whatever anyone else bid. You see how this can be awkward with only 2 tickets? Anyway I played auctioneer for a tense hour. One guy went to $325, then I would relay that to the other two. They both countered by going up to $400 independently. I thought that was it, I didn't really want to go back and forth too many times 😂 \n\nThen last minute out of NOWHERE a new guy came in with $500. That would mess me up completely because now I had two guys tied at #2 place for the ticket. The way I resolved it was to relay this info to the person at $325, who immediately responded by pushing it up to $525 so that I now had a clear #1 and #2 bidder. By this time I was well out of time and the bidding period had ended, so that would leave both guys with $400 out of luck. Fortunately, I had one last friend offer their invites. I offered it to them if they threw in a final $100 each. Both accepted!\n\n## Summary\n\nAt the end of the day, **we were able to raise $2575 to support Black Girls Code, Hack the Hood, Black Tech Mecca, Code 2040, All Star Code, and /dev/color**.\n\n![image](https://user-images.githubusercontent.com/6764957/85073631-b4636600-b1ed-11ea-9a75-71d48ea9d41e.png)\n\nThank you to (in no particular order) [Jordan Gensler](https://twitter.com/VapeJuiceJordan), [Faraz Ahmad](https://twitter.com/farazamiruddin), `anonymous`, [Shane Deconinck](https://twitter.com/ShaneDeconinck) and everyone else who chipped in for a good cause!\n\n## #HeyForGood\n\nIf you have HEY invites too and would like to do what I did, consider not selling your invites and instead running a competitive donation process for nonprofits you like. Give people a shopping list like you do a wedding registry or Amazon wishlist!\n\n[Here's my list](https://twitter.com/swyx/status/1267545334294564864) together with their donation links:\n\n- [@BlackGirlsCode](https://twitter.com/BlackGirlsCode): https://blackgirlscode.com/donations.html\n- [@devcolororg](https://twitter.com/devcolororg): https://secure.actblue.com/donate/devcolor\n- [@blacktechmecca](https://twitter.com/blacktechmecca): https://blacktechmecca.org/donate-1\n- [@AllStarCode](https://twitter.com/AllStarCode): https://allstarcode.org/donate/\n- [@LatinaGeeks](https://twitter.com/LatinaGeeks): https://latinageeks.com/donate/\n- [@hackthehood](https://twitter.com/hackthehood): https://hackthehood.org/donate.html\n- [@Code2040](https://twitter.com/Code2040): https://code2040-2020.funraise.org\n- [@i_amthecode](https://twitter.com/i_amthecode): https://iamthecode.org/contribute/\n"
  },
  {
    "slug": "con-man-life",
    "data": {
      "title": "My Life as a Con Man",
      "description": "Confidence is a dual edged sword. I trafficked in confidence when I was in finance, and now I see it everywhere I look.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\n## First, My Story\n\n\"How confident are you?\" \n\nMy boss would often ask this question of me, his eyes trying to pierce through mine into the cloud of self-doubt beyond.\n\nBefore I was a developer, I was an equities analyst in a [market neutral hedge fund](https://en.wikipedia.org/wiki/Market_neutral). If that sounds like a lot of gobbledygook, just think of it as the purest form of stock market trading possible. Regardless of the direction of the market that day, my job was to come up with stock and options trading ideas to buy low, sell high, and make money. Not necessarily in that order. My boss, the portfolio manager, would assess these ideas and implement them in our portfolio according to how much he agreed with us.\n\nI would always stall on the confidence question. Most stock pitches take the same shape - a list of 4-6 key reasons why the trade might go well, and a shorter list of 2-3 reasons why it might not. Any intellectually honest analyst should be able to argue *both* sides of an investment thesis, and be able to change their minds when the facts change. \n\nBut if the facts were obviously one-sided, then others in the market would realize it too, and the payoffs would adjust to account for that fact. No free lunch, right? \n\nSo you'd ~~never~~ rarely get a \"90% sure thing\", you were often pitching 60%-40% reward to risk probabilities (I am grossly oversimplifying here). This is a problem when your [confidence intervals](https://en.wikipedia.org/wiki/Confidence_interval) on your confidence exceed 10%.\n\nMy job title was Analyst, I viewed myself as an analyst – an analyst's job was to be as right as possible, to be a source of truth. So I gave balanced views, reasonable estimates, and admitted all my blind spots.\n\nMy boss, the portfolio manager, weighed that strongly against my confidence. You'd do the same thing! If an advisor of yours made a recommendation, but didn't seem very sure about it, you'd weight it less than someone who had a clear message on what to do. Watch Ray Dalio call it [believability weighting](https://www.linkedin.com/pulse/work-principle-5-believability-weight-your-decision-making-ray-dalio) in his [TED talk](https://www.ted.com/talks/ray_dalio_how_to_build_a_company_where_the_best_ideas_win/transcript?language=en).\n\nMy coworker, G, (also an analyst) was exactly that kind of person. G would make absurd extrapolations from unproven datapoints. He'd wildly exaggerate potential profits, and diminish any risks. Any data point that remotely supported the thesis got thrown into the pitch, quantified or not. The more bullet points the better. Anything more equivocal in nature got thrown out. \n\nG did everything you're not supposed to do in quantitative trading. He'd backtest a massive battery of strategies and only pitch things that had worked enormously well in hindsight. He'd throw arbitrary weights on statistically insignificant data because the story fit better. He'd muck with chart axes to make divergences seem bigger than they really were and pick time windows that looked the most favorable. Financial models, if at all done, were just used to reinforce a preconceived idea.\n\nAnd then he'd get up in front of the room and deliver the most bombastic, hyped-up pitch of a trade idea of anyone in our firm. He was a champion debater in his youth – he was quick on his feet and had a counter to every criticism, a riposte to every response. Above all, he was **confident**.\n\nHe knew it was intellectually dishonest.\n\nBut he also knew it didn't matter. He knew we knew. And that we'd let him do it... as long as he made money.\n\nIt shouldn't surprise you that his pitches were accepted and traded in far bigger size compared to mine, despite us having the same tenure. His blowups were huge, but his wins were *massive* too. In our first year he carried the team, and got probably 1-2 million for his troubles. By our second year he was dictating his own terms, and for all intents and purposes left to start trading his own portfolio in a different office. A superstar in the making. He'd wind up in circles of industry legends like Paul Tudor Jones and Peter Thiel, and we lost touch.\n\nI'd try to imitate him, of course, since he'd been so effective. I spent less time in the details and more in the big picture. More time on confident persuasion rather than collecting information. My heart wasn't in it. Every time my boss asked: \"How confident are you?\" I'd break eye contact. I'd assert confidence, and then see my recommendation basically get ignored when my boss saw right through me.\n\nEvery time I put my reputation on the line I also put my job on the line. Eventually the stress got to me and [I burned out](https://www.freecodecamp.org/news/shawn-wang-podcast-interview/).\n\n## Realizations\n\nWith the distance of a few years I now know a few things I didn't then.\n\nThe institutional investment business suffers from the classic [principal-agent problem](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem). You took a percentage of profits on the upside, but none of the losses if you lost. Furthermore, if you were a *young analyst* and put up stupendous numbers early on, you would be viewed as a rockstar and have an accelerated career. Nobody was interested in \"average\" or \"just fine\". War stories of huge wins and terrible losses were *way* more interesting than slow and steady gains in-line with peers.\n\nIt turns out that it is easier to **act** like you have found a great trade idea than it is to find a truly overlooked idea, especially when you have to do it on a schedule.\n\nMy mistake was viewing my job as trying to be as right as possible. G played a different game. He understood asymmetric tradeoffs not just on a *per-trade* level, but also at a *career* level. I played play the game by its written rules, G played it by its unwritten ones. From a nihilist, cynical perspective, if you viewed this whole game as a series of coin tosses, I was the shmuck trying to perfect the theoretical expected value of coin tossing. G went and found the biggest heaviest coin he liked, tossed it a couple times, and used the momentum from his wins to start a bank. Don't get me wrong – he was right more often than not. But so was I, to a lesser extent. There was skill mixed with luck. The difference was he *really* knew how to leverage the shit out of his situation and make the most out of his existing skills.\n\nMy other posthoc realization is that I had a terrible boss, and by extension was in a terrible firm. It was irresponsible to let a confident employee effectively take over the team portfolio on intellectually dishonest grounds. He should not have delegated his judgment to meta-judgment of his own analysts rather than objectively assess the facts as presented. It might be evident to you, with the benefit of the narrative I laid out, but trust me that you wouldn't have any idea that this is not ideal as a first year buyside analyst who was just happy to be there.\n\n## Trafficking in Confidence\n\nA hedge fund is paid a LOT to take risk. Otherwise why would you invest in a hedge fund at all? Just stick everything in an index ETF and call it a day. Our *real* job was to manufacture confidence for taking risk. \n\n**We weren't stock analysts. We were Confidence Men.**\n\nThe truth is, I suspect that how our fund worked is how most of the world works. \n\nNow that I am out of finance I still see this pattern everywhere in life. The people who shout the loudest get the most attention on social media. Entire mobs converge on the absolute correct policy prescription with anecdotal information and no controlled tests (and of course, no responsibility to follow through on implementation). Politicians and businesspeople respond in kind. \n\n> **NOTE: THIS IS *NOT* A CRITIQUE OF CURRENT AFFAIRS**. This is just an observation of human behavior that has been in my mind for >5 years.\n\nFinance is rife with Con Men. Finance Con Men know the price of everything and the value of nothing. You are being fed empty confidence whenever a stock ticker rolls across your TV screen with percentage changes quoted down to two decimal places. In the leadup to the 2008 crisis, Con Men conned other Con Men with AAA-rated CDO's, once a convenient lie in the [Li Copula](https://www.wired.com/2009/02/wp-quant/) was found that gave dumber Con Men confidence. Your own financial advisers have to give you confidence they do not have, because you pay them to.\n\nI think trafficking in confidence happens at much smaller scales too. VC's who extolled the virtues of remote work insisted on meeting entrepreneurs in person before funding them, presumably to gain confidence based on a firm handshake and a healthy dose of ~~implicit bias~~ pattern matching. Overloaded managers ask their direct reports to make estimates and keep up metrics without really caring whether those estimates and metrics are the right ones. Employee OKRs become mechanisms for translating [Principal-Agent problems](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) into [Goodhart's Law](https://www.swyx.io/writing/eponymous-laws/). Even we in our personal capacities instinctively prejudice and change our assessment of products, companies, even *ahem* blogposts based on social proof: likes, retweets, endorsements, testimonials. \n\nAll of these are tools of the trade of the Con Man.\n\nWhen you really think about it, we are *addicted* to confidence. We view the less confident as weak and the more confident as leaders. We see confident people as more attractive and employ all sorts of tricks to make ourselves look more confident. We encourage people to \"fake it til you make it\". \n\nI view this as a modern malaise because we've made it so much easier to engender confidence without substance. Yet our reliance on confidence itself has always been a necessary evil of our existence. \n\nHumans aren't anything special in this regard. The concept of [mimicry](https://en.wikipedia.org/wiki/Mimicry) in nature describes this process. At the beginning, poisonous snakes might evolve some distinctive markings. So predators learned to avoid snakes with those markings. Then, non-poisonous snakes also evolved those markings, because it turned out that those features also helped with survival. Every marking, every behavior, every indicator that has signal eventually becomes noise. I can no longer find it, but there was a excellent essay going around in the aftermath of the 2008 crisis applying [Rene Girard's ideas of human mimesis](https://en.wikipedia.org/wiki/Ren%C3%A9_Girard#Mimetic_desire) to how the ratings agencies were tricked into approving high risk investments for AAA ratings by financially engineering according to their publicly disclosed criteria. This goes back as far back as the 1500's, when this was known as [Gresham's law](https://en.wikipedia.org/wiki/Gresham%27s_law).\n\nIt's all a Confidence shell game, and you're both playing it and being played by it.\n\nI wish I had a prescriptive four step plan to fix it for you, but I'm afraid I don't. The best I can do is to make you aware of it. I'm not confident anything can be done more than that. If that makes you like this piece less, perhaps that is lesson one :)\n\n**If the devil is in the details, the greatest trick the devil ever pulled was convincing the world that details don't matter.**\n\n![Alt Text](https://media1.tenor.com/images/e3b35108d870de8fc57773a778c46c4c/tenor.gif)\n\n> Besides what you see, I have confidence in me! *- [Julie Andrews, in the Sound of Music](https://www.youtube.com/watch?v=RV-6qbUHVww) – not really related, it's just the only \"confidence\" related song I can think of*\n"
  },
  {
    "slug": "4-q-a-s-on-blogging-for-developers-lc9",
    "data": {
      "title": "4 Q&A's on Blogging for Developers",
      "description": "Answering Q&A's",
      "tag_list": [
        "advice"
      ]
    },
    "content": "\nI got some questions from [Ashwin](https://twitter.com/ashwin_g_g) that I figured I would answer in public! \n\n> Note: A lot of these topics are covered in [my upcoming book](https://www.swyx.io/writing/i-m-writing-a-book-45a8/).\n\n**Q: Relevance of blogs in our career or benefits of blog writing?**\n\nThe benefits of blogging are the same as the benefits of [Learning in Public](https://www.swyx.io/writing/learn-in-public/). By writing for our past selves, we help our future selves, and our peers who are also going through the same journey. As a benefit, blogging is lower commitment and far more flexible than building up a portfolio. If you work in a field that is not so visual, your blog IS your portfolio. It demonstrates passion and interest in your field, and ability to explain complex topics in simple words. \n\nThe most direct benefit for your career is that when people google you, they find an engaged professional passionate about learning more, instead of a dull LinkedIn page repeating exactly the same info as the resume. If businesses care about their SEO, you should care about your own SEO too. \n\nBut indirect benefits also matter. Your blog becomes your knowledge base, and you will find yourself referring to your own blogposts at work. This is GREAT - it means you are scaling yourself, and also it means you [Don't End the Week with Nothing](https://training.kalzumeus.com/newsletters/archive/do-not-end-the-week-with-nothing) from your work. If you only try to play the career ladder at work and do nothing visible from outside of work, then you will have to work a lot harder during interviews to serialize this experience down to a short resume and a short interview.\n\n**Q. What do you think are the essential components of a good blog content?**\n\nIt is interesting to others, and it is VERY interesting to you. Find that intersection, and keep having a curious mind. Nothing else matters. A BAD blog is one where you fuck around with tooling for a few months and then only write 3 posts. \n\n### JUST.\n\n### WRITE.\n\n### DON'T BE THIS PERSON.\n\n[![https://pbs.twimg.com/media/D2HyrDbVAAAYBM1?format=jpg&name=medium](https://pbs.twimg.com/media/D2HyrDbVAAAYBM1?format=jpg&name=medium)](https://twitter.com/markdalgleish/status/1108433814647300097)\n\n**Q: What motivates you to write a new blog? How do you keep yourself motivated in the midst of cumbersome and repetitive work?**\n\nI enjoy sharing what I learn. I have friends who keep me motivated and I look forward to my future self benefiting from what I do today. \n\nI am good now. I will be **formidable** in 10, 20, 40 years. \n\nThink *really* long term and you will play a different game than 99% of developers.\n\n**Q: How do you target a blog at the right audience? How do you deal with feedback and criticism.**\n\nI write for my own interests. I only care about the audience when it is my job to make things accessible for specific groups. \n\nFeedback gets incorporated into my writing immediately, so that they get positive feedback on their feedback and it encourages them to read future writing.\n\nCriticism is fine. [You can learn SO MUCH on the Internet for the Low, Low Price of your Ego](https://kentcdodds.com/chats-with-kent-podcast/seasons/01/episodes/you-can-learn-a-lot-for-the-low-price-of-your-ego-with-shawn-wang/). Learn how to listen to your detractors. They will be your **biggest** teachers, and eventually, fans. You learn most from getting things wrong in public. \n\nOf course, if they get abusive, block them. There will forever be more armchair critics than creators. You don't need their destructive energy in your life if they do not act in good faith."
  },
  {
    "slug": "making-aws-amplify-work-with-rollup-2d9m",
    "data": {
      "title": "Making AWS Amplify work with Rollup",
      "description": "AWS Amplify assumes CommonJS, which Rollup is allergic to. I recently discovered that you can make it work with Rollup with a few tweaks.",
      "tag_list": [
        "aws",
        "amplify",
        "rollup",
        "javascript"
      ]
    },
    "content": "\nAWS Amplify assumes CommonJS, which Rollup doesn't work well with (Hence all Amplify web app examples use Webpack). I recently discovered that you can make it work with Rollup with a few tweaks.\n\nLet's take the default Svelte app, which uses Rollup:\n\n```bash\nnpx degit sveltejs/template my-svelte-project\ncd my-svelte-project\nnpm install\n```\n\nThis default rollup template lacks just two things you need to use Amplify with Rollup. Install `@rollup/plugin-json`:\n\n```bash\nnpm i -D @rollup/plugin-json\n```\n\nAnd add it to your `rollup.config.js`. Also set the `node-resolve` plugin's `preferBuiltins` option to false:\n\n```js\nimport resolve from \"@rollup/plugin-node-resolve\";\nimport json from \"@rollup/plugin-json\"; // new!\n\nexport default {\n  // ...\n  plugins: [\n    // ...\n    resolve({\n      browser: true,\n      preferBuiltins: false, // new!\n      dedupe: [\"svelte\"],\n    }),\n    json(),                  // new!\n    // ...\n  ]\n}\n```\n\nAnd now you are done!\n\nThis setup will work fine with Amplify. For a full demo adding a full Amplify CRUD backend to a working Svelte frontend in under 30 mins, check out [my recent practice run here](https://www.youtube.com/watch?v=hCoRYc_FzK0&feature=youtu.be)!\n\nDev.to Embed: {% youtube hCoRYc_FzK0 %}"
  },
  {
    "slug": "hello-aws",
    "data": {
      "title": "Bringing AWS to App Developers",
      "description": "Where Amplify fits in AWS' trajectory, and why I am joining",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\n**AWS is just too hard to use, and it's not your fault.** Today I'm joining to help AWS build for App Developers, and to grow the Amplify Community with people who **Learn AWS in Public**.\n\n## Muck\n\nWhen AWS officially relaunched in 2006, Jeff Bezos famously pitched it with eight words: \"[We Build Muck, So You Don’t Have To](https://aws.amazon.com/blogs/aws/we_build_muck_s/)\". **And a lot of Muck was built**. The 2006 launch included 3 services (S3 for distributed storage, SQS for message queues, [EC2 for virtual servers](http://blog.b3k.us/2009/01/25/ec2-origins.html)). As of Jan 2020, there were [283](https://medium.com/cloudpegboard/how-many-aws-services-are-there-51dda44fa946). Today, one can get decision fatigue just trying to decide which of [the 7 ways to do async message processing in AWS](https://serverlessfirst.com/aws-async-message-services/) to choose.\n\nThe sheer number of AWS services is a punchline, but is also testament to principled [customer obsession](https://www.amazon.jobs/en/principles). With rare exceptions, AWS builds things customers ask for, never deprecates them ([even the failures](https://web.archive.org/web/20200507083627/https://www.businessinsider.com/amazon-andy-jassy-learning-from-failed-amazons-projects-2016-10?IR=T)), and only lowers prices. Do this for two decades, and multiply by the growth of the Internet, and it's frankly amazing there aren't more. But the upshot of this is that everyone understands that they can trust AWS never to \"move their cheese\". Brand AWS is therefore more valuable than any service, because it cannot be copied, it has to be earned. Almost to a fault, AWS prioritizes stability of their [Infrastructure as a Service](https://en.wikipedia.org/wiki/Infrastructure_as_a_service), and in exchange, businesses know that they can give it their most critical workloads.\n\nThe tradeoff was beginner friendliness. The AWS Console has improved by leaps and bounds over the years, but it is virtually impossible to make it fit the diverse usecases and experience levels of [over one million customers](https://web.archive.org/save/https://www.businessinsider.com/amazon-over-a-million-cloud-customers-2015-1?IR=T). **This was especially true for app developers**. AWS was a godsend for backend/IT budgets, [taking relative cost of infrastructure from 70% to 30% and solving underutilization by providing virtual servers and elastic capacity](https://www.quora.com/How-and-why-did-Amazon-get-into-the-cloud-computing-business-Rumor-has-it-that-they-wanted-to-lease-out-their-excess-capacity-outside-of-the-holiday-season-November%E2%80%93January-Is-that-true). But there was no net reduction in complexity for developers working at the *application* level. We simply swapped one set of hardware based computing primitives for an on-demand, cheaper (in terms of [TCO](https://en.wikipedia.org/wiki/Total_cost_of_ownership)), unfamiliar, proprietary set of software-defined computing primitives.\n\nIn the spectrum of IaaS vs PaaS, App developers just want an opinionated platform with good primitives to build on, rather than having to build their own platform from scratch:\n\n![https://techgoeasy.com/wp-content/uploads/2017/08/cloud_iaas_paas_saas.png](https://techgoeasy.com/wp-content/uploads/2017/08/cloud_iaas_paas_saas.png)\n\nThat is where Cloud Distros come in. \n\n## Cloud Distros Recap\n\n[I've written before about the concept of Cloud Distros](https://www.swyx.io/writing/cloud-distros/), but I'll recap the main points here:\n\n- From inception, AWS was conceived as an \"[Operating System for the Internet](https://techcrunch.com/2016/07/02/andy-jassys-brief-history-of-the-genesis-of-aws/)\" (an analogy echoed by [Dave Cutler and Amitabh Srivasta](https://www.zdnet.com/article/red-dog-five-questions-with-microsoft-mystery-man-dave-cutler/) in creating Azure).\n- Linux operating systems often ship with user friendly customizations, called \"[distributions](https://en.wikipedia.org/wiki/Linux_distribution)\" or \"distros\" for short.\n- In the same way, there proved to be good (but ultimately not huge) demand for \"Platforms as a Service\" - with 2007's Heroku as a PaaS for Rails developers, and 2011's [Parse](https://en.wikipedia.org/wiki/Parse_(platform)) and [Firebase](https://en.wikipedia.org/wiki/Firebase) as a PaaS for Mobile developers atop AWS and Google respectively. \n- The PaaS idea proved early rather than wrong – the arrival of Kubernetes and AWS Lambda in 2014 presaged the modern crop of cloud startups, from JAMstack CDNs like Netlify and Vercel, to Cloud IDEs like Repl.it and Glitch, to managed clusters like Render and KintoHub, even to moonshot experiments like Darklang. The *wild* diversity of these approaches to improving App Developer experience, **all built atop of AWS/GCP**, lead me to christen these \"Cloud Distros\" rather than the dated PaaS terminology.\n\n[![https://dev-to-uploads.s3.amazonaws.com/i/raav7mrz1p6n15sv9zxs.png](https://dev-to-uploads.s3.amazonaws.com/i/raav7mrz1p6n15sv9zxs.png)](https://www.swyx.io/writing/cloud-distros/)\n\n## Amplify\n\nAmplify is the first truly first-party \"Cloud Distro\", if you don't count Google-acquired Firebase. This does not make it automatically superior. Far from it! AWS has a lot of non-negotiable requirements to get started (from requiring a credit card upfront to requiring IAM setup for a basic demo). And let's face it, its UI will never win design awards. That just categorically rules it out for many App Devs. In the battle for developer experience, AWS is not the mighty incumbent, it is the *underdog*.\n\nBut Amplify has at least two *killer* unique attributes that make it compelling to some, and *at least* worth considering for most:\n\n- **It scales like AWS scales**. All Amplify features are built atop existing AWS services like S3, DynamoDB, and Cognito. If you want to eject to underlying services, you can. The same isn't true of third party Cloud Distros ([Begin](https://begin.com/) is a notable exception). This also means you are paying the theoretical low end of costs, since third party Cloud Distros must either charge cost-plus on their users or subsidize with VC money (unsustainable long term). AWS Scale doesn't just mean raw ability to handle throughput, it also means edge cases, security, compliance, monitoring, and advanced functionality have been fully battle tested by others who came before you.\n- **It has a crack team of AWS insiders**. I don't know them well yet, but it stands to reason that working on a Cloud Distro from within offers *unfair* advantages to working on one from without. (It also offers the standard disadvantages of a bigco vs the agility of a startup) If you were to start a company and needed to hire a platform team, you probably couldn't afford this team. If you fit Amplify's target audience, you get this team for free.\n\nSimplification requires opinionation, and on that Amplify makes its biggest bets of all - curating the \"best of\" other AWS services. Instead of using one of the myriad ways to setup AWS Lambda and configure API Gateway, you can just type `amplify add api` and the appropriate GraphQL or REST resources are set up for you, with your infrastructure fully described as code. Storage? `amplify add storage`. Auth? `amplify add auth`. There's a half dozen more I haven't even got to yet. But all these dedicated services coming together means you don't need to manage servers to do everything you need in an app. \n\nAmplify enables the \"**fullstack serverless**\" future. AWS makes the bulk of its money on providing virtual servers today, but from both internal and external metrics, it is clear the future is serverless. A bet on Amplify is a bet on the future of AWS.\n\n> Note: there will forever be a place for traditional VPSes and even on-premises data centers - the serverless movement is additive rather than destructive. \n\nFor a company famous for having every team operate as separately moving parts, Amplify runs the opposite direction. It normalizes the workflows of its disparate constituents in a single toolchain, from the hosted [Amplify Console](https://docs.aws.amazon.com/amplify/latest/userguide/welcome.html), to the [CLI](https://docs.amplify.aws/cli) on your machine, to the [Libraries/SDKs](https://docs.amplify.aws/lib/q/platform/ios) that run on your users' devices. And this works the *exact same way* whether you are working on an iOS, Android, React Native, or JS (React, Vue, Svelte, etc) Web App.\n\nLastly, it is just abundantly clear that Amplify represents a different kind of AWS than you or I are used to. Unlike most AWS products, [Amplify is fully open source](https://github.com/aws-amplify). They write integrations for all popular JS frameworks (React, React Native, Angular, Ionic, and Vue) and Swift for iOS and Java/Kotlin for Android. They do support on [GitHub](https://github.com/aws-amplify/) and chat on [Discord](https://discord.gg/jWVbPfC). They even advertise on podcasts you and I listen to, like [ShopTalk Show](https://shoptalkshow.com/) and [Ladybug](https://ladybug.dev/). **In short, they're meeting us where we are**. \n\nThis is, as far as I know, unprecedented in AWS' approach to App Developers. I think it is paying off. Anecdotally, Amplify is growing [three times faster](https://devchat.tv/react-native-radio/rnr-165-full-stack-development-with-react-native-on-the-cloud-with-nader-dabit/) than the rest of AWS.\n\n> Note: If you'd like to learn more about Amplify, join [the free Virtual Amplify Days event](https://awsamplifydays.splashthat.com/) from Jun 10-11th to hear customer stories from people who have put every part of Amplify in production. I'll be right there with you taking this all in!\n\n## Personal Note\n\nI am joining [AWS Mobile](https://aws.amazon.com/products/mobile/) today as a Senior Developer Advocate. AWS Mobile houses Amplify, [Amplify Console](https://aws.amazon.com/amplify/console/?c=m&sec=srv) (One stop CI/CD + CDN + DNS), [AWS Device Farm](https://aws.amazon.com/device-farm/) (Run tests on real phones), and [AppSync](https://aws.amazon.com/appsync/) (GraphQL Gateway and Realtime/Offline Syncing), and is closely connected to [API Gateway](https://aws.amazon.com/api-gateway/?c=m&sec=srv) (Public API Endpoints) and [Amazon Pinpoint](https://aws.amazon.com/pinpoint/?c=m&sec=srv) (Analytics & Engagement). AppSync is worth a special mention because it is what first put the idea of joining AWS in my head.\n\nA year ago I wrote [Optimistic, Offline-first apps using serverless functions and GraphQL](https://gist.github.com/sw-yx/108d90755aa3f34401dcb488c2f0f5aa) sketching out a set of integrated technologies. They would have the net effect of making apps feel a lot faster and more reliable (because *optimistic and offline-first*), while making it a lot easier to develop this table-stakes experience (because the GraphQL schema lets us establish an eventually consistent client-server contract). \n\n9 months later, the Amplify DataStore was announced at Re:Invent (which [addressed most of the things I wanted](https://www.swyx.io/writing/svelte-amplify-datastore/)). I didn't get everything right, but it was clear that I was thinking on the same wavelength as someone at AWS (it turned out to be [Richard Threlkeld](https://www.youtube.com/watch?v=KcYl6_We0EU&list=WL&index=4&t=5s), but clearly he was supported by others). AWS believed in this wacky idea enough to resource its development over 2 years. I don't think I've ever worked at a place that could do something like that.\n\nI spoke to a variety of companies, large and small, to explore what I wanted to do and figure out my market value. (As an aside: It is TRICKY for developer advocates to put themselves on the market while still employed!) But far and away the smoothest process where I was \"on the same page\" with everyone was the ~1 month I spent interviewing with AWS. It helped a lot that I'd known my hiring manager, [Nader](https://twitter.com/dabit3) for ~2yrs at this point so there really wasn't a whole lot he didn't already know about me (a huge benefit of [Learning in Public](https://www.swyx.io/writing/learn-in-public/) btw) nor I him. The final \"super day\" on-site was challenging and actually had me worried I failed 1-2 of the interviews. But I was pleasantly surprised to hear that I had received unanimous yeses!\n\n[Nader](https://twitter.com/dabit3) is an industry legend and personal inspiration. When [I completed my first solo project at my bootcamp](https://www.swyx.io/speaking/fullstack-crossbones/), I made a crappy React Native boilerplate that used the best UI Toolkit I could find, [React Native Elements](https://react-native-elements.github.io/react-native-elements/). I didn't know it was Nader's. When I applied for [my first conference talk](https://www.swyx.io/speaking/react-not-reactive/), Nader helped review my CFP. When I decided to get better at CSS, [Nader encouraged and retweeted me](https://twitter.com/dabit3/status/1005263955735072768?s=20). He is constantly helping out developers, from sharing invaluable advice on [being a prosperous consultant](https://medium.com/@dabit3/the-prosperous-software-consultant-5dc8d705c5dd), to [helping developers find jobs during this crisis](https://twitter.com/dabit3/status/1241757522526076929?s=20), to [using his platform to help others get their start](https://twitter.com/dabit3/status/1266018672046374915). He doesn't just lift others up, he also puts the \"heavy lifting\" in \"undifferentiated heavy lifting\"! I am excited he is leading the team, and nervous how our friendship will change now he is my manager.\n\nWith this move, I have just gone from bootcamp grad in 2017 to getting hired at a BigCo L6 level in 3 years. My friends say I don't need the validation, but I gotta tell you, it does feel nice.\n\nThe coronavirus shutdowns happened almost immediately after [I left Netlify](https://www.swyx.io/writing/farewell-netlify/), which caused complications in my visa situation (I am not American). I was supposed to start as a US Remote employee in April; instead I'm starting in Singapore today. It's taken a financial toll - I estimate that this coronavirus delay and change in employment situation will cost me about $70k in foregone earnings. This hurts more because I am now the primary earner for my family of 4. [I've been writing a book](https://www.swyx.io/writing/i-m-writing-a-book-45a8/) to make up some of that; but all things considered I'm glad to still have a stable job again. \n\nI have never considered myself a \"big company\" guy. I value autonomy and flexibility, doing the right thing over the *done* thing. But AWS is not a typical BigCo - it famously runs on \"two pizza teams\" (not literally true - Amplify is more like 20 pizzas - but still, not huge). I've quoted Bezos [since my second ever meetup talk](https://www.swyx.io/speaking/creating-cra/), and have always admired AWS practices from afar, from the [6-pagers](https://www.cnbc.com/2018/04/23/what-jeff-bezos-learned-from-requiring-6-page-memos-at-amazon.html) right down to [the anecdote told in Steve Yegge's Platforms Rant](https://gist.github.com/chitchcock/1281611). Time to see this modern colossus from the inside.\n\nAWS has been making developers more productive for 14 years. For everyone at AWS, myself included, today is [Day 1](https://www.fool.com/investing/2017/04/13/jeff-bezos-says-it-will-always-be-day-1-at-amazon.aspx)."
  },
  {
    "slug": "js-third-age",
    "data": {
      "title": "The Third Age of JavaScript",
      "description": "A bunch of things are moving in JavaScript - it is quite feasible that the JS of 10 years from now will look totally unrecognizable",
      "tag_list": [
        "reflections",
        "javascript"
      ]
    },
    "content": "\n*Discussions: [HN](https://news.ycombinator.com/item?id=23248256) | [Twitter](https://twitter.com/swyx/status/1263123032328925186) | [Dev.to](https://dev.to/swyx/the-third-age-of-javascript-45hn)*\n\nEvery 10 years there is a changing of the guard in JavaScript. I think we have just started a period of accelerated change that could in future be regarded as the **Third Age of JavaScript**.\n\n\n![Third age](https://dev-to-uploads.s3.amazonaws.com/i/rlixanixq8pyrpg9ivrv.png)\n\n## The Story So Far\n\n**The first age of JS, from 1997-2007**, started with a bang and ended with a whimper. You all know Brendan Eich's story, and perhaps it is less known how the ES4 effort faltered amidst strong competition from closed ecosystems like Flash/Actionscript. The full origin story of JS is better told by its principal authors, Brendan Eich and Allen Wirfs-Brock, in [JavaScript: The First 20 Years](https://www.swyx.io/writing/js-20-years/). \n\n![https://basbossink.github.io/presentations/modern-javascript/images/ecmascript-history.png](https://basbossink.github.io/presentations/modern-javascript/images/ecmascript-history.png)\n\n**The second age of JS, from 2009-2019**, started with the *annus mirabilis* of 2009, where npm, Node.js, and ES5 were born. With Doug Crockford showing us [its good parts](http://shop.oreilly.com/product/9780596517748.do), users built a whole host of [JS Build Tools](https://www.swyx.io/writing/jobs-of-js-build-tools/) and libraries, and extended JS' reach to both desktop and new fangled smart phones. Towards 2019 we even saw the emergence of specialized runtimes for JS on phones like [Facebook's Hermes](https://github.com/facebook/hermes) as well as compiler first frontend frameworks like [Svelte 3](https://svelte.dev/blog/svelte-3-rethinking-reactivity). \n\n## The Third Age\n\n2020 feels like the start of a new Age. If the First Age was about building out a language, and the Second Age was about users exploring and expanding the language, the Third Age is about clearing away legacy assumptions and collapsing layers of tooling. \n\n> Note: I have pitched the [Collapsing Layers](https://www.swyx.io/writing/collapsing-layers/) thesis before!\n\nThe main legacy assumption being cleared away is the JS ecosystem's reliance on CommonJS, which evolved as a series of compromises. Its replacement, ES Modules, has been waiting in the wings for a while, but lacked the momentum to truly take a leap because existing tooling was slow but \"good enough\". On the frontend, modern browsers are equipped to handle these in small amounts too, but [important details haven't yet been resolved](https://v8.dev/features/modules#bundle). The [Pika/Snowpack](https://www.snowpack.dev/) project is positioned to accelerate this future by providing a facade that can disappear as ES Modules get worked out. As a final bonus, IE11 will begin its slow march to [end of life starting this year and ending in 2029](https://www.swyx.io/writing/ie11-eol/).\n\nThe other assumption going away is that JavaScript tools must be built in JavaScript. The potential for type safety and [10x-100x performance speedup](https://github.com/evanw/esbuild) in hot paths is too great to ignore. The \"for JS in JS\" ideal was chipped away with the [near complete takeover of JavaScript by TypeScript](https://twitter.com/swyx/status/1260888049958838272?s=20) and now Deno and Relay are proving that people will learn Rust to contribute to core JS tools. [Brandon Dail predicts](https://twitter.com/aweary/status/1001874375472168960?s=20) this conversion will be done by 2023. We will continue to write JavaScript and TypeScript for the majority of surrounding tooling where approachability outweighs performance. Where we used to think about \"[Functional Core, Imperative Shell](https://www.destroyallsoftware.com/screencasts/catalog/functional-core-imperative-shell)\", we are now moving to \"**Systems Core, Scripting Shell**\". \n\n> Note - [this is contested](https://mrale.ph/blog/2018/02/03/maybe-you-dont-need-rust-to-speed-up-your-js.html), and [Python's PyPy indicates this isn't a foregone conclusion](https://stackoverflow.com/questions/2591879/pypy-how-can-it-possibly-beat-cpython).\n\n**Layers are also collapsing in interesting ways**. Deno takes a radical approach of writing a whole new runtime, collapsing a bunch of common tools doing tasks like testing, formatting, linting and bundling into one binary, speaking TypeScript, and even including a [standard lib](https://deno.land/manual/standard_library). Rome takes a different tack, collapsing all those layers atop of Node.js (as far as I know, I'm not too close to it). \n\nSomething that didn't exist 10 years ago and now is a fact of life is public clouds (AWS, Azure, GCP, et al). JavaScript has an interesting relationship with the cloud that I cannot quite articulate - Cloud platform devs wouldn't touch JS with a 10 foot pole, but yet JS is their biggest consumer. AWS Lambda launched with JS first. There's also a clear move to collapse layers between your IDE and your cloud and remove the pesky laptop in between. Glitch, Repl.it, Codesandbox, [GitHub Codespaces](https://twitter.com/github/status/1258068851809497089), Stackblitz and more are all [Cloud Distros](https://www.swyx.io/writing/cloud-distros/) leveraging JS to explore this space. Meanwhile, [JAMstack](http://jamstack.org/) providers like Netlify and Vercel tackle it from the PoV of collapsing layers between your CI/CD and your CDN, and removing the pesky running server in between.\n\nEven in frontend frameworks, the activity going on is fascinating. Svelte [collapsed everything from animations to state management into a compiler](https://www.swyx.io/speaking/svelte-space-elevator/). React is exploring [metaframeworks](https://www.swyx.io/writing/react-distros/) and [client-server integration](https://twitter.com/dan_abramov/status/1259614150386425858). And Vue is [working on an \"unbundler\" dev server project named Vite](https://github.com/vuejs/vite). \n\nIn summary: Third Age JS tools will be\n\n- Faster\n- ESM first\n- Collapsed Layers (One thing doing many things well instead of many things doing one thing well) \n- Typesafe-er (built with a strongly typed language at core, and supporting TS in user code with zero config) \n- Secure-er (from dependency attacks, or lax permissions) \n- Polyglot\n- Neo-Isomorphic (recognizing that much, if not most, JS should run first at buildtime or on server-side before ever reaching the client) \n\nThe result of all of this work is **both a better developer experience** (faster builds, industry standard tooling) and **user experience** (smaller bundles, faster feature delivery). It is the final metamorphosis of JavaScript from site scripting toy language to full application platform. \n\n## The Death of JavaScript?\n\nIf [Gary Bernhardt's predictions hold true](https://www.destroyallsoftware.com/talks/the-birth-and-death-of-javascript), the Third Age may be JavaScript's last (his timeline gives JS until 2035). There is always the looming specter of Web Assembly - even Brendan Eich has pivoted his famous saying to \"Always Bet on JS - and WASM\". He originally thought JS could be \"the universal virtual machine\", but [told me once that](https://twitter.com/BrendanEich/status/1001307081725562882?s=20) WASM now is the ultimate fulfillment of that idea.\n\nIf so - **we're in the Endgame now**. \n\n## Your Turn\n\n**What will the end of JavaScript's Third Age around ~2030 look like?** Let me know your guess 👇\n\n\n![Image courtesy of Jim Nielsen](https://pbs.twimg.com/media/EYe3aBjXkAUrRzD?format=png&name=small)"
  },
  {
    "slug": "working-reverse-9-5",
    "data": {
      "title": "Working The Reverse 9 to 5",
      "description": "Having weird sleeping hours is working out... very well for me.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nFor the past month I've been working approximately 9pm to 5am.\n\nYes, you read that right. 9 **PM** to 5 **AM**. Graveyard shift. It's working out **very well** and I thought I would share that weird fact.\n\nI didn't choose this life, it chose me. The combination of [the Coronavirus Recession](https://www.swyx.io/writing/coronavirus-recession/) and [me leaving my job](https://www.swyx.io/writing/farewell-netlify) meant I had to temporarily leave the US and head back to Singapore where I am from.\n\nSingapore is exactly 12 hours ahead of New York Time - 3pm there is 3am here. However my next job is still US based, so I wanted to have some overlap with US hours.\n\nI had adjusted to Singapore time already after [my mandatory 14 day quarantine upon returning home](https://twitter.com/swyx/status/1247831801227145216?s=20). It started in small drips and drabs - staying up late for calls with the US, or doing one of [the many conference talks I have done recently](https://www.swyx.io/speaking/). I would sleep at 1am, then 2am, then 3am intermittently. Purely for fun - I was unemployed, might as well keep up appearances (literally!).\n\nThen [Daniel Vassallo](https://twitter.com/dvassallo) challenged me to [write a book](https://www.swyx.io/writing/i-m-writing-a-book-45a8/), and suddenly I became a Full Time Author™. I was dumb enough to choose the nontechnical topic of early dev career advice. Writing is *hard* at best, and I suspect nontechnical topics are way harder than normal technical blogging, b/c it's so unclear what to include and there is no objectively correct advice (except for [Learn in Public](https://www.swyx.io/writing/learn-in-public/)!). I found myself writing all the way to sunup, which is roughly 6-7am Singapore time.\n\nI discovered that this overlapped nicely with the US workday since it is 6-7pm EST and 3-4pm PDT. And of course it totally overlaps with Europe.\n\nMeanwhile family life (I am staying with my parents) carried on, I had to hang out with them and have meals together and play board games and help them with errands and stuff. Personal time and exercise has also been important for me during this time. So splitting this Asia/US life felt quite overwhelming.\n\nSo for now I have settled into this routine:\n\n- from 1pm - 9pm I would be mostly with my family. (8 hours)\n- from 9pm - 5am I would be mostly working/online. (8 hours)\n- from 5am - 1pm I would sleep. (8 hours)\n\nFudge +/- 1 or 2 hours here and there for the odd bursts of excitement or days I am feeling off, I have been living like this for a full month now (I am writing this at 10.23pm my time).\n\nHere are benefits I noticed:\n\n- Default intermittent fasting. Since I only eat with my family I only have an 8 hour window to eat lunch and dinner, and sometimes I just skip lunch. \n- My family doesn't disturb me after 9pm, nothing in Singapore disturbs me after 9pm. With clear IRL boundaries, I am free to focus solely on work.\n- Since I have been awake half the day anyway, I can start the US morning pretty fresh and alert and pretty prepared in terms of what I want to do that day. Often I will come prepared with something I want to publish or write or tweet.\n- I work until I am tired, and I sleep right away since I work from home. While/before sleeping, my mind still processes problems I have been chewing on from the workday.\n- I wake up with a clean slate and have no work on my mind.\n\nI feel like this is the antithesis to [the Miracle Morning](https://www.miraclemorning.com/) productivity enthusiasts, most recently Jocko Willink, espouse. Why can't this work? It seems to be working fine for me.\n\n> Author's note: I name this the \"reverse 9 to 5\" because the \"inverse 9 to 5\" reads to me like working 5pm to 9am, which isn't what I'm going for."
  },
  {
    "slug": "slack-fumble",
    "data": {
      "title": "Slack is Fumbling Developers",
      "description": "Reflecting on the sudden switch away from Slack for Developer communities",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\n*[Reddit](https://www.reddit.com/r/programming/comments/i2buze/slack_is_fumbling_developers_and_the_rise_of/) | [HN](https://news.ycombinator.com/item?id=24012496) | [Dev.to](https://dev.to/swyx/slack-is-fumbling-developers-142f)*\n\nA few days ago [I had a sudden realization](https://twitter.com/swyx/status/1260929975454056449): I hadn't been active in Slack in the 2 months since [I left Netlify](https://www.swyx.io/writing/farewell-netlify/). Those of you who live and work in Slack know how big this is - I personally went from opening Slack practically *every day* from 2015 - 2020, to zero meaningful usage whatsoever.\n\nIn its place, I am now active in a ton of Developer Discord channels. I feel like this is a *meaningful* shift this year, and based on responses, [I'm](https://twitter.com/jkup/status/1260936175172419590?s=20) [not](https://twitter.com/1Marc/status/1261960701666623488?s=20) [alone](https://twitter.com/stolinski/status/1260943494635352066?s=20).\n\n## But They Didn't Want Developers?\n\nThe obvious proximate cause of course, is that I am professionally and personally highly attuned to developer communities, and Slack has actively pushed away developer communities. [Quote Harry Hedger](https://twitter.com/harry_hedger/status/1261967137989513216?s=20):\n\n> Slack lost [Reactiflux](https://www.reactiflux.com/) bc they wanted $70k/month to support them. \n\nSure, I get that this was a deliberate decision. Storage costs money. But *how much* are we talking? Further, every company understands the logic of running \"loss leaders\" in order to seed stickiness and future growth. **While literally every other company on Earth is throwing all sorts of free benefits to attract developers**, Slack found developer communities growing like a weed on it, and flicked them away like so much dandruff. Stewart Butterfield is like the anti-[Ballmer](https://www.youtube.com/watch?v=VM-2OVNt-eQ).\n\nI'm not here to argue that Slack lost developers because it didn't want developers. That's a tautology. I'm here to argue that **this is a strategic fumble** that opened up the field to whatever will eventually replace Slack for startups (Microsoft Teams' success is difficult to assess so I will ignore it here).\n\n## Slack Losing Its Way\n\n**Developers are your canary in the coalmine** for user experience - because they create the damn things! \n\nSlack's original appeal was that it had a much better user experience than prior work chat/email/collaboration tools including the now-dead HipChat and Campfire. Reams of VC pitch decks were made about the \"Consumerization of the Workplace\" thanks to Slack's approachable design (as [MetaLab never fails to remind you](https://medium.com/@awilkinson/slack-s-2-8-billion-dollar-secret-sauce-5c5ec7117908)) and early touches like emoji reactions and bots. The theory was that we demand the UX polish that we see in our personal lives, in our professional lives as well.\n\nFast forward to today: \"I’ve been using Discord a ton lately.  Slack for work and Discord for hobbies.\" That's from [Marc Grabanski](https://twitter.com/1Marc/status/1261960701666623488), who knows a thing or two about developer trends.\n\nI'll raise you another point: In 2016, Slack was originally backronym'ed into the [Searchable Log of All Conversation and Knowledge](https://www.businessinsider.com/where-did-slack-get-its-name-2016-9). \n\nFast forward to today and advice from the curiously high amount of \"how to work from home\" content you've undoubtedly been receiving is overwhelmingly [Communicate Asynchronously](https://about.gitlab.com/blog/2015/04/08/the-remote-manifesto/#2-communicate-asynchronously) and [Use Anything But Slack for Long Lived Knowledge](https://knowyourteam.com/m/lessons/161-managing-remote-teams/topics/1331-process-and-tools-how-to-collaborate-effectively-when-your-team-is-remote#but-are-there-tools-you-just-cant-live-without).\n\n## The Game is Changing\n\nMeanwhile the UX bar has risen in the past 5 years:\n\n- Slack makes you create a email and password anew every time you join a new Slack. Clicking a Discord invite immediately lands you in the channel as long as you're logged in.\n- Slack walks you through the full onboarding experience every time you join a new Slack. I've joined probably 80 Slacks, I'm tired of the \"👋 Hi, Slackbot here!\" welcome messages on how to use Slack, or the downright *buggy* step through guide that prompts me to put in my profile picture even though Slack clearly figured out that I use the same damn picture every time and is already displaying it. Discord does exactly none of this.\n- You need to repeat the whole email and signup song and dance when you move to your phone. Discord is cross-platform-first - sign in on desktop and you're also signed in on mobile.\n- (Developer specific) The new WYSIWYG editor rolled out last year [made it annoying](https://www.vice.com/en_us/article/pa7nbn/slacks-new-rich-text-editor-shows-why-markdown-still-scares-people) for developers accustomed to What We Type Is What We Get Thank You Very Much. Not only does Discord not get in the way of your typing, it offers **SYNTAX HIGHLIGHTING** if you add [language identifiers](https://help.github.com/en/github/writing-on-github/creating-and-highlighting-code-blocks#syntax-highlighting) to your code fenced blocks! ([More on HN on this](https://news.ycombinator.com/item?id=21589647))\n\nThe past year or so in Office Productivity has been a free-for-all as other companies moved into Slack's open flanks. Notion has torn it up as the Searchable Knowledge Base to beat today. Zoom has overwhelmingly captured video chat, despite Slack also offering it natively. Even a bootstrapped startup like [Tuple](http://tuple.app/) has become successful despite Slack buying the literal [category maker](https://techcrunch.com/2015/01/28/slack-buys-screenhero-to-add-screen-sharing-and-voice-chat-to-its-work-messaging-platform/) in the space. And of course Discord and even Telegram have made inroads in real time chat.\n\nMy intuition is that [the Great Unbundling](https://stratechery.com/2017/the-great-unbundling/) has come for Slack. Whether or not it survives will probably depend on this: Is text chat really [a feature or a product](https://jtbd.info/feature-vs-product-42bf2dad2764)? Either way, having to answer that question is an undesirable strategic position for Slack to be in. There is no victory in this fight.\n\n> Note: if you feel lost right about here, I released the [Developer's Guide to Tech Strategy](https://www.swyx.io/writing/dev-guide-to-tech-strategy/) chapter of [my upcoming book](https://twitter.com/coding_career) recently, feel free to pause and check it out.\n\n## Not Over Yet\n\nSlack has fumbled a ball, but it still has a strong lead. Two features still make Slack a strong fit for realtime workplace communication: [threaded messages](https://www.theverge.com/2017/1/18/14305528/slack-threads-threaded-messages), which Discord refuses to build, and [Shared Channels](https://slackhq.com/shared-channels-growth-innovation), which [I've gone on record](https://twitter.com/swyx/status/1096347638696267778?s=20) as saying will add years' worth and billions of dollars of revenue. Both were introduced in 2017, probably the peak year of innovation for Slack before creating [the Slack fund](https://www.google.com/search?q=slack+fund&oq=slack+fund&aqs=chrome..69i57l2j69i59l2j69i65l2j69i61j69i60.843j0j7&sourceid=chrome&ie=UTF-8) in 2018 and then its 2019 IPO. \n\nFrom my limited anecdata, the no-code [Workflow Builder](https://venturebeat.com/2019/04/24/slack-launches-workflow-builder-for-businesses-to-make-apps-without-code/) hasn't yet had significant adoption, but I would be happy to see this change over time. All great platforms eventually add low/no-code automation.\n\nSlack has also made heavy investments for Enterprise Adoption, with things like [Enterprise Grid](https://slackhq.com/introducing-slack-enterprise-grid) (also in 2017! hmm...). I of course don't have any insight into Slack's growth in the Enterprise. I'm fully aware of the economic incentives of going upmarket and forsaking low end customers.\n\n## Communities Over Teams\n\nThe net result of all this is that Slack is now very much not [the operating system for your team](https://www.bloomberg.com/features/2016-design/a/stewart-butterfield/) that it set out to be. However its entire user experience is tied to teams. \n\nMore specifically - having one single home team, interacting with that team on one device, and the assumption that you don't join or change teams very often. Perhaps this is the disconnect we are sensing.\n\nSlack's focus on teams may be becoming outdated in this new world we live in, where Deep Work and organized knowledge bases come at a premium, and our professional work crosses professional and personal boundaries, and spans across multiple devices, modalities and even multiple communities we both lead and participate in. \nDiscord's focus on communities may win hearts and eventually wallets. Here's [Kurt Kemple on Discord's community features](https://twitter.com/kurtkemple/status/1260947289436303367?s=20), for thought:\n\n> Discord is more community friendly IMO. Things like moderation, roles, boosting, group A/V, all make it a great place for a more democratized and scaleable platform.\n\nBy focusing on communities instead of siloed teams, Discord becomes a friend of community leaders. In other words, **Discord aggregates aggregators**. In a world of [Aggregation Theory](https://stratechery.com/aggregation-theory/), this is a very good thing.\n\nI thought it was Paul Graham or Benedict Evans who said this, but am unable to find the source: \"What hackers do for fun today, we will do at work tomorrow\". \n\n> Edit: It was [Chris Dixon](https://cdixon.org/2013/03/02/what-the-smartest-people-do-on-the-weekend-is-what-everyone-else-will-do-during-the-week-in-ten-years): \"What the smartest people do on the weekend is what everyone else will do during the week in ten years\"\n\n**Hackers are deserting Slack in droves**. Slack should be on high alert.\n\n---\n\nRecommended reads:\n\n- Mule's Dark Horse Discord: How a gaming chat platform is secretly connecting the internet, and defining the future of work: https://mule.substack.com/p/dark-horse-discord\n- Kevin Kwok's The Arc of Collaboration: https://kwokchain.com/2019/08/16/the-arc-of-collaboration/\n\n\n## Postscript\n\nHere's a full inventory of emails you opt in to EVERY DAMN TIME you sign up for Yet Another Slack:\n\n- NAME_OF_SLACK on Slack: New Account Details\n- Confirm your email to join NAME_OF_SLACK\n- You're all set up on NAME_OF_SLACK in Slack\n- New messages from 2 conversations in NAME_OF_SLACK\n- Join your team's conversations on NAME_OF_SLACK workspace\n- Notifications from the NAME_OF_SLACK workspace\n- Slack account sign in from a new device\n\nJust f right off with this engagement spam."
  },
  {
    "slug": "ie11-eol",
    "data": {
      "title": "IE11 Mainstream End Of Life in Oct 2020",
      "description": "Making the case for Mainstream EOL of IE 11.",
      "tag_list": [
        "ie11",
        "webdev"
      ]
    },
    "content": "\n> Note: I am not a Microsoft expert/insider, these are just datapoints compiled together by an interested amateur. Greg Whitworth of the Microsoft Edge team replied [here](https://twitter.com/gregwhitworth/status/1260714305487376386)!\n\nIt's widely known that IE11 is scheduled for [End of Life in 2025](https://death-to-ie11.com/).\n\nIt's *less* widely known that this calculation is [tied to the end of life of Windows 10](https://www.microsoft.com/en-us/microsoft-365/windows/end-of-ie-support), which is indeed in Oct 2025.\n\nIt's **much less** widely known that Oct 2025 is the date for *extended support* of Windows 10, and that **mainstream support** for Windows 10 ends on **Oct 13 2020**. \n\n**This means \"Mainstream End of Life\" for IE 11 is this year**, not 5 years from now. In fact, it is **exactly five months from now**.\n\n## Mainstream vs Extended?\n\nNot being a Windows sysadmin, this was news to me too. But it makes sense on deeper inspection. Windows 10 was released in July 2015, so a 5 year \"mainstream support\" period with free upgrades sounds about right. \n\n**Enterprises pay extra for an additional 5 years of \"extended support\"**, which covers security updates as well as nonsecurity updates, but [not new features](https://meritum.cloud/windows-and-sql-server-2008-eos/). \n\nBeyond that, you might be able to buy \"Extended Security Updates\" for 3 years. I could not find Win 10's pricing, but [Windows 7 cost up to $200 per device per year](https://www.zdnet.com/article/how-much-will-staying-patched-on-windows-7-cost-you-heres-the-price-list/).\n\nAn additional complication to consider is that there were [3 Enterprise LTSB/C versions of Windows 10](https://support.microsoft.com/en-us/help/13853/windows-lifecycle-fact-sheet). The 2025 Extended EOL date corresponds to that first version released in 2015, but there are two other versions, with EOLs going out to Jan 2029. So, any death of Win10/IE11 will be staggered rather than immediate.\n\nHowever this is still true: **The first end of Mainstream Support of Windows 10 is in Oct 13, 2020**. This year.\n\n## End of Life\n\nI'll confess that \"Mainstream End of Life\" is a term I made up. Microsoft doesn't recognize it. In fact, as far as I can tell, Microsoft doesn't even use \"End of Life\" anywhere on their documentation. They have \"End of Sales\" and \"End of Service\", and \"End of Support\". No End of Life. It's a colloquial term.\n\nWhen *developers* talk about a \"Windows 10 End of Life\", they seem to mean End of Support. I write this blogpost to point out: \n\n- End of support comes in two stages: Mainstream and Extended\n- Mainstream Support ends in 5 months\n- Non Enterprise users do not get Extended Support.\n\nThis is what I mean by \"Mainstream End of Life\" of Windows 10, and therefore IE 11.\n\n## Why the Timing is Right\n\nMicrosoft's own approach to EOLing IE11 has been more subtle: \n\n- Microsoft itself has [argued against using IE for everything since 2019](https://redmondmag.com/articles/2019/02/07/microsoft-argues-against-using-ie.aspx). \n- It then [announced IE compatibility mode](https://www.computerworld.com/article/3393193/microsoft-will-demote-ie-to-a-mode-inside-edge.html) in Chromium-based Edge, which began rolled out on Jan 15 this year.\n- That release was manual, but the full rollout of \"Edgium\" is underway. In February, [rollout via Windows Update began](https://borncity.com/win/2020/02/26/microsoft-chromium-edge-rollout-via-windows-update-started/). This rollout happens gradual stages [until a full, wider release happens in the summer](https://9to5google.com/2020/01/15/microsoft-edge-re-launch-chromium-windows-macos/) (**aka, before Oct 2020**)\n\nMicrosoft also has [had precedent in dropping support early for IE](https://www.computerworld.com/article/3332682/why-full-chromium-edge-means-end-times-for-ie.html) - and Edgium with IE11 mode may pave the way for end of Extended Support to come sooner than 2025.\n\nThe coronavirus pandemic also had an unexpected benefit to this movement - because most Win10/IE11 installations are on older office machines, working from home means that overnight, millions of Win10/IE11 users realized they could move without significant impact. (*I'm sure there are plenty of Win10 laptops so... idk*)\n\n## IE11's market share\n\nI'll preface this by noting the obvious: browser market share varies *widely* depending on what your customer demo is. You care about your customers more than you really care about global averages. But just to round things out where they stand right now:\n\n- [Statcounter](https://gs.statcounter.com/) has global share at 1.41%\n- [W3counter](https://www.w3counter.com/globalstats.php) has (global?) share at 1.75%\n- The [US Government](https://analytics.usa.gov/) has share at 3.7%\n- You can [see browser share by your country here](https://canistop.net/b/ie/11) (*for some reason Saint Helena, Christmas Island, the Vatican, and Korea randomly love IE*)\n\nWith the facts already laid out, it's not hard to see that this year will be the first year Edge overtakes IE:\n\n[![https://upload.wikimedia.org/wikipedia/en/7/70/BrowserUsageShare.png](https://upload.wikimedia.org/wikipedia/en/7/70/BrowserUsageShare.png)](https://en.wikipedia.org/wiki/Usage_share_of_web_browsers)\n\n## Recommendations for Developers\n\n**The development cost of IE11 support is high**. Enumerating that cost is out of scope for this blogpost, but at a minimum, you should be employing the [`module/nomodule` Differential Serving](https://css-tricks.com/differential-serving/) approach for serving legacy JavaScript. This ensures you don't penalize the ~98% of your users with modern browsers for the 1-2% of IE users.\n\nHowever, even that approach doesn't account for CSS support and IE11 browser bugs/quirks. Those will still take up code weight and developer time. The key thing to realize here is that **all code and developer time spent on IE11 support is effectively subsidized by other users on modern browsers**. In theory, they all pay you (roughly) the same revenue per user, but you spend disproportionately more resources supporting legacy browser users. Hardly seems fair.\n\nTherefore my recommendation is this: By IE11 Mainstream EOL (13 Oct 2020),\n\n1. **Explicitly drop IE11 support for all non-Enterprise users**\n2. Offer *paid* IE11 support for all who need it past mainstream EOL (be careful to price it correctly for its true cost) (or use https://quik.dev/)\n3. Publicly notify a registry of peers in your industry who have also decided to stop support, like [Dailymotion](https://developer.dailymotion.com/changelog/dailymotion-no-longer-supports-internet-explorer/), [GitHub, Stack Overflow, Zendesk, Slack, Atlassian, YouTube](https://death-to-ie11.com/)\n\nPoint 2 helps you justify this move as a potential revenue stream - if you are doing this work anyway, you might as well get properly paid for it instead of stealing time from other users.\n\nPoint 3 is there because **this is a collective action problem** - this becomes *more successful the more people do it*, particularly within peer groups. I considered setting up a `change.org` petition, but have not done so because I don't consider myself a fit leader for this movement - I am just pointing out facts for people who may not know them. But if you [tweet at me](https://twitter.com/swyx) I will collect you and your company's vote and help whoever wants to [take up creating this coalition](https://www.byteperceptions.com/leadership-psychology/building-a-change-coalition.html) (is that you?! please be you!).\n\n[Mike Sherov has also advocated actively blocking IE11](https://mike.sherov.com/ie11-countdown/) based on lessons from the Deaths of IE6 and IE8. Since we started getting automatic upgrades to Edge in February, this is finally a feasible IT policy you can justify.\n\n## What Now?\n\nI know I'm writing for a very narrow audience. I expect developers reading this post to be in 3 camps: EITHER you already don't support IE11, OR you work at a BigCorp and you can pry IE11 from their cold dead hands, OR you work at a company that basically supports IE11 because your competitors do and because, for now, IE11 share is still above >1%.\n\nIf you fit that last description, I wrote this for you. **We have a window, right now.** You have 5 months to advocate internally, with all the stakeholders, and help make the Internet get a tiny bit safer and our industry a tiny bit simpler. You can also [nag your users to update their browsers](https://browser-update.org/).\n\nI'm rooting for you."
  },
  {
    "slug": "the-day-I-became-a-software-engineer",
    "data": {
      "title": "The Day I Became A Software Engineer",
      "description": "A single mindset shift that changed my entire attitude to software.",
      "tag_list": [
        "advice"
      ]
    },
    "content": "\nI've used software almost all my life. A single mindset shift changed my entire attitude to software. It turned me from a software *user* into a software *engineer*. **It took me 30 years**. I'd like to share that day with you.\n\n---\n\nWhen I was 10, I was playing [Space Cadet Pinball](https://www.pcgamer.com/heres-how-to-bring-space-cadet-3d-pinball-back-to-windows/) and [Mario Teaches Typing](https://playclassic.games/games/educational-dos-games-online/play-mario-teaches-typing-online/) on the shared family computer. I became a software user. Not a software engineer.\n\nWhen I was 14, my entire friend group found [ICQ](https://en.wikipedia.org/wiki/ICQ). I discovered [MUDs](https://en.wikipedia.org/wiki/MUD) and spent way too much time online. I became an Internet user. Not a software engineer.\n\nWhen I was 18, I helped run battalion logistics [in the Army](https://en.wikipedia.org/wiki/National_service_in_Singapore). My unit tracked everything in Excel, and it became my job to output accurate monthly reports and forecasts. I become a business software user. Writing Excel formulas now! Still not a software engineer.\n\nWhen I was 22, I learned statistics and finance in college. I also ran a student investment fund. My classes used [R/S-PLUS](https://en.wikipedia.org/wiki/S-PLUS), [MATLAB](https://en.wikipedia.org/wiki/MATLAB), and yes, Excel with a little [VBA](https://en.wikipedia.org/wiki/Visual_Basic_for_Applications). I became an academic software user. Writing code for class! But still not a software engineer.\n\nWhen I was 26, I helped trade billions of dollars' worth of interest rate swaps, currency derivatives, and eventually stocks. The bank/fund I worked at had software engineers, but they were too busy for little old me. So I had to learn [Haskell/Mu](https://www.quora.com/Why-did-Standard-Chartered-need-its-own-Haskell-compiler), F# and Python to write the stuff I needed. Nothing amazing, mind you, just enough to get by. But I became a [quantitative trader](https://www.investopedia.com/articles/active-trading/112614/steps-becoming-quant-trader.asp). Managing money with code! Yet still not a software engineer.\n\nWhen I was 30, I [burned out of finance](https://www.freecodecamp.org/news/shawn-wang-podcast-interview/), and took a year to decide to learn to code for real. I spent a year [completing FreeCodeCamp and Fullstack Academy](https://medium.com/hackernoon/no-zero-days-my-path-from-code-newbie-to-full-stack-developer-in-12-months-214122a8948f) and got my first dev job at [corporate venture](https://twosigmaiq.com/). My job title literally said \"Software Engineer\"! But I was *still* not a software engineer.\n\n---\n\nI spent ~30 years as a software **user**. For 30 years, I would try to do things with software. When I ran into a problem, I looked through support forums. I asked colleagues. I (sometimes) [read the manual](https://en.wikipedia.org/wiki/RTFM). If I found nothing, I was done. It wasn't possible. Next tool, next task, sorry boss. You'll have to get a Real Software Engineer to figure this out.\n\nI had this mindset even through learning to code. First I was told what to code and how to code, by professors and bosses. FreeCodeCamp did the same thing: write this line here, see that result. It was particularly acute with [MeteorJS](https://www.meteor.com/) - if an [Atmosphere package](https://guide.meteor.com/using-atmosphere-packages.html) didn't exist, it couldn't be done. You'll have to get a Real Software Engineer to build it.\n\nIt persisted when I learned React. If I wanted to do X, I would search for \"How to Do X with React\". Google wanted me to believe that my problems would be solved if I could type in the magic incantations to summon the right blogpost. That I could ask stupid questions like \"What's the Best ______?\" absent any context. If something didn't work, I'd file an issue and wait for the maintainer to fix it. I'd find someone's hacky workaround, paste it in to my code, and if it works, happily carry on. It'll be fixed eventually by someone who knows what they're doing.\n\nI knew how to write tests, but I didn't *believe* in them. You could say I wrote tests *performatively* - on demand, because someone said it was a good idea, because some interviewer was watching, because some user asked for it. In a way, it just mirrored my own attitude to my own code. I knew how to write code, but I just didn't *believe* mine was worth anything.\n\n---\n\nI turned 34 last week. **Things are different now.**\n\nI don't recall the exact day I decided to stop being afraid of [`node_modules`](https://www.reddit.com/r/node/comments/9o5fzk/what_is_node_modules_used_for/). It must have been around June 2018. I had a bug, docs were out of date, and no help was forthcoming. I'd peeked in there before, but it always looked like a mess of junk. But I opened it up, guessed the right place, put in a few `console.log`s... and figured it out!\n\nI knew the objectively correct answer, because I was looking at the running source code. I could give a solution to my own issue, and contribute a fix. I could make a judgment call on whether this code was adding value. I could [apply local fixes](https://github.com/ds300/patch-package/), or fork it altogether. I appreciated the separation between app and framework, between developer and platform, between \"antipattern\" or \"best practice\" - but equally could make up my own mind whether I agreed with it, **once I understood how it worked**.\n\nI found I could do this again, and again. It'd be frustrating at times, and I lacked a lot of fundamental knowledge where I'd get stuck. But it worked more often than not. I stopped passing the buck. Taking responsibility for my own code. Thinking for myself. Learning about tradeoffs, and making them. Becoming that person other developers go to when they need help. Writing those docs and blogposts they look for on Google. Always improving my knowledge of how things fundamentally work, by sharing it.\n\nEverything is just layers of abstraction. Made by humans just like me. If I run into a problem, I can [search for help](https://www.swyx.io/writing/how-to-google-your-errors-2l6o/). I can [write it up](https://www.swyx.io/writing/learn-in-public/). I can [View Source](https://twitter.com/swyx/status/1113635187915673600). Think through [First Principles](https://www.swyx.io/writing/first-principles-approach/). I use git for version control and tests to [hold the line](https://twitter.com/swyx/status/1237894223384334339). With enough time, effort, and mentors, no problem is intractable.\n\nNow I am a software engineer."
  },
  {
    "slug": "dev-guide-to-tech-strategy",
    "data": {
      "title": "Developer's Guide to Tech Strategy",
      "description": "This is a _very_ high level overview of tech strategy; that is, the _business of software_ rather than the art and science of creating software itself.",
      "tag_list": [
        "career",
        "strategy"
      ]
    },
    "content": "\nThis is a _very_ high level overview of tech strategy; that is, the _business of software_ rather than the art and science of creating software itself.\n\nIt goes without saying that your coding does not have independent value; it must be _applied usefully_ on some economic problem to have sustainable real world impact.\n\n## Table of Contents\n\n## Tech Strategy and Your Career\n\nPerhaps more pertinent to your career: your coding ability and, to some degree, financial outcomes, will be associated with the success of your company (we infer better coding ability in someone who was an early engineer at Uber than we do a random unknown startup, despite having no knowledge of their actual contributions).\n\nEven if you don't care about that, and never intend to be a founder, your understanding of the business you're in allows you to offer suggestions and prioritize work in alignment with economic opportunity. It may not feel like much, but as the person closest to the code, you have a _tremendous_ amount of autonomy to the final experience delivered:\n\n- You make delivery estimations, and give advance warning of serious technical blockers\n- You make technical tradeoffs between the \"quick and dirty\" way and the \"right\" way\n- You pick abstractions for scale and pluggability vs rejecting premature abstraction\n- You evaluate commercial solutions compared to the cost of a custom solution - the infamous \"[build vs buy](https://www.scalyr.com/blog/build-vs-buy/)\" decision\n- You can defensively code for every probable system failure, or understand where failures are more acceptable than their cure\n- You sweat the fine A11y/UI/UX/Uptime/Response time details\n- You can find the \"low hanging fruit\" ideas to implement, offered by your data models and pre-existing frameworks (and plugins), and can suggest them to your product owners.\n\nAs you advance in autonomy, you will even get to pick the projects you work on and to pitch new initiatives that you eventually own (this is a GREAT career move).\n\nIf you [read the story of how Google Maps' Satellite view was almost named \"Bird Mode\"](https://9to5google.com/2019/02/25/sergey-brin-google-maps-bird-mode/), you will understand how your broad ranging powers even come right down to product names.\n\nA well run company will not put you in this position, but when the chips are down, **Developers are designers and product managers of last resort.**\n\nAs you increase in seniority, you will also have to grow in your business judgment. In fact, taking a glance over every Engineering Career Ladder will tell you how important your business impact is to your career advancement.\n\nFinally - the technologies that you work with are also strongly influenced by their economic incentives. \"Free and Open Source\" does not mean \"Free of any commercial considerations\" nor does it mean \"Open Direction decided by Direct Democracy\". The platforms you run on (whether it is the browsers or the public clouds, databases, payment/fulfilment platforms, or even language distributions) all have massive (I'm talking 10 figures and up in some cases) investments.\n\n## Software is Eating the World\n\n**I am assigning this to you as required reading.** In 2011, [Marc Andreesen wrote \"Why Software is Eating the World\"](https://a16z.com/2011/08/20/why-software-is-eating-the-world/) which set out the foundational thesis of his venture capital firm. It foretold the rise of massive software businesses in the decade since, and made the case for why every industry is now in the business of software. Marc has since updated this with takes on [healthcare](https://a16z.com/2019/08/16/software-eaten-world-healthcare/), [biotech](https://a16z.com/2019/08/23/software-eats-healthcare-bio-next/) and [crypto](https://a16z.com/2019/08/29/internet-past-crypto-future-crypto-regulatory-summit/).\n\nThis is now widely understood in the software industry and you should at least be aware of it even if you disagree with it. (Though it does make the software engineer the center of the new world.)\n\n## Horizontal vs Vertical\n\nThe basic split in tech strategy you should be aware of is **Horizontal vs Vertical** businesses.\n\nIf you work on infrastructure you might be familiar with \"Horizontal vs Vertical scaling\". This is different. Here we are talking with respect to your customers:\n\n- If your business is designed from the ground up to serve a single industry or customer profile, you are a **Vertical** business. Vertical businesses typically grow by offering more and more capabilities to serve the customers they have, despite these features existing as standalone offerings elsewhere. The goal is to be a \"One Stop Shop\". User experience can be *mindblowing* when it is vertically integrated - but frustrating when limitations arise.\n- If your business can be used by customers across any industry, you are a **Horizontal** business. Horizontal businesses typically grow by reaching more and more kinds of customers, building any integrations required or adding knobs and configs to accommodate their usecase. The goal is to \"Do One Thing Well\" and be the best in the world at it.\n\nYou'll also run into two other typical analogies offered to describe this divide:\n\n- **Apple (Vertical) vs Android (Horizontal)**: Apple is fully vertically integrated for the high end smartphone market, from Payments, Messaging, Photos, and Operating System all the way down to Processor. Android is a free open source operating system and it is used by all sorts of phone manufacturers, and extreme customization of the launcher and widget experience is the norm.\n- **Bundling (Vertical) vs Unbundling (Horizontal)**: a reference to this famous Jim Barksdale/Marc Andreesen quote:\n\n  > There are 2 ways to make money in software - bundling and unbundling.\n\nNone but the most disciplined businesses are 100% horizontal or vertical. There is usually a healthy debate within the company as to which direction to pursue, as both are valid ways to grow. But pursuing both signals lack of vision, inability to accept tradeoffs and will lead to problems in resourcing, product development and sales and marketing.\n\nYou'll also hear this idea applied to other aspects of business - Horizontal vs Vertical Integration, Horizontal vs Vertical Acquisition, and Horizontal vs Vertical Strategy. They are all variants of business expansion along one of these lines.\n\n## Business Models\n\nThe next dimension you should be aware of is **business models**. Quite simply, this answers the question of \"who pays?\", \"what directly makes revenue go up?\", and, not often enough, \"what must you spend money on?\"\n\nThe most common ones you should be aware of are _Agencies, Advertising, Subscriptions, and Marketplaces_. Most other companies that employ software engineers have aspects of some of these embedded within them. I cannot possibly do justice to them in the space I have here, but I will at least introduce them here and try to give you what you need to learn more.\n\n### Agencies\n\nThe **Agency** model is the most common one for small teams. I don't have numbers for this, but my guess is it is responsible for most tech jobs as well.\n\n- With an agency model, you have one or more clients and **you are paid for your time**.\n- If you are a dev team within a bigger, non-tech company, you are basically an in-house agency with one client.\n- If you are a consultant or freelancer, you are a one-person agency.\n- There are a thousand small ways to tweak dev setups and payment terms, but broadly, as the amount of dev-hours grows, the amount of money flowing into the agency grows.\n\nIdeally, you should be trying to get the most done per hour, but cynically, bad incentive systems can lead to just booking more hours. The common thread is that your income isn't fully pinned to the success of your client's business, which is sometimes a feature and often a bug of [the Principal-Agent Problem](https://www.intelligenteconomist.com/principal-agent-problem/). Despite its flaws, agencies are still so popular simply because of the sheer amount of work that needs to be done, and the specialized talent needed to do certain high-skill types of work.\n\n### Advertising\n\nThe **Advertising** model is next most common. Here you make money from getting more traffic to your site, or usage of your product, and selling advertiser spots.\n\n- Sometimes the advertising is _display ads_ (paying for [Cost Per Mille](https://en.wikipedia.org/wiki/Cost_per_mille)s - aka ears and eyeballs - just to be there and build brand awareness).\n- However, most advertisers these days prefer _performance based_ marketing - paying for directly measurable user actions e.g. [Cost per Click](https://sproutsocial.com/glossary/cost-per-click/).\n\nMost social networks and news/opinion sites run this way, though there is an [absolutely massive assortment of marketing technology](https://chiefmartec.com/2019/04/marketing-technology-landscape-supergraphic-2019/) to help ad buyers find the best ad inventory for them. Because end users pay nothing and advertisers pay for access, the derisive view is that \"Users are the Product\". However this may not always be a negative - [the Wirecutter](https://thewirecutter.com/) and [the Points Guy](https://thepointsguy.com/) are both well regarded high quality content sites that make their money from [affiliate marketing](https://neilpatel.com/what-is-affiliate-marketing/), which is just a reformulation of performance based marketing.\n\n### Subscription\n\nThe next most common type of business model is **Subscriptions**:\n\n- If you sell usage of your software, this is known as Software as a Service (SaaS), which is an investment category all of its own. The common characteristic of the [IaaS/PaaS/SaaS](https://www.bmc.com/blogs/saas-vs-paas-vs-iaas-whats-the-difference-and-how-to-choose/) models is they transform [Fixed Cost to Variable Cost](https://www.investopedia.com/ask/answers/032515/what-difference-between-variable-cost-and-fixed-cost-economics.asp) which provides immediate value for customers.\n- Content subscriptions are the other major category, for example for audio (e.g. Spotify), video (e.g. Netflix), news (e.g. the New York Times), blogging (e.g. Stratechery), data (e.g. Crunchbase) or membership to a professional group/community. All of which require software to support them.\n\nSince users directly pay for the software/content/membership, and can walk away at any time, the incentive alignment is clear - use subscription revenue to make a better offering, which helps drives more subscriptions, which helps finance a better offering, and so on. Since digital content can be replicated infinitely, the [gross margin](https://en.wikipedia.org/wiki/Gross_margin) and therefore cashflow of these kinds of business is high. To grow, the business has to grow [its marketing funnel](https://openviewpartners.com/blog/the-ultimate-saas-funnel-guide/), increase [conversion rates](https://www.custify.com/7-ways-saas-companies-can-increase-conversion-rates), keep a lid on cost of content (e.g. revenue sharing with content creators), and decrease [churn](https://en.wikipedia.org/wiki/Churn_rate).\n\nMost subscription businesses are a buffet - pay your subscription, and you can consume all-you-can-eat. This has an inherent flaw - some people just eat a whole lot more than most. This is expensive to support and essentially the lighter users subsidize their \"abuse\" of the platform (by bringing down average usage). Therefore all subscription business eventually start charging [per-seat](https://en.wikipedia.org/wiki/Per-seat_license), and then find their way toward some form of metered billing (using some form of [value metrics](https://www.chargebee.com/blog/saas-pricing-and-value-metrics/)).\n\n### Marketplaces\n\nMarketplaces are the hardest software businesses to build, and therefore there are fewer of them than other kinds of business. However, once established, they exhibit gobsmacking dual sided [network effects](https://www.investopedia.com/terms/n/network-effect.asp), which makes them very valuable.\n\nMarketplaces match buyer and seller, just like their offline counterparts. The marketplace gives both sides an assurance of [liquidity](https://techcrunch.com/2017/07/11/marketplace-liquidity/) (buyers can find what they want, sellers can sell what they have or the marketplace will die) and quality (buyers are good customers, sellers must meet standards, or they get kicked off the platform). In exchange, it takes a fee, from either the buyer or seller. Because the fee typically is a percentage of the money that changes hands, this is called a [take rate](https://www.fool.com/investing/2017/01/08/why-the-take-rate-is-so-important-in-e-commerce.aspx) and marketplaces want to grow the [Gross Merchandise Volume](https://en.wikipedia.org/wiki/Gross_merchandise_volume) it is based on. Take rates range wildly based on platform power - [Gumroad charges 3.5%](https://gumroad.com/features/pricing) while [Apple and Google's app stores take 30%](https://www.theregister.co.uk/2018/08/29/app_store_duopoly_30_per_cent/).\n\nThis model sounds simple, but there are a lot of ways to make additional revenue. For example, suppliers often pay certification or listing fees, or they could instead _be paid_ to join. It turns out that the marketplace's own site is prime ad space, while customers will pay for better service, so it is true that you can build both an [entire advertising business](https://www.marketingdive.com/news/amazons-ad-sales-surge-41-to-record-48b/571467/) AND an [entire subscription business](https://en.wikipedia.org/wiki/Amazon_Prime) INSIDE a marketplace business, both of which Amazon has done.\n\n**In a way, this is the business model to end all business models, because you essentially now run your own economy.**\n\nTwo final, major advantages you need to know:\n\n- Marketplaces don't own inventory, since suppliers are the ones to bring their inventory to market. This makes them [asset light](https://www.monkshill.com/views/2019/3/5/bits-vs-atoms-why-we-invest-in-asset-light-businesses), which means they can scale enormously with little investment. Airbnb offers more room nights than any hotel chain in the world without owning real estate, Uber and Lyft transport more passengers than any taxi company without owning a car.\n- Large enough marketplaces drive both seller and buyer to optimize for each other - buyers want high ratings (esp when buying repeat _services_), sellers want great reviews. The slightest nuances of the platform - everything from picture dimensions to product offered - will be exploited to exactly meet the marketplace's needs. This not only means that buyers and sellers are optimizing for each other for free, it also makes starting a competitor marketplace extemely difficult since the investment has been made and the reputation gained.\n\nThese benefits don't come free - **marketplaces are hard to build** for a few reasons:\n\n- **Fakes and Disputes**: Marketplaces offer an implicit or explicit guarantee of quality, which means they need a way to handle what happens when things go wrong.\n  - If goods are sold, you must handle the issue of fakes, lemons, and returns. This doesn't seem fun at all, but [Zappos](https://www.zappos.com/c/shipping-and-returns) made great return policy a competitive advantage.\n  - If services are sold, you must handle the billion things that can go wrong when strangers work with strangers, and you're not around to verify what actually happened. Airbnb had to roll out a [Million Dollar Liability Insurance Program](https://techcrunch.com/2014/11/20/airbnb-rolls-out-million-dollar-liability-insurance-program-for-hosts/) to reassure hosts.\n- **Cutting out the Middleman**: Every strong enough supplier eventually chafes at paying the take rate. At stake is not only more revenue, but also a more direct, long term relationship with the customer, free of any unfavorable changes the marketplace may make in future. For service marketplaces, buyers and sellers who like each other enough can simply take their relationship \"offline\". This means that buyer and seller _churn_ is a huge problem for marketplaces, and it must provide a compelling reason for both sides to stay on even after they have found each other.\n- **The Chicken-and-Egg Issue** (alternatively, the \"cold start\" problem): If there aren't enough buyers, it is not compelling for suppliers to join the platform. If there aren't enough suppliers, buyers won't even come by. A marketplace can toggle back and forth between being [demand or supply constrained](https://www.lennyrachitsky.com/p/how-to-know-if-youre-supply-or-demand) a few times in its life.\n\nOne way to get past many of these issues is to be your own supplier - so that you only grow the demand side of the market. You could view all ecommerce businesses as \"one-sided marketplaces\", although the trendy term for this is [Direct to Consumer](https://www.cbinsights.com/research/direct-to-consumer-retail-strategies/) or \"D2C\".\n\n## Platforms and Aggregators\n\nI've used the word \"**Platform**\" a couple times without definition. The word is horrendously overloaded, so that you can never really be sure what is meant without more context. When it comes to the business of software, though, you can do a lot worse than listen to [Chamath Palihapitiya quoting Bill Gates](https://stratechery.com/2018/the-bill-gates-line/):\n\n> I was in charge of Facebook Platform. We trumpeted it out like it was some hot shit big deal. And I remember when we raised money from Bill Gates, 3 or 4 months after — like our funding history was $5M, $83M, $500M, and then $15B. When that \\$15B happened a few months after Facebook Platform(...) Gates said something along the lines of, “That’s a crock of shit. This isn’t a platform. **A platform is when the economic value of everybody that uses it, exceeds the value of the company that creates it. Then it’s a platform.**\n\nHe's right. And because Platforms are such tremendous economic centers of gravity, we need to differentiate them from your average, run-of-the-mill marketplaces (which, I hope I have established, are powerful economic engines already).\n\nThe 3 most important platforms of all time are Windows, iOS and Android. [Per Ben Thompson](https://stratechery.com/2017/amazons-operating-system/), it's no coincidence that both are operating systems:\n\n- **Windows was the OS for the Desktop Age**: Developers made apps to run on Windows, manufacturers made hardware to run Windows, which Windows again more attractive for both sides. Quoting Ben:\n\n  > The end result was one of the most perfect business models ever: commoditized hardware vendors competed to make Windows computers faster and cheaper, while software developers simultaneously made those same Windows computers more capable and harder to leave.\n\n- **iOS and Android are the OS for the Mobile Age**: Developers make apps for iOS and Android, users are trained to find everything via the Google and Apple Play Stores, which makes iOS and Android more attractive for both sides.\n\n  > Note: Ben originally called Google Search a platform, but [changed his mind](https://stratechery.com/2018/techs-two-philosophies/) as he developed his theory further.\n\nPlatforms exist on a higher level than business model - for Windows it was licensing, where for Google it is ads. Yet the 2 sided nature of a Platform makes it seem like a Marketplace, and Bill Gates' definition seems comparable to GMV.\n\nBut Platforms don't play the transaction volume game - their M.O. is to look at the most critical usecases and to build them out as subsequent products. Windows built out Office (Word, Excel, Outlook, etc), and then Windows Server. Google built GSuite (Docs, Sheets, Gmail, etc), and acquired YouTube.\n\nA contrasting economic model to Platforms are known as **Aggregators**.\n\n### Aggregators\n\nAggregators are the main characters of [Aggregation Theory](https://stratechery.com/2015/aggregation-theory/), defined [by Ben Thompson](https://stratechery.com/2017/defining-aggregators/).\n\n> Note: If this seems strangely focused on the theories of one person, it's because Ben has shaped the entire zeitgeist of tech with this theory, to the point of being quoted at Apple keynotes - so I feel no choice but to _have_ to introduce it to you.\n\nAggregators must have three characteristics:\n\n- Direct relationship with users (payments, accounts, regular usage)\n- Zero [Marginal Costs](https://en.wikipedia.org/wiki/Marginal_cost) for serving users\n- **Demand-driven** Multi-sided Networks with _Decreasing_ Acquisition Costs (aka a very specific type of the 2 sided network effect we have discussed)\n\nAggregators take advantage of a fundamental shift in power enabled by the Internet and the digital economy. Because the marginal cost of digital goods is zero, the ability to generate profits has shifted from companies that control the distribution of scarce resources (Suppliers) to those that control **demand** for abundant ones (Aggregators).\n\n**If you _aggregate users_, you call the shots.** This means great user experience is paramount, and explains the tremendous amount of investment in web and mobile clients over the past 10 years. (One answer to why React developers are strangely in demand.)\n\nLevels of Aggregators:\n\n1. **Supply Acquisition** - they have a great user relationship, but buy their supply, e.g. Netflix and Spotify. Content cost is a concern.\n2. **Supply Transaction Costs** - they don't buy their supply, but pay some marginal costs to bring suppliers onboard, e.g. Uber and Airbnb.\n3. **Zero Supply Costs** - they don't buy their supply, and incur no supplier acquisition cost, e.g. Amazon.\n4. **Super-Aggregators** - they have at least _three_ sides - users, suppliers, advertisers, and zero marginal costs on all of them. E.g. Facebook (with Instagram), Snapchat and Google.\n\n### Platforms vs Aggregators\n\nIt can help to situate these two models by contrasting them. I'll point you to [Ben's writeup on his site](https://stratechery.com/2019/a-framework-for-regulating-competition-on-the-internet/):\n\nPlatforms (e.g. Windows) are critical for their suppliers (e.g. Windows apps) to function, Aggregators (e.g. Google) aren't critical for their suppliers (e.g. websites) to function.\n\nPlatforms facilitate a relationship between users and 3rd-party developers, while Aggregators intermediate the relationship between users and 3rd-party developers.\n\nPlatforms help people do things (aka [Bicycles for the mind](https://youtu.be/4x8wTj-n33A)), Aggregators do things for people.\n\nTo find more information, you can read Ben Thompson's body of work - be aware, his definitions changed between 2015 to 2019. Also, the commonly accepted usage of the word \"Platforms\" also encompasses Aggregators.\n\nOne final point is relevant for us - Both Platforms and Aggregators make it so much easier for suppliers to reach customers that it enables new types of businesses to be created atop them. Apple, Microsoft, YouTube, Amazon, Teachable and others have all minted millionaires and developers can make a great living working on them or for them.\n\n## Other Strategic Perspectives\n\nAs you might see, the analysis of what drives the rise and fall of tech companies can get very nuanced indeed. Though tech giants get all the limelight, good ideas are fractal, and you can apply them in smaller contexts within your professional network, language ecosystem, and internal company politics.\n\nI haven't any room left but want to point you to a few more interesting dynamics you can investigate on your own:\n\n- **The funding of Open Source** is always a point of contention - [Open Core models](https://coss.media/open-core-definition-examples-tradeoffs/) are increasingly viable and benefit the developer ecosystem greatly while also being great employers. However, if your core is open, anyone can compete with you on hosting your core, so this has caused [the Great Relicensing](https://coss.media/open-source-creator-licensing-fundamentals/).\n- **Land Grab vs Organic Growth**: some business opportunities must grow extremely quickly due to a Winner-Takes-Most network effect, and so should raise VC, others will always be one of many and profit and [unit economics](https://www.quora.com/What-are-unit-economics) should be a core focus for [bootstrapped](https://www.investopedia.com/terms/b/bootstrap.asp) growth. [Joel Spolsky has a good introduction to this idea](https://www.joelonsoftware.com/2000/05/12/strategy-letter-i-ben-and-jerrys-vs-amazon/).\n- **Categorical Imperatives**: I have a suspicion that software has intrinsic desires, expressed by inevitable and unimaginative customer and product manager feature requests. If you anthropomorphize the codebase you are working on and treat it as a living, breathing thing, you can think about what IT \"wants\". You can therefore predict what features you are going to have to build. Examples:\n  - [Every collaboration app wants email](http://www.catb.org/jargon/html/Z/Zawinskis-Law.html)\n  - Every data analysis app wants to be Excel\n  - Every marketplace wants to be two-sided\n  - Every social app wants chat\n  - Every [User Generated Content](https://en.wikipedia.org/wiki/User-generated_content) app wants stories\n  - Every B2B app wants a dashboard\n  - Everyone eventually wants a CMS\n\n\n---\n\nAuthor's Note: *This is part of the Strategy section of my upcoming book on [Cracking the Coding Career](https://gumroad.com/products/bAZJq/). If you liked this, come check out the rest of the topics!*"
  },
  {
    "slug": "cloud-distros",
    "data": {
      "title": "Cloud Distros",
      "description": "The next step in the evolution of the Cloud is specialized distros.",
      "tag_list": [
        "cloud"
      ]
    },
    "content": "\n[The news of ~~Zeit~~ Vercel raising $21m](https://vercel.com/blog/zeit-is-now-vercel) ([slide deck here](http://archive.is/lZAq2)) is great occasion for taking stock of what is going on with cloud startups. As Brian Leroux (who runs [Begin.com](http://begin.com/)) [observes](https://twitter.com/brianleroux/status/1252609328340627457), with reference to [Netlify's $55m Series C last month](https://www.netlify.com/blog/2020/03/04/netlify-secures-series-c-funding-to-push-forward-our-vision-for-the-web/):\n\n> Between just Netlify and Vercel the VC community has put over 70MM in cloud focused on frontend dev in 2020. \n\nHaven't AWS/GCP/Azure owned the cloud space? What is the full potential of this new generation of startups basically reselling their services with some value add?\n\n## Cloud's Deployment Age\n\nI am reminded, again, of [Fred Wilson's beloved Carlota Perez framework](https://avc.com/2015/02/the-carlota-perez-framework/) that I wrote about in [React Distros](https://www.swyx.io/writing/react-distros/). First you have an Installation Age, with a lot of creative destruction. Then, with the base primitives sorted out, we then build *atop* the installed layer, in a Deployment Age:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/raav7mrz1p6n15sv9zxs.png)\n\nI think the same dynamics I outlined with frontend frameworks is happening here with cloud services. I'm obviously a LOT less well versed with the history of cloud, so please **please take this with a grain of salt**. \n\n## The \"Failure\" of PaaS\n\nThe argument is that the Big 3 Cloud Providers are mostly providing the new commoditized primitives on which the next generation of cloud services will be built. AWS is AWS, Azure maybe caters to the dotNet/Microsoft crowd better, whereas GCP maybe differentiates on Kubernetes and Machine Learning. Basically everyone has a container thing, a data thing, a file storage thing, a serverless thing, and so on.\n\nA nice way to think about it, which I attribute to [Guillermo](http://softwareengineeringdaily.com/wp-content/uploads/2019/01/SED744-Zeit.pdf) (but I'm not sure about), is that these basic services are the new \"Hardware\". Instead of going to [Fry's](https://en.wikipedia.org/wiki/Fry%27s_Electronics) and picking up a motherboard, we now go to the AWS Console and pick up a `t2.micro` or to Azure for a Durable Function. Instead of debating Sandisk vs Western Digital we match up AWS Aurora vs Azure DocumentDB. The benefits are clear - we don't get our hands dirty, we can easily ([too easily?](https://twitter.com/ShortJared/status/1243257486062178304)) scale with a single API call, and thanks to Infra-as-Code we can truly treat our infra like [cattle, not pets](https://devops.stackexchange.com/questions/653/what-is-the-definition-of-cattle-not-pets).\n\nWhen the Big N clouds launched, [the expectation was that Platform as a Service (PaaS) would win out over Infrastructure as a Service (IaaS)](https://twitter.com/borisjabes/status/1238269374194020352?s=20). I mean - [look at this chart!](https://camo.githubusercontent.com/9e606e1daf967c64a62b11d2bdbff12b4b2b84d7/68747470733a2f2f76656e74757265626561742e636f6d2f77702d636f6e74656e742f75706c6f6164732f323031312f31312f696161732d706161732d736161732e6a70673f726573697a653d3634302532433433392673747269703d616c6c3f773d3634302673747269703d616c6c) - if you were running a Software business, would you want to run it atop an IaaS or a PaaS? It made intuitive sense, and both Google App Engine and [Azure](https://www.cubesys.com.au/10-years-of-azure/) originally launched with this vision, while [Salesforce bought Heroku](https://techcrunch.com/2010/12/08/breaking-salesforce-buys-heroku-for-212-million-in-cash/) within 3 years of founding.\n\nBut this thesis was *wrong*. As Patrick McKenzie [recently noted](https://capiche.com/ama/patrick-mckenzie-stripe):\n\n> I'm surprised that Heroku's model didn't win over AWS' model and that DevOps is accordingly a core competence at most SaaS companies. This seems obviously terrible to me every time I'm doing DevOps, which probably took ~20% of all engineering cycles at my last company for surfacing very little customer value.\n\nThis rings true. As moderately successful as Heroku, Parse, and Firebase were, they are dwarfed by the size of the big clouds' IaaS businesses. It turns out that most people just wanted to lift and shift their workloads, rather than start new apps from scratch on underpowered platforms. Assisted by Docker, this acquired the rather unfortunate name of \"cloud native\". (Unfortunate, because there are now \"more native\" versions of building cloud-powered apps than \"containerize everything and somehow mention agile\")\n\nBut I don't think the PaaSes were wrong. \n\nThey were just early.\n\n## Developer Experience as a Differentiator\n\nThe thing about hardware providers is that they don't cater well to specific audiences. By nature, they build for general use. The best they can do is offer up a default \"Operating System\" to run them - the AWS Console, Google Cloud Console, Microsoft Azure Portal ([Dave Cutler literally called Azure a Cloud OS](https://softwareengineeringdaily.com/2020/03/19/pulumi-infrastructure-as-code-with-joe-duffy/) when it began).\n\nMeanwhile, the \"undifferentiated heavy lifting\" (aka [Muck](https://aws.amazon.com/blogs/aws/we_build_muck_s/)) of wrangling datacenters turned into \"undifferentiated heavy lifting\" of messing with 5 different AWS services just to set up a best practices workflow. \n\nSo increasingly, intermediate providers are rising up to provide a better developer experience and enforce opinionated architectures (like [JAMstack](http://jamstack.org/)):\n\n  - Netlify\n  - Vercel\n  - Repl.it\n  - Begin.com\n  - Glitch\n  - Render.com\n  - Amplify\n  - KintoHub\n  - Darklang\n\nThe working name for this new generation of cloud providers, used by [Martin Casado](https://softwareengineeringdaily.com/2019/01/28/software-chasms-with-martin-casado/), [Amjad Masad](https://www.spreaker.com/user/10197011/the-state-and-future-of-software-develop), and [Guillermo Rauch](http://softwareengineeringdaily.com/wp-content/uploads/2019/01/SED744-Zeit.pdf), is \"**second layer**\" or \"**higher level**\" cloud providers. \n\nNobody loves these names. It doesn't tell you the value add of *having* a second layer. Also the name implies that more layers atop these layers will happen, and that is doubtful.\n\n## Cloud Distros\n\nI think the right name for this phenomenon is **Cloud Distros** (kinda gave this away in the title, huh). The idea is both that the default experience is not good enough, and that there are too many knobs and bells and whistles to tweak for the average developer to setup a basic best practices workflow.\n\nOk, I lied - there is no *average* developer. There are a ton of developers - ~40m, going by GitHub numbers. They don't all have the same skillset. The argument here is that **cloud is going from horizontal, general purpose, off the shelf, to verticalized, opinionated, custom distributions**. There are ~300,000 AWS Cloud Practitioners - yet, going by [Vercel's numbers](http://archive.is/lZAq2), there are 11 million frontend developers.\n\nIn order to cross this \"chasm\", the cloud must change shape. We need to develop custom \"Distros\" for each audience. For the Jamstack audience, we now have Netlify, Amplify, Begin and Vercel. For the Managed Containers crew, we have Render and KintoHub. For the Hack and Learn in the Cloud folks, we have Glitch and Repl.it. What the business nerds call verticalization or bundling, developers call \"developer experience\" - and it is different things to different people. \n\nWhat's funny is these startups all basically run AWS or GCP under the hood anyway. They select the good parts, abstract over multiple services and give us better defaults. This is a little reminiscent of [Linux Distros](https://www.techradar.com/sg/best/best-linux-distros) - you can like Ubuntu, and I can like Parrot OS, but it's all Linux under the hood anyway. We pick our distro based on what we enjoy, and our distros are made with specific developer profiles in mind too.\n\n## The Future of Cloud Distros\n\nWhat we have now isn't the end state of things. It is still too damn hard to create and deploy full stack apps, especially with a serverless architecture. **Serverless cannot proclaim total victory until we can recreate [DHH's demo from 15 years ago](https://www.youtube.com/watch?v=Gzj723LkRJY) in 15 minutes.** I have yet to see a realistic demo replicating this. Our users and their frameworks want us to get there, but the platforms need to grow their capabilities dramatically. In our haste to go serverless, we broke apart the monolith - and suffered the consequences - now we must rebuild it atop our new foundations.\n\n[Begin](http://begin.com/) and [Amplify](https://amplify.aws/) have made some great steps in this direction - offering integrated database solutions. GitHub Codespaces, Codesandbox, Replit, and [Coder.com](https://coder.com/) are putting the IDE in the browser. [Darklang](https://darklang.com/) takes the integration aaaalll the way down past the IDE and even to the language and cloud infrastructure (!). [Render](http://render.com/) and [KintoHub](https://www.kintohub.com/) buck the serverless trend, offering a great developer experience for those who need a running server. \n\nThere's probably no winner-takes-all effect in this market - but of course, there can be an Ubuntu. This generation of Cloud Distros is fighting hard to be the one-stop platform for the next wave (even the next generation) of developers, and **we all win as a result**.\n\n---\n\n**Disclosures**: I [formerly](https://www.swyx.io/writing/farewell-netlify/) worked at Netlify and have interviewed with some companies mentioned here."
  },
  {
    "slug": "osi-layers-coding-careers",
    "data": {
      "title": "OSI Layers for Coding Careers",
      "description": "Let's think about the value chain of humans that code and how we interact.",
      "tag_list": [
        "career"
      ]
    },
    "content": "\nI have always enjoyed the clarity given by [the OSI Model](https://en.wikipedia.org/wiki/OSI_model). It provides a mental framework for how different technologies like HTTP, TCP, IP, and 802.11 come together, each solving their assigned problems, and mostly seamlessly interoperating (with the help of some [Narrow Waists](https://www.swyx.io/writing/narrow-waists/)).\n\nI think it'd be an interesting exercise to divvy up the universe of software jobs along the same lines. I always think of how the value chain for humans is kind of similar, from the Hardware folks who make it all possible, all the way up to the Application folks who create business value, and all the interesting software jobs in between.\n\nSo without further ado, here it is:\n\n![Alt Text](https://pbs.twimg.com/media/EVz7MCMUcAETHO5?format=jpg&name=large)\n\nHere my focus is on accounting for people who are primarily expected to code for their jobs. If you are a founder or PM or developer advocate, you might code every so often, but during that time you are acting as one of these roles rather than as your main job.\n\nBecause I primarily work at the App layer, you may also see some bias due to my lack of knowledge of other layers. Please feel free to add/correct me :)\n\nDescriptively, I see the primary axis of software jobs as adding value from Machines all the way to End Users:\n\n- At the lowest level we have people who work with **Hardware**. Their job is to expose new **Capability** for us hungry hungry software devs [to eat up](https://en.wikipedia.org/wiki/Wirth%27s_law).\n- Then, technically not required but a practical reality these days, we have the **Cloud/Datacenter** people. Their main job is **Availability**. Arguably you could view them and other [Cloud Distros](https://gist.github.com/sw-yx/ff8a4f6757286444fa20b43f6b98b205) as providing \"virtual\" Hardware for the rest of us devs to run on.\n- Next, we have people who make **Runtimes**. This includes Browser Devs, Language designers, and Framework/Tooling/Infrastructure Devs that are basically responsible for all the **Developer Experience** we enjoy.\n\n> The three layers we just covered I call **Platform** development. They don't really have anything specific to do with the **Products** that the next three layers create, but of course they make it all possible.\n\n- Backend developers create internal/private/non-User-Facing **Services** that encode the secure Business Logic of the apps. I've taken a rather expansive definition here, including Machine Learning, Research, Process Automation, Security, Compliance, and Billing. The idea is that UX isn't the focus - it's more the functionality that is the focus here.\n- Then, we have developers who work on **Applications** of all the prior layers. This is everyone else who codes - from Game Devs to Plugin Devs, to Sales/Support engineers and Mobile/Web Devs. They are uniquely responsible for creating a great **user experience**.\n- Lastly, we have end users who create software with the software we give them. **End User Computing** lets users create software, but without traditional coding. This includes everyone else from Business Analysts working in Excel to [productivity geeks setting up #NoCode automation](https://www.swyx.io/writing/no-code-rpa/).\n\nThe assertion is that as these jobs get more numerous as we get closer to Users, who are more diverse and therefore require more customization. Work is also \"further away from the metal\" and when we code at higher layers, we are increasingly encouraged to pretend that the resource constraints of layers below don't exist (for developer experience). This is, of course, a leaky abstraction - but a lot of the time it works.\n\nI don't have much else to observe for now. Do you think there are other dimensions we can split these jobs, and what other commonalities can you find between layers?\n\n\n---\n\nAuthor's Note: *This is part of the Strategy section of my upcoming book on [Cracking the Coding Career](https://gumroad.com/products/bAZJq/). If you liked this, come check out the rest of the topics!*"
  },
  {
    "slug": "marketing-yourself",
    "data": {
      "title": "How to Market Yourself",
      "description": "Assemble your Personal Brand, your Domain, and your Coding Skills/Business Value, then Market Yourself in Public + at Work.",
      "tag_list": [
        "advice",
        "marketing",
        "career"
      ]
    },
    "content": "\n## TL;DR\n\nPersonal Brand + Domain + Coding Skills/Business Value => Market Yourself in Public + at Work\n\n![image](https://user-images.githubusercontent.com/6764957/79151050-59338280-7dfc-11ea-83cb-5a8f4a1f39a0.png)\n\nA nice summary is [here](https://moontowermeta.com/marketing-yourself/). Related podcasts ([like this](https://www.productionreadypod.com/episodes/marketing-yourself-your-products-by-learning-in-public-w-shawn-wang) and [this](https://www.youtube.com/watch?v=QFHO2-8fGtM) and [this](https://aquestionofcode.com/71-how-should-developers-market-themselves-shawn-wang/)).\n\n## When to Use This\n\nIdeally you are constantly Marketing Yourself, but it's understandable that you don't want it to take over your whole life. So: pull up this tactic when:\n\n- You have done something significant that you are proud of/enjoyed\n- Just before some major professional move or project launch (hiring/promotion/idea)\n\nFor the rest of this essay I will primarily talk in terms of Marketing Yourself, but the tactics here also include marketing your ideas and your projects.\n\n## Introduction\n\nMarketing is important for your career. I don't have to justify this; according to a recent survey I saw, 91% of you already agree.\n\nThe more common doubt people have is in their _ability_ to market themselves well. They see \"Tech Celebrities\", and then they look at themselves, and they say: \"I'm not like that, when I put out a blogpost I don't get a billion likes,\" or \"I don't want to be like them, that seems hard.\"\n\nThe mistake here is equating Marketing with Celebrity. It's like saying your favorite restaurant shouldn't bother trying because McDonald's exists. They're two different (but related) things!\n\nYou are a product. You work really hard on making yourself a great product. You owe it to yourself to spend some time on your Marketing even if you don't want to be a \"Celebrity\". Like it or not, **people want to put you in a box**. Help them put you in an expensive, high-sentimental-value, glittering, easy to reach box. Preferably at eye level, near Checkout, next to other nice looking boxes.\n\nIt's [not that hard to be better than 95%](https://danluu.com/p95-skill/) of devs at Marketing. The simple fact is that most devs don't do the basic things that people tell them to do. I think this has two causes:\n\n- It's not code. Code is black and white. Marketing is shades of gray.\n- A lot of advice is very generic. \"Blog more\". Devs often need more help transpiling Business Talk to actionable instructions.\n\nLet me try.\n\n## You Already Know What Good Personal Marketing Is\n\nYou may not feel confident in _practicing_ good marketing, but you should realize you are being marketed _at_ ALL. THE. TIME. Therefore you can be a world class expert in marketing _that resonates with you_. That's the kind that you can practice - not that other scammy, sleazy, invasive, privacy destroying kind.\n\nYou've almost certainly already benefited from good marketing - by finding out about something from someone somewhere, that registered a hook in your mind, that eventually drove you to check it out, and now you cannot function without it.\n\nAnd you certainly want to benefit in the other direction - you _want_ to be that thing that others find out about from someone somewhere. You _want_ to register hooks in people's minds. You _want_ to drive people to check you out. And you _want_ people to prioritize working with you.\n\nOne constraint you have that other marketers wish they had, is that **you don't have to market to the whole world**. You can target specifically the audiences you want to work for, and no more than that - meaning, as long as you are well-known in those circles, you don't need a public presence at all. Your conversion rate will be higher, and your stress probably lower (as will be your luck surface area).\n\n## Personal Branding\n\nThe topic of Marketing Yourself is pretty intertwined with Personal Branding. If you're like me, you've never really thought about the difference until right now.\n\nThink of yourself as a plain, unmarked can of soda. You've got awesome fizz inside. Branding would slap distinctive logo and colors on the can. And then Marketing is responsible for getting you, the freshly minted can of Coca Cola, in front of people.\n\n**Branding is the stuff that uniquely identifies you. Marketing just gets your awesome in front of people.**\n\nOf course, it helps marketing to have strong branding. This is why they are correlated. In fact, the strongest branding _creates its own market_. You don't want a laptop, you want a Macbook. You don't want an electric vehicle, you want a Tesla. I could list more examples, but I trust you understand.\n\nIt's _really easy_ to sell to a market in which you are the only seller. Almost literally shooting fish in the barrel. [Nobody can compete with you at _being you_](https://nav.al/competition-authenticity).\n\nThe other wonderful feature of personal branding is that it is entirely up to you to create stuff that uniquely identifies you. There's no store somewhere from which you pick a brand off the shelf and put it on like a new coat. You create it _from thin air_, with the full dimensionality of all human diversity has to offer. 7 billion humans on Earth doesn't even come close to exhausting the possible space of unique selling points you can pick.\n\n### Picking a Personal Brand\n\nYour Personal Brand is how people talk about you when you're not in the room.\n\nSo naturally, one way to _start_ picking a brand is to listen to the one people naturally chose for you.\n\nCaution: you may not like what you hear! That's ok! That's what we're trying to fix.\n\n### Personal Anecdote Time!\n\nIf you can get a friend to tell it to you straight, good. If you can get some people on a podcast talking about you without you there, good. Or, like me, you can _accidentally_ eavesdrop on a conversation.\n\nI swear I did this unintentionally - the first time I found out I had established an incredibly strong personal brand was when I was at a house party with 20 friends and friends of friends. While in a small group, I overheard someone behind me talking about me. They introduced me as \"that guy that preaches Learn In Public\". Then, at a later hour, I heard another person introduce me without me there. Then, again, when joining a new group, a third person introduced me the exact same way.\n\nI don't consider myself a personal branding expert. But I understood instantly that I had pulled off a very important feat. I had written so much about a topic that multiple people instantly associate me with that topic. It's not _critical_ that they say it in the exact same way, as that can be a bit creepy/culty, but it's good enough to use the same terms.\n\n### Anything But Average\n\nThere are other aspects of my personal brand that don't get as much attention. But I bring it up front and center when it is relevant. I changed careers at 30. I used to be in Finance. I served as a Combat Engineer in the Army. I am from Singapore. I speak Mandarin. I've written production Haskell code. I sing Acapella. I am a humongous Terry Pratchett fan (GNU Terry Pratchett). I love Svelte and React and TypeScript. I am passionate about Frontend/CLI tooling and developer experience. I listen to way too many podcasts. The list goes on.\n\nBut I have this list _cold_. I know _exactly_ what parts of me spark interest and conversation. Therefore I can sustain interest and conversation longer, and people know when to call on me. You should keep a list too - know your strengths and unfair advantages.\n\nWhat I do NOT consider my personal brand is the stuff that doesn't differentiate me at all. For example, when asked about my hobbies, I deflect extremely quickly. I identify as a \"Basic Bro\" - I have my PS4, and Nintendo Switch, I like Marvel movies and watch the same Netflix shows you watch. Just like the million other Basic Bros like me.\n\nTotally basic. Totally boring. NOT a personal brand.\n\nIn fact anything not \"average\" is a good candidate for inclusion. In particular: Diversity is strength. Adversity is strength. Weakness is strength. Nothing is off limits - the only requirements are that you be comfortable self identifying with your personal brand, AND that it evokes **positive emotions** as a result.\n\nI'm serious about that second part - You don't want trolling or outrage or cruel sarcasm to be your brand, nor do you want to bum people out all the time. **Entertain, Educate, Inspire, Motivate** instead.\n\n### Identity + Opinions\n\nWhat I did accidentally, you can do intentionally.\n\nA nice formula for a personal brand is `Identity + Opinions`. A personal brand based solely on who you are, doesn't really communicate what you're about. A personal brand based solely on what you do, is quite... impersonal. People like knowing a bit of both, you should give it to them.\n\nYou can be:\n\n- the Mormon that teaches JavaScript Testing\n- the Theater Nerd that loves Cloud Computing\n- the Knitter that encourages Accessibility\n- the Pianist that evangelizes State Machines (_thanks to schwayse on my livestream for suggesting this one_)\n\nIn the right circles, there are exactly 1 person for each of these I just listed. I don't even have to say who they are.\n\nIdentity doesn't have to be so personal if you're uncomfortable with it. Professional affiliations work. You can be \"That Applitools Gal that created Test Automation University\" or \"That Googler that maintains RxJS\" or \"That Coursera Guy that loves GraphQL\". It's just a little awkward when you eventually leave.\n\nI really want to give you more hints on this, but I'm afraid if I gave more examples I might limit your imagination. Don't even take this formula as a given. It's just one template.\n\n### Consistency\n\nHumans love consistency. Developers _REALLLLY_ love consistency.\n\nHere's an idea of how much Humans love consistency. We often want people who are famous for doing a thing, to come on to OUR stage, and do the thing. Then they do the thing, and we cheer! Simple as that. There's so much chaos in the world and having some cultural touchstones that never change is comfort and nostalgia and joy bundled up into one. Here's [Seth McFarlane being prodded to do the voice of Kermit the Frog and Stewie from Family GUy](https://www.youtube.com/watch?v=AP_aom1IgqI) - something he's done a billion times on a billion talk shows - but he does it anyway and we love it anyway. We LOVE when people Do The Thing!\n\nSimilarly, when we market ourselves, we should be consistent. People love seeing the same names and faces pop up again (Caveat: you should mainly be associated with positive vibes when you do this).\n\n**I recommend taking consistency to an extreme level.** We used to do this offline with business cards. Online, our profiles have become not only our business cards, but also our faces. The majority of people who see you online will never see you in person. In most platforms, your profile photo is \"read\" before your username. Your username is in turn read before your message. Your message is read more than any link you drop. And so on. Therefore I strongly recommend:\n\n- **Photo**. Take a good photo and use the same photo _everywhere_. A professional photographer is worth it, but even better can be something with a good story, or an impressive venue. If possible, try to show your real face, and try to smile. This puts you ahead of ~50% of users already who don't understand the value of this. Companies spend millions on their logos - why shouldn't you spend some time on it? We are irrationally focused on faces, and we really like it when people smile at us. Thankfully, because it's just a photo, it costs us _nothing_ to smile at everybody all the time. It's a really easy way to associate your face with positive emotions. And when we see you pop up on multiple different platforms with the same face, we light up! The emotion completely transfers, and the branding is nonverbal but immediate.\n- **Real Name**. Show your real, professional name if possible, unless your username is your working name. This works especially well in anonymous platforms like Reddit and Hacker News, because you are taking an additional step of de-anonymizing yourself. People respect this.\n- **Username**. Your username should be your name if possible (so people can guess it), or failing which, something you intimately identify with. You should probably have the same one on most platforms, so that people can find you/tag you easily. Some, like myself, will simply use their usernames as their working names for ever. This can be a branding opportunity as well, similar to how music artists adopt mononyms and how fighter pilots adopt callsigns.\n- **Words**. You should consistently associate yourself with a small set of words. Where a bio is allowed, you should have those words prominently displayed. For example, it doesn't take a lot to show up whenever SVG Animation or React and TypeScript are mentioned. You can set Google Alerts or Tweetdeck filters for this, and before long you'll just get associated. When you _have_ your own words, like a catchphrase or motto, and it catches on, that is yet another level of personal branding.\n\n_You will have made it when people start making fun of you_. I'm not 100% serious, but I'm at least a little bit serious: Can people make memes of you? If so, that's a personal brand.\n\nAll this personal branding will be 10x more effective when you have a Domain.\n\n## You Need a Domain\n\nYou Need a Domain.\n\nI mean this in both ways:\n\n1. Set up a site at `yourname.com` that has all your best work\n2. Pick a field that you are About.\n\nThe first is hopefully obvious - instead of putting all your work on a platform somebody else owns, like Twitter, YouTube, LinkedIn or other industry blog, have it primarily discoverable on your site/blog. This builds your site as a destination and lets you fully control your presentation and narrative - even off-site, on Google. _Having a distinctive site design is yet another point of personal branding that, because you are a dev, costs basically nothing._ People come to my site and they remember my scrollbars.\n\nBut the second meaning deserves more introspection: I am asking you to [plant your flag](https://krebsonsecurity.com/2018/06/plant-your-flag-mark-your-territory/). Put up your [personal bat signal](https://andrewchen.co/professional-blogging/).\n\n### Planting Your Flag\n\nI used to have a very crude, kinda sexist name for this idea: \"Be The Guy\". This is because I noticed how many guys were doing this:\n\n- [The Points Guy](https://thepointsguy.com/) is the Internet's pre-eminent authority on travel perks (It is now also a 9-figure business - pandemic aside)\n- [The RideShare Guy](https://therideshareguy.com/) is who Wall Street called upon when Uber and Lyft IPO'ed\n- Science communicators have definitely caught on to this. Neil deGrasse Tyson _always_ introduces himself as your Personal Astrophysicist. But he's completely owned by Bill Nye - The **Science** Guy!\n\nIf you skim over \"the Guy\" as a gender neutral shorthand, the actually important thing about having \"a Guy\" is that you look better just by \"Knowing a Guy\". [Listen to Barney Stinson brag in How I Met Your Mother](https://www.youtube.com/watch?v=BHpBVFFsL8U):\n\n> You know how I got a guy for everything?... My suit guy, my shoe guy, my ticket guy, my club guy, and if I don't have a guy for something I have a Guy guy to get me a guy!\n\nThis effect is real and it is **extraordinarily powerful**.\n\nJust by \"having a guy\" for something, you suddenly feel no desire to overlap with that person's domain. You can now focus on something else. And, to the extent you do that, you are now _utterly dependent_ on \"having a guy\". You're also extremely invested in your \"guy\" (aka go-to person, the gender is not important) being as successful and prominent as possible, so that you look better by association.\n\nIt should strike you now that being someone's go-to person is very valuable, and that this also scales pretty much infinitely.\n\nYou get there by **planting a flag** on your domain, and saying, [this is what I do](https://microconf.gen.co/patrick-mckenzie/). People _want_ expertise. People _want_ to defer to authority. People don't actually _need_ it all the time, they just want the option just in case. People love hoarding options. You can satiate that latent insecurity indefinitely. Most people also define \"expertise\" simply as \"someone who has spent more time on a thing than I have\" (The bar is depressingly low, to be honest. People should have higher standards, but they just don't. This is a systematic weakness you can - responsibly - exploit.)\n\n### Picking A Domain\n\n> Btw, are you chafing for career advancement, or want to be seen as a leader by your peers? My stock advice is, find an area that is important but under-owned and become everyone's go-to expert on that topic. - [Charity Majors](https://twitter.com/mipsytipsy/status/1247006442265198595?s=20)\n\nYou don't need to get too creative with this one. You want to connect yourself to something important:\n\n- Maybe something people deal with daily but don't really think about too much (especially if they know they are leaving something on the table, like airline points - it is easy to make money from helping people unlock free money).\n- Maybe something people only deal with once in a blue moon, but when they do it REALLY hurts (so you gain unfair expertise by specializing in having repeated exposure to rare events across multiple customers).\n\nThere are a bunch of these, so to narrow down even more, look out for something you disproportionately love. Look for your own revealed preferences - search a topic in Slack or Twitter and see how often you talk about it. Look up your own YouTube watch history. An ideal domain for you is something that seems like work to others but you have fun digging into.\n\nWith everything you love, there are things to hate. Find something within what you love, that you are ABSURDLY unsatisfied with. That love-hate tension can fuel you for years.\n\nFor any important enough problem, there are plenty of experts. Do you feel like you haven't narrowed enough? Shrink your world. Be an internal expert at your company for your domain. This also helps you focus on things that bring value to a company, and therefore your career. It's also a very natural onramp to being an external expert when you leave.\n\n### Claiming Your Domain\n\nPicking your domain is 90% of the journey. Most people don't even get that far. To _really_ clean up, be prolific around that domain. Show up. To every conversation. I kind of joke about this as \"High Availability for Humans\".\n\nBy showing up consistently, you become part of the consideration set. Humans don't have room for a very wide consideration set. It's usually 2. If we make lists and try really hard, we can get up to 10 (see: the Oscars).\n\nThink about the last time you purchased soap. You probably buy 1 of 2 brands of soap. But there are 100 on the shelves. They just weren't in your consideration set. So they never stood a shot.\n\nSo your goal, as a brand, is to make it in. You do that by being Highly Available.\n\nBy the way, we also have huge [Availability Bias](https://whatis.techtarget.com/definition/availability-bias) when it comes to recall. We conflate \"first to your lips\" with \"being the best\". We're also really good at backwards justifying what we just called the \"best\" by pulling up a bunch of bullet point reasons that have nothing to do with being \"first to your lips\". (Did I mention we like consistency?)\n\nIt's your job to earn the right to be the best (and to define what that means), but also entirely within your control to be _considered_ the best, which is what claiming your domain looks like.\n\n### Give Up Freedom - For Now\n\nThe flip side of planting your flag is you shouldn't plant it anywhere else. People like to see commitment. It implies, and usually does mean, that you have no choice but to be a domain expert. You signal commitment by giving up optionality. This is 100% OK - what you lose in degrees of freedom you gain 10x in marketing ability.\n\n> Author's note: 10x may be an understatement. [Cory House saw a 15x increase in enquiries](https://www.youtube.com/watch?v=Y4wnbkatj20&feature=youtu.be&t=1311) when he went from \"general dev consulting\" to \"helping teams transition to React\". Same dev, different pitch, 15x opportunities.\n\nThe secret is - and don't tell anyone - that if you pick a Domain and it doesn't work out, _you can still pivot if you need to_. Nobody's going to hold it against you, as long as you don't pivot too often.\n\nIf you really aspire towards more general prominence, you will find a much easier time of it if you first prove yourself in a single Domain.\n\n### Blogging\n\nBlogging is usually mentioned prominently in the \"Marketing for Developers\" space - so I feel I must address the elephant in the room, despite it being a subset of the general mindset I want you to have.\n\nI will always encourage you to blog - but don't fool yourself that merely pushing a new post every month alone will do anything for you by itself. That's just motivational shit people say to get you started. There's a lot of generic, scattershot advice about how [you should blog more](https://www.youtube.com/watch?v=SAiSRuX4iW8). These are usually people trying to sell you a course on blogging. (Except [Steve Yegge](https://sites.google.com/site/steveyegge2/you-should-write-blogs)!)\n\nThe fact is Blogs gain extra power when they are focused on a Domain. [CSS Tricks](https://css-tricks.com/) is a well known blog in the Frontend Dev space, and, as you might guess, for a long time it's domain was entirely CSS tricks. (It's expanded since then). Like everything else you follow, it's all about Signal vs Noise.\n\nBlogs let you get more juice out of that Domain Name you own, by constantly updating it with fresh content. You can also use it to feed that other most valuable online business asset: your email list! Overall, it is just a good general principle to own your own distribution.\n\nTwitter is a form of microblogging. It lets you export data easily and your content shows up on Google without an auth wall. All good things. But you're still subject to an algorithmic feed. Definitely not a distribution you own - but it can be worth it to make the Faustian bargain of growing faster on a platform (like Twitter) first, then pivoting that to your Blog/Mailing list when you have some reach. Growing a Blog/Mailing list from zero with no other presence is hard.\n\n## Marketing your Business Value vs Marketing your Coding Skills\n\n### Business Value\n\nA large genre of \"Marketing for Developers\" advice basically reduces you to an abstract Business Black Box where your only role and value to the company is to Grow Revenue or Reduce Cost (or Die Trying?). I call this **Marketing Your Business Value**. This is, of course, technically correct: Technology is a means to an end, and ultimately your employer has to make ends meet and justify your salary. It is _especially_ in your interest to help them justify as high a salary as possible.\n\nHave _at your fingertips_ all the relevant statistics, data, quotes, and anecdotes for when you solve major product pain points, or contributed a major revenue generating/cost saving feature. Julia Evans calls this a [Brag Document](https://jvns.ca/blog/brag-documents/). You should be able to recite your big wins on demand, and frame it in terms of [What's In It For Them](http://www.salesprogress.com/coaching-leadership/bid/96324/WIFT-What-s-In-It-For-Them), because _you will probably have to_. Managers and Employers are well intentioned, and want to evaluate you fairly and objectively, but often the topic of your contributions comes up completely without warning and out of context, and you want to put yourself on the best footing _every time_.\n\nConsider this Applied Personal Branding - success is when your boss is being able to repeat everything you say you've done to _her_ boss, to advocate for you as fullthroatedly as you should do yourself. Make _that_ easy. If you can, get it down to a concise elevator pitch - Patrick McKenzie is fond of citing a friend's Business Value as \"[wrote the backend billing code that 97% of Google’s revenue passes through](https://kalzumeus.com/2011/10/28/dont-call-yourself-a-programmer/).” Enough said.\n\n### Coding Skills\n\nUnfortunately, Market Business Value is not at all helpful advice for people who have yet to make attributable business impact through their work: Code Newbies and Junior Devs. Sometimes, even as a Senior Dev, you are still trying to market yourself to fellow Devs. These two situations call for a different kind of marketing that is underexamined: **Marketing your Coding Skills**.\n\nTo do this other kind of marketing, you basically have to understand the psyche of your target audience: Developers. What are they looking for?\n\nThere are explicit requirements (those bullet points that companies list on job descriptions) and implicit requirements (subconscious biases and unnamed requirements). You can make it very complicated if you want to, but I think at the core Developers generally care about one thing: that you **Do Cool Shit**. Some have an expansive definition of Coding Skills - even if you've done something totally unrelated, they'll easily assume you can pick up what you need later. Others need something closer to home - that you've Done Cool Shit in a related tech stack.\n\nIf you're marketing yourself for employment, then the Risk Averse will also want to know that you have also **Covered Your Bases** - That, alongside the upside potential of hiring you because you've Done Cool Shit, the downside risk of you being a bad hire is minimized. Do you know Git? Can you solve [FizzBuzz](https://blog.codinghorror.com/why-cant-programmers-program/)? Is your code an unreadable, undocumented mess? This is covered if you have shepherded a nontrivial project from start to finish, and have people you can ask for references. If instead you're just marketing your projects and ideas, then downside matters less - it's easy to walk away.\n\nThe definition of **Cool** really depends on your taste, but people's interests are broadly predictable in aggregate. If you look at tech sections of popular aggregator sites like Reddit and sort by, say, most upvoted posts in the past Year, you can see patterns in what is popular. In fact, [I've done exactly that for /r/reactjs](https://www.swyx.io/writing/react-survey-2019/)!\n\nEven if your project is less visual, and more abstract, you still need to explain to the average programmer why your project is Cool - it solves a common/difficult problem, or it uses a new technology, or it has desirable performance metrics. The best **Cool Shit** will be stuff you have been paid money for and put in production, and that people can go check out live. If you don't have that yet, you can always Clone Well Known Apps (automatically Cool) - or win a Hackathon (check out [Major League Hacking](https://mlh.io/)) - or [Build Your Own X from Scratch](https://github.com/danistefanovic/build-your-own-x), another popular developer genre.\n\n### Portfolios vs Proof of Work\n\nUsually the advice is to assemble your Cool Shit in a Portfolio. Portfolios do 2 good things and 2 bad things:\n\n- Portfolios display your work easily and spells out the quick takeaways per piece - You control your narrative!\n- Portfolios help you diversify your appeal - if one project doesn't spark interest, the next one might!\n  - In this sense it is most like a Stock Portfolio - you're diversifying risk rather than adding upside.\n- Portfolios look skimpy without quantity - meaning you can feel forced to Go Wide instead of Go Deep, Quantity over Quality.\n- Portfolios overly bias toward flashy demos (which doesn't really help if you're not trying to focus on Frontend Dev/Design)\n  - You can and should _buy_ designs if design isn't a skill you're trying to market - it gives your projects an instant facelift which is generally worth multiples of the <\\$100 that a premium design probably costs.\n\nSome people plan their projects by how it will look on a Portfolio - the dreaded \"Portfolio Driven Development\". That lacks heart and it'll show when you have to talk about your projects at interviews and talks. Instead, just pursue projects that seem most interesting to you, and then figure out how to present it later.\n\nThe simple fact is that there are a wide variety of devs and dev careers for whom Portfolios make no sense at all. Your humble author is one of them. You can market your coding skills through any number of more relevant ways, from doing major contributions to Open Source, to being Highly Available surrounding a Domain, to Blogging. The most general, default marketing skill is definitely Blogging. You can write about any kind of technical topic in your blog.\n\n## Marketing Yourself In Public\n\nThe better you have a handle on your Personal Brand, your Domain, your Business Value or your Coding Skills, the easier time you will have marketing in public. Everything we've discussed up to this point is useful in public, so I'll just leave you with a few more pointers to consider whenever you engage and want to promote yourself online.\n\n**Pick a Channel.** The best marketing channels are the ones you're already on. You have a natural affinity for it for whatever reason. For me, it was Reddit, and then Twitter. Dev communities like Dev.to are great too, as are the ones you build on your own (aka your mailing list). Just be aware that some platforms are less rewarding than others - e.g. Facebook charges you to reach your own subscribers, LinkedIn is full of spam, Reddit and Hacker News don't show an avatar so you don't get to imprint your personal brand. I think Instagram and YouTube are _huge_ areas of opportunity for developers. Just pick one or two, and go all in. A lot of people use social media tools like Buffer to crosspost - this is misguided because you end up underinvesting in every platform and everyone can always tell you just aren't sincere.\n\n**Don't Lie.** Most things are taken at face value online, and this is wonderful for getting your message out there. But if you misrepresent what you were responsible for, or straight up fabricate something, you will eventually get found out. We like to think that things live forever online, but I think it's actually easier to erase something from Google than it is to undo the reputational damage caused by a stupid lie. People will hold it against you for years, and you will not have a chance to defend yourself or atone for your sins. Stephen Covey calls this [the Speed of Trust](https://www.speedoftrust.com/). Once you lose trust, everything you say gets run against a suspicion check, and you have to put up more proof points to be taken seriously. This also applies to promises of future commitment too - [if you simply do what you say you were going to do, you will stand out](https://twitter.com/swyx/status/1193467826310238208).\n\n**Don't Share Secrets.** You will gain more privileged information over time as you grow in your career. This is advantageous to you, and you should do everything you can to demonstrate you are a trustworthy guardian of that information. People might flatter you to get that information, or offer an information swap. But the only way to encourage more information flow to you is to show that you can keep a secret. If it helps, I've started flatly saying \"that's not my info to discuss\" and people usually get the hint. I always think about Christopher Lee, who fought in the British Special Forces in World War 2 before his legendary acting career. When pried for information about what he did in the War, he would say: \"**Can you keep a secret? Well, so can I.**\"\n\n**Inbound vs Outbound Personal Marketing.** Borrowing from Hubspot's [Inbound marketing](https://www.hubspot.com/inbound-marketing) and Seth Godin's [Permission marketing](https://seths.blog/2008/01/permission-mark/). **Outbound Personal Marketing** is what most people do what they look for jobs - only when they need it, and trawling through reams of job listings and putting their CV in the pile with everyone else. **Inbound Personal Marketing** is what you'll end up doing if you do everything here right - people (prospective bosses and coworkers, not recruiters) knowing your work and your interests, and hitting you up on exactly the things you love to do.\n\n**Market Like Nobody's Watching.** Because probably nobody is, when you're just starting out. It's OK, this is your time to experiment, screw up, find your voice. Because normal comfort zones are not set up to market yourself, you should try to **do a little more than you're comfortable** with. An aggressive form of this advice? If you're not getting complaints about how you're showing up everywhere, you're not doing it enough. This makes sense to some people, and is way too upfront and annoying for others. We all have to find our balance - it's your name on the line after all.\n\n**Market Like One Person's Watching.** Marketing is more effective when it is targeted at a specific someone instead of just everyone. Customize your message to audience. Focus on what's in it for them, tell them why they care. People often don't know what they want or why they care - quote their prior selves if possible.\n\n**Market for the Job You Want.** This is a variant of \"Careful what you wish for... You just might get it.\" You'll probably end up getting what you market yourself for... make sure it's something you want!\n\n## Marketing Yourself At Work\n\nIt's both easier and harder to market yourself at work. It's easier, because it's a smaller pond, and your coworkers have no choice but to listen to you. It's harder, because while you have people's attention, abuse will not be tolerated.\n\nIf you are obnoxious online, people can mute you and carry on with their lives. If you are obnoxious at work, it can backfire pretty directly on you. In particular - always share credit where due and never take credit for something that wasn't yours. Of course this applies in public too, but enough people do it at work that I feel the need to remind you.\n\nBasics aside - you probably agree that it's important to ensure you get visibility for the work that you have done. Here are some ideas:\n\n- **Log your own metrics for significant projects.** Before-after latencies. Increase in signups. Reduced cloud spend. Uptime improvement. Increased session time. I'm sure your company has an expensive, comprehensive and well instrumented metrics logging system (this is a joke - one does not exist). **Don't trust it.** It will fail you when you need it most, or be unable to tell the story you want told. Hand collect metrics, links, press coverage. Take screenshots. Collect qualitative anecdotes, quotes, shoutouts. The best time to do this is right after you see a good result - you will never have time in future to go back for it. Stick it in a special place somewhere for a special occasion - like, say, a performance review. If you use Slack, it can be helpful to make your own Slack channel and Slack yourself your own notable achievements. This gives you a nice chronological log of work.\n- **Awesome Status Updates.** Status Updates are a humdrum routine at most workplaces. Most people put no effort into them. You can buck the trend by making them _awesome_ with just a little more effort. I've seen this \"flip the bits\" in team morale, where people realize they can either continue being boring or join in the effort to do the best work of their lives, due to one person doing this.\n- **Unprompted Status Updates.** Sometimes a project is disorganized enough that there aren't even regular updates scheduled. Management probably vaguely knows what is going on, but is too busy to ask for more. You can take leadership in a vacuum simply by doing your own regular status updates.\n- **Do Demos.** Offer to do them every time. This is an internal marketing opportunity that people regularly turn down because of the stress of public speaking or that it is more work. You don't even have to wait for an assigned time for demos - since most workplaces are now at least partially remote and asynchronous, you can simply put up a short recording of your own demo! Good demos will spread virally - and so will you. Caveat: make sure you have the stakeholder approvals you need before you do this - don't demo work that isn't ready for a demo.\n- **Signature Initiative.** I stole this idea from Amazon, but I'm sure other workplaces have terms for this. Basically, something that you do on the side at work, that sidesteps the usual org chart and showcases your abilities and ideas. After pitching the idea unsuccessfully for 2 years, [Zack Argyle convinced Pinterest's CEO to turn Pinterest into a PWA based on a Hackathon project](https://twitter.com/swyx/status/1128141051406032898). This had [tremendous business impact](https://medium.com/dev-channel/a-pinterest-progressive-web-app-performance-case-study-3bd6ed2e6154) and probably made his subsequent career. But don't worry, that's a very high bar, your contributions don't have to be that product related. Simpler initatives I've seen can be just offering more opportunities to share your interests with fellow devs. Start an internal book club. Lunch and learn series. Leaders at [Google](https://www.blog.google/topics/inside-google/talks-google-shared/) and [Etsy](https://www.etsy.com/codeascraft/talks) started an external talk series, why can't you? Matthew Gerstman [started a JavaScript Guild at Dropbox](https://dropbox.tech/frontend/what-we-learned-at-our-first-js-guild-summit), and created a newsletter, forum, and event for hundreds of his fellow engineers to improve their craft. **This is wonderful!** I did similar at Two Sigma, and when I left, my coworkers genuinely said they would miss my sharing and discussions.\n\nFor more ideas on becoming indispensable at work, check out Seth Godin's [Linchpin](https://www.amazon.com/gp/product/1591844096/). You can find a decent summary [here](https://deanyeong.com/reading-note/linchpin/).\n\n## Things That DO NOT MATTER\n\n- **Appealing to Everybody.**\n- **Short term Optimizations.**\n  - Day of the week and Time of Day that you post\n  - Short term analytics (e.g. weekly traffic)\n  - It's not really that they don't matter, it's just that you should be working on more evergreen things that make short term nonsense irrelevant.\n- **Being a Celebrity.**\n  - [Better to be rich and unknown than poor and famous](https://twitter.com/naval/status/1041513555961438208?lang=en). If you can build a successful tech career without being a celebrity, then _absolutely_ do that - unless you just crave attention anyway.\n  - I haven't mentioned followers once in this entire essay. You can buy followers and everyone can tell. It looks sad.\n  - Building real relationships with peers and mentors you respect is way more fulfilling than raw numerical mass appeal.\n\n## Recap\n\nThat was a LOT of high level marketing concepts. Do take a while to digest them. The last section is going to be a grab bag of tactical ideas for marketing yourself - _after_ you get the important details in place.\n\nTo recap:\n\nAssemble your Personal Brand, your Domain, and your Coding Skills/Business Value, then Market Yourself in Public + at Work.\n\n![image](https://user-images.githubusercontent.com/6764957/79151050-59338280-7dfc-11ea-83cb-5a8f4a1f39a0.png)\n\n## Appendix: Marketing Hacks\n\nWelcome back. Here's a list of \"hacks\" that can get you quick wins with Marketing Yourself. Try them out and let me know how it goes!\n\n- **Help Others Market.** This is so simple as to feel \"dumb\" even pointing it out, but it works. You want practice in marketing, but don't want to take the full plunge yourself, or don't feel like you have something to offer yet. You can find others who are brilliant but uninterested in marketing, and offer free marketing help!\n- **Crosspost on Industry Blogs.** A nice way to get attention for your work is to do great work on someone else's platform. Industry blogs (and newsletters) are pretty much always looking for quality content. For Frontend Dev, the ones with rigorous editing are CSS Tricks, Smashing Magazine, and A List Apart.\n- **Collaborations.** Related to crossposting and helping others - basically you can raise your profile by working with others who already have very high profiles. Justin Mares bootstrapped his own profile by [coauthoring a book](https://www.amazon.com/Traction-Startup-Achieve-Explosive-Customer/dp/1591848369) with Gabe Weinberg, Founder of DuckDuckGo. Same for [Blake Masters with Peter Thiel](https://en.wikipedia.org/wiki/Zero_to_One). Basically if you can work out a non exploitative deal where you do a bunch of legwork but learn a lot, and then copublish with the author, take it. That's a rare deal; most often you will just be [Picking Up What They Put Down](https://www.swyx.io/writing/learn-in-public-hack/) and working your way to become a peer the slow way. If they have a meetup, forum, podcast, or whatever platform, show up on theirs, and then get them to show up on yours.\n- **Industry Awards.** Some people set a lot of weight by awards and certifications. Well regarded programs include Microsoft MVP, Google GDE, AWS Heroes. As a pure signaling mechanism, it works like anything else works - an unhappy mix of credible, gamified, and incomplete. But having a bunch of logos on your site/slides generally help you, so long as they are not your biggest claim to fame.\n- **Memorable words/catch phrase/motto.** This is used by companies and reality stars alike, and can be a bit tacky if you try to, well, [make fetch happen](https://www.youtube.com/watch?v=Pubd-spHN-0), but if you strike a nerve and capture the zeitgeist you can really carry your message far. Nike spends _billions_ to make sure that every time you think of the words \"Just Do It\", they come up. You can do that too.\n- **Friendcatchers.** [Make them](https://www.swyx.io/writing/friendcatchers/).\n- **Visualize your work.** If you draw, you can be WORLD BEATING at marketing. Draw everything you can. [Even the invisible stuff](https://illustrated.dev/drawinginvisibles1). ESPECIALLY the invisible stuff.\n  - If you say you cannot draw, that's a lie. Use [Excalidraw](https://excalidraw.com/).\n- **Elevator Pitch.** In the old days, this pitch was literally for when you ran into the decisionmaker in a 30 second elevator ride. A typical template goes something like \"if you're a `<role>` who `<point of view>`, I/my thing `<what it does>` in `<some eyepopping metric>`\". In this day of both TikTok and podcasts, attention spans are both shorter and longer than that. You need to tailor your elevator pitch accordingly. Be able to sell yourself/your idea/project in:\n  - 1 hour\n  - 15 minutes\n  - 5 minutes\n  - 30 seconds\n  - 280 characters (a tweet)\n  - (stretch goal) [2 words](https://www.swyx.io/writing/two-words/)\n- **Summarize the top 3 books in your field for your blog.** This idea is often attributed to Tim Ferriss, but I'm sure multiple people have come up with it. The idea is you don't have to be original to have a great blog - it's easy to bootstrap your web presence - and your own expertise - by covering existing ground. If you do it very well you can rise to prominence purely because of it - Shane Parrish and Mike Dariano built [Farnam Street](http://www.farnamstreetblog.com/) and [The Waiter's Pad](https://thewaiterspad.com/) purely off summaries. Then practice marketing your summaries - Syndicate on Twitter. Make talks out of it at work and on YouTube.\n\n---\n\nAuthor's Note: *This is the Marketing Yourself chapter of my upcoming book on [Cracking the Coding Career](https://gumroad.com/products/bAZJq/). If you liked this, come check out the rest of the topics!*"
  },
  {
    "slug": "i-m-writing-a-book-45a8",
    "data": {
      "title": "I'm Writing A Book!",
      "description": "I'm writing a Dev Career Advice book... and I'm scared shitless!",
      "tag_list": [
        "career",
        "webdev"
      ]
    },
    "content": "\n## TL;DR\n\nI'm writing a Dev Career Advice book! It will have all my best thoughts for early career devs looking to go from Code Newbie to Senior Dev. ~~I'm putting up [100 copies for presale](https://gumroad.com/l/bAZJq/Presale) for 50% off!~~ You can now find the book on it's own site at [www.LearnInPublic.org](https://www.LearnInPublic.org/)!\n\nIf you'd like behind-the-scenes update of the book writing process, you can follow [the Book's Twitter account](https://twitter.com/Coding_Career) or [the IndieHackers page](https://www.indiehackers.com/product/cracking-the-coding-career).\n\n## A little about Me\n\nHi, I'm [swyx](https://twitter.com/swyx)! 👋\n\nDeconstructing career success is a passion of mine. Throughout my own career change journey, I have been looking for common traits among top performers and trying to reverse engineer their formulas for success.\n\nI've applied them to my own career with great results. I went from [deciding to learn web development in 2017](https://medium.com/hackernoon/no-zero-days-my-path-from-code-newbie-to-full-stack-developer-in-12-months-214122a8948f), to starting my first dev job in 2018, to getting hired as a senior dev at a top tech firm in 2020!\n\n## Why I'm Writing This Now\n\nI have an extensive track record of technical and nontechnical blogging. Yet, my best received posts have been career advice pieces, helping hundreds of thousands of developers:\n\n- [Learn in Public](https://www.swyx.io/writing/learn-in-public)\n- [The Ultimate Hack for Learning In Public: Pick Up What They Put Down](https://www.swyx.io/writing/learn-in-public-hack)\n- [How to Learn in Private](https://www.swyx.io/writing/learn-in-private)\n\nI could write a technical book for my first book, but somehow my gut tells me that a \"soft skills\" one would help a lot more people, especially those who have recently decided to start a dev career.\n\n## Topics Covered\n\nLearning is just the tip of the iceberg as far as savvy career building goes. Here are some juicy topics I have planned:\n\n- **Dev Careers By the Numbers**: An examination of the facts we have on dev careers\n- **The Operating System of You**: We humans mostly run on the same hardware. It makes sense that performance differences are strongly influenced by the software we run - primarily our \"operating system\"!\n- **Tests Are Your Friend**: In more ways than one...\n- **When Ignorance is Power**: how to turn your impostor syndrome into a strength\n- **Marketing Yourself Without Being a Celebrity**\n- **Know Your Tools**: A poor craftsperson blames their tools. A rich craftsperson knows them inside out.\n- **Betting on Technologies**: How to place your career bets (and cut your losses EARLY)\n- **Developer's Guide to Twitter**\n- **The Benefits of Writing A Lot For Your Software Career**\n- **Don't Go Home With Nothing**\n- **You Need A Domain**\n- **The Price of your Ego**: You can learn so much on the Internet, for the low, low price of your Ego\n- **Startups vs BigCos**: Interviewing, Financial Outcomes and Career Growth\n- **How and When to Go Remote**: Advice for devs who want to succeed as remote devs!\n- **From Junior to Senior**: How do you get to the next level?\n- What else do you want me to cover? 🤗\n\n[A full Table of Contents is available on the Gumroad page](https://gumroad.com/products/bAZJq/).\n\n## That's Great!... What's it Called?\n\nBelieve it or not I spent hours agonizing over this question. I think I finally have an answer:\n\nMy new book will be called [**The Coding Career Handbook**](https://www.LearnInPublic.org/)! 🎉\n\nI am starting to write it *today*. So it's definitely not ready for purchase yet, but if you'd like to show some support and win my undying gratitude, ~~[I'm putting up 100 copies for presale here with a 50% discount!](https://gumroad.com/l/bAZJq/Presale)~~ you can [buy a presale copy here](https://www.LearnInPublic.org)!\n\nI'll keep posting excerpts on this blog, and take and respond to your questions as I go along.\n\nIt's really hard to write career advice when I'm just one person. This is the scariest, most uncomfortable thing I've done in years. But I believe I can help those of you who choose to join me on this journey, and want to build exceptional dev careers!"
  },
  {
    "slug": "dynamodb-book",
    "data": {
      "title": "5 Things I Learned from The DynamoDB Book",
      "description": "Reviewing Alex DeBrie's new the DynamoDB Book",
      "tag_list": [
        "books",
        "dynamodb",
        "aws",
        "database"
      ]
    },
    "content": "\nI've written before that [reading books cover to cover](https://www.swyx.io/writing/learn-in-private/#improving-what-goes-in) is one of the best ways for intermediate devs to discover gaps in knowledge and strengthen their knowledge of design patterns. \n\nOver the past couple weeks I've had the opportunity to review Alex DeBrie's new [The DynamoDB Book](https://www.dynamodbbook.com/). It is excellent and sets a very high bar for accessible technical writing of a very abstract and complex subject.\n\n## My Context\n\nI've never used DynamoDB in production, so I am not qualified to assess technical merit - there are other experts to do that. I write in order to share my experience of the book as a DynamoDB *beginner*.\n\nI'm not new to NoSQL - I've actually spent more time with Fauna, Firebase and MongoDB than I have Postgres, MySQL and SQLite - but NoSQL kind of follows a perversion of the [Anna Karenina principle](https://en.wikipedia.org/wiki/Anna_Karenina_principle) \n\n> \"All SQL DBs are alike; each NoSQL DB is NoSQL in its own way.\"\n\nMy interest is personal though - [my next job](https://www.swyx.io/writing/farewell-netlify/) will use DynamoDB extensively, and in any case I expect that knowing foundational AWS services will be relevant for the rest of my life, and therefore a very good use of time.\n\n## Structure\n\nThis 450 page book has 22 chapters. There's no explicit grouping of them but here are my unofficial groupings:\n\n- Chapters 1-6 (~120 pages): Explaining DynamoDB Concepts, APIs, and Expressions\n  - Related writing (not part of the book but representative) - [DynamoDB Transactions: Use Cases and Examples](https://www.alexdebrie.com/posts/dynamodb-transactions/), [DynamoDBGuide.com](https://www.dynamodbguide.com/what-is-dynamo-db)\n- Chapters 7-9 (~50 pages): Advice for DynamoDB Data Modeling/Implementation\n  - You can sample Ch. 8 - [The What, Why, and When of Single-Table Design with DynamoDB](https://www.alexdebrie.com/posts/dynamodb-single-table/)\n- Chapters 10-16 (~90 pages): Strategies for one-to-many, many-to-many, filtering, sorting, migrations, and others\n  - You can sample Ch. 11 - [Strategies for oneto-many relationships](https://www.alexdebrie.com/posts/dynamodb-one-to-many/)\n- Chapters 17-22 (~150 pages): 5 increasingly complex Data Modeling Examples - modeling a Session Store, Ecommerce App, a CMS backed Deals app, GitHub's backend and Migrations for the GitHub backend\n\nI write these details down to emphasize the ridiculous amount of thought put into this book. A lesser book would stop at Chapter 9 - mostly explaining factual statements in a more accessible way than the official docs, and then ending with some advice. \n\nBut, in fact, the *majority* of the book comes in Chapters 10-22 - chock full of hard won advice, and worked examples for you to apply what you just learned, designed to prepare you for every real world scenario. This truly goes above and beyond, and turns the book from a read-once-and-discard deal into a reusable reference tome you will consult for the entirety of your DynamoDB usage.\n\n## 5 Things I Learned\n\nThere's way too much to write down, but I figured I should force myself to make some notes to process in public for myself and others.\n\n### 1. Why Generic PK and SK names\n\nDynamoDB is marketed as a \"key value store\". The keys are split into Partition Keys (PK) and Sort Keys (SK) which helps DynamoDB scale behind the scenes, but also opens up some query patterns that let you do a lot more than simple key value lookup. \n\nBecause of the benefits of [Single Table Design](https://www.alexdebrie.com/posts/dynamodb-single-table/), we *overload* the meaning of PKs and SKs to accommodate multiple domain objects, and use `{TYPE}#{ID}` conventions within the key rather than designate a fixed type to each key. An SK can consist of both `ORG#ORGNAME` and `USER#USERNAME` in the same item collection. This lets you query both in a single query.\n\nIf this makes you queasy, I have't fully made peace with it too. It's basically the developer contorting themselves to fit the abstraction leak of DynamoDB. I rationalize it by basically regarding DynamoDB as a low level tool - it is closer to a linear [memory address register](https://en.wikipedia.org/wiki/Memory_address_register) than a DB. \n\nThink about it - DynamoDB promises single digit millisecond latency, but in exchange you have to be hyperaware which address you are slotting your data in and manage it carefully. That's just like dealing with low level memory!\n\nWith a low level tool, you understand that you need to manage more of its details, but in exchange it gives you more performance than anything else possible. This is backed up by Alex quoting [Forrest Brazeal](https://www.trek10.com/blog/dynamodb-single-table-relational-modeling): \n\n> A well-optimized single-table DynamoDB layout looks more like machine code than a simple spreadsheet.\n\nYou could build a DB layer atop DynamoDB that abstracts away these PK/SK shenanigans, but you do run the risk of making very inefficient queries because you have no control/knowledge over the data modeling. Maybe you care, maybe you don't (maybe you're no worse off from the inefficient queries made in SQL). There's a whole section called \"Don't use an ORM\" in the book, though [Jeremy Daly's DynamoDB Toolbox](https://github.com/jeremydaly/dynamodb-toolbox) and [AWS' Document Client](https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/dynamodb-example-document-client.html) are OK. But it is clear that for stable data access patterns (eg you intend to run Amazon.com until the heat death of the universe), taking over low level PK/SK modeling details for DynamoDB will yield best possible results.\n\n### 2. Why Global Secondary Indexes\n\nThere are two types of Secondary Indexes in DynamoDB - Local and Global (aka LSI and GSI). LSIs can use the same PKs as the main table, but a different SK, whereas GSIs can use any attributes for both PK and SK.\n\n|      | LSI                                                               | GSI                                                            |\n|------|-------------------------------------------------------------------|----------------------------------------------------------------|\n| Pros | Option for strongly-consistent reads                              | PK flexibility, Creation time flexibility.                     |\n| Cons | Must use same PK as table. Must be created when table is created. | Eventual consistency. Needs additional throughput provisioning |\n\nGiven the importance of flexibility over strong consistency, it's clear why GSIs are so much more popular than LSIs. I don't have any numbers but vaguely also recall seeing on the Twitterverse that GSI replication delays are very rarely a problem. \n\nI wonder if AWS publishes p99 GSI replication numbers.\n\n## 3. KSUIDs for Unique, Sortable IDs\n\nK-Sortable Unique Identifiers (KSUIDs) are a modification of the UUID standard [by the fine folks at Segment](https://segment.com/blog/a-brief-history-of-the-uuid/) that encodes a timestamp while also retaining chronological ordering when sorting as a string.\n\n- Here's a UUIDv4: `96fb6bdc-7507-4879-997f-8978e0ba0e68`\n- Here's a KSUID: `1YnlHOfSSk3DhX4BR6lMAceAo1V`\n\nThe benefit of using a KSUID compared to a UUID is that KSUIDs are **lexicographically sortable**. KSUIDs embed a timestamp, which you can decode and sort if you have a [ksuid implementation](https://github.com/segmentio/ksuid) handy - but also if you simply sort by the generated ID's they will sort themselves out chronologically (without any knowledge of how to decode KSUIDs!). \n\nThis feature makes KSUIDs ideal as unique identifiers for DynamoDB keys, where you can use a condition expression like `#id BETWEEN :start and :end` where `:start` and `:end` represent starting and ending ID's of a range you want to query.\n\nI don't know how widely KSUIDs are known given this idea was only released in 2017, but I think this is useful even beyond DynamoDB.\n\n## 4. Sparse Indexes for Filtering\n\nA sparse (secondary) index *intentionally* excludes certain items from your table to help satisfy a query (aka not merely as a result of PK/SK overloading).\n\nWhen you write an item, DynamoDB only copies it to the secondary index if the item has elements of the specified key schema for that index. This is useful in two ways:\n\n- **Using sparse indexes to provide a global\nfilter on an item type**\n  - Example: You have a list of Organizations (PK), each Organization has Members (SK), a few Members are Admins (role represented attributes). You want to query for Admins, without pulling ALL members of every organization. Setting up a sparse index that only includes Admins then lets you quickly and efficiently query them.\n- **Using sparse indexes to project a single\ntype of entity**\n  - Example: You have customers, orders, and inventory linearly laid out in PK/SKs in a single table. You want to query for all customers only. Setting up a sparse index that only includes customers helps you do that.\n\nI think you can regard this as a \"filter\" or \"projection\", depending on your functional/linear/relational algebra preferences :)\n\n## 5. Normalization is OK\n\nYou'd think that Normalization is anathema in NoSQL, but it is actually recommended as one of the many-to-many strategies in the book! The given example is Twitter - a user follows other users, and other users follow yet more users. A social network. Every tweet or change in profile would cause a huge fan-out, aka \"write thrashing\".\n\nThe recommended solution is storing User as PK, and then the SK has both User (again) and `FOLLOWING#<Username>`. When one user wants to view users they follow, we:\n\n- use the Query API to fetch the User's info and initial few users they follow\n- use the BatchGetItem API to fetch detailed User info for each user followed.\n\nThis is making multiple requests, but there is no way around it. This pattern is also applicable in ecommerce shopping carts.\n\n## Conclusion\n\nI mean. The premium package costs $249 ($199 for launch) for now. If you were to hire Alex to even do a consultation phone call or workshop for you you'd need at least 10x that. If you use or will use DynamoDB in any serious way, you would save a ton of time and pain and money by going through this book.\n\n## Disclaimers\n\nI reviewed a free prepublication draft of the book, which was provided unconditionally to me as I happened to be responding to Alex's email newsletter for the book. I receive no other compensation from doing this."
  },
  {
    "slug": "how-to-use-class-instead-of-classname-with-preact-and-typescript-2bjh",
    "data": {
      "title": "How to Use class instead of className with Preact and TypeScript",
      "description": "Bottom Line Up Front   If you are using TypeScript with Preact aliased as React, you can add...",
      "tag_list": [
        "preact",
        "typescript",
        "dx"
      ]
    },
    "content": "\n## Bottom Line Up Front\n\nIf you are using TypeScript with Preact aliased as React, you can add an ambient declaration to use `class` instead of `className`:\n\n```ts\n// anyname.d.ts - place this anywhere in your project\ndeclare namespace React {\n  interface HTMLAttributes<T> {\n    // Preact supports using \"class\" instead of \"classname\" - need to teach typescript\n    class?: string;\n  }\n}\n```\n\n## My Context\n\nMost React users aren't using every feature of React. For most usecases, using Preact is exactly equivalent. As point of proof: This blogpost you are reading is [written in a custom CMS in Preact](https://github.com/sw-yx/dev-to-cms), by me, a React dev who has never used Preact and only skimmed [the Differences page on their docs](https://preactjs.com/guide/v10/differences-to-react).\n\nThis is a good idea because of the limited downside - you can swap back to React in a single command if you end up needing its power - and the instant upside - A [Next.js + Preact page bundle will now come in as low as 20kb of JS](https://twitter.com/_developit/status/1234595685611208704) whereas a trip to [the Create Next App docs](https://create-next-app.github.io/) will download at least 100kb of JS off the bat (note: these aren't apples to apples comparisons). If you'd like to give it a try, I've been building [a small Preact + Next.js + TypeScript + TailwindCSS starter repo](https://github.com/sw-yx/smaller-safer-serverless-starter).\n\nHowever I'm not here to write about anything as important as all that 😂. I'm here writing about a much smaller benefit of using Preact - [you can use `class` instead of `className`](https://preactjs.com/guide/v10/differences-to-react/#raw-html-attributeproperty-names)!\n\n## Why `class` over `className`\n\nThis is mainly a pain point because I use [Tailwind UI](https://tailwindui.com/components), which involves a lot of copying and pasting mountains of code that looks like this:\n\n```html\n<div class=\"relative bg-gray-50 overflow-hidden\">\n  <div class=\"hidden sm:block sm:absolute sm:inset-y-0 sm:h-full sm:w-full\">\n    <div class=\"relative h-full max-w-screen-xl mx-auto\">\n      <svg class=\"absolute right-full transform translate-y-1/4 translate-x-1/4 lg:translate-x-1/2\" width=\"404\" height=\"784\" fill=\"none\" viewBox=\"0 0 404 784\">\n        <defs>\n          <pattern id=\"f210dbf6-a58d-4871-961e-36d5016a0f49\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" patternUnits=\"userSpaceOnUse\">\n            <rect x=\"0\" y=\"0\" width=\"4\" height=\"4\" class=\"text-gray-200\" fill=\"currentColor\" />\n          </pattern>\n        </defs>\n        <rect width=\"404\" height=\"784\" fill=\"url(#f210dbf6-a58d-4871-961e-36d5016a0f49)\" />\n      </svg>\n      <svg class=\"absolute left-full transform -translate-y-3/4 -translate-x-1/4 md:-translate-y-1/2 lg:-translate-x-1/2\" width=\"404\" height=\"784\" fill=\"none\" viewBox=\"0 0 404 784\">\n        <defs>\n          <pattern id=\"5d0dd344-b041-4d26-bec4-8d33ea57ec9b\" x=\"0\" y=\"0\" width=\"20\" height=\"20\" patternUnits=\"userSpaceOnUse\">\n            <rect x=\"0\" y=\"0\" width=\"4\" height=\"4\" class=\"text-gray-200\" fill=\"currentColor\" />\n          </pattern>\n        </defs>\n        <rect width=\"404\" height=\"784\" fill=\"url(#5d0dd344-b041-4d26-bec4-8d33ea57ec9b)\" />\n      </svg>\n    </div>\n  </div>\n```\n\n## The Problem\n\nNow because [I'm using Next.js and TypeScript with Preact](https://github.com/developit/nextjs-preact-demo), I use Preact with a React alias - basically lying to TypeScript that we are using React so we benefit from it's mature tooling across VS Code and Next.js.\n\nHowever React doesn't use `class` for classes, it uses `className`! (At least until [React Fire](https://github.com/facebook/react/issues/13525) lands.) So I have two choices:\n\n- either go through and rename every `class` to `className` - like a heathen - every time I use Tailwind\n- or try to use `class` as Preact lets me do\n\nThe problem goes back to what I stated above: we lied to TypeScript that we're using React, so it's not going to let us use `class`:\n\nThis:\n\n```jsx\n<div class=\"bg-black\">Does this work?</div>\n```\n\nLeads to:\n\n```\n(JSX attribute) class: string\nType '{ children: string; href: string; class: string; }' is not assignable to type 'DetailedHTMLProps<AnchorHTMLAttributes<HTMLAnchorElement>, HTMLAnchorElement>'.\n  Property 'class' does not exist on type 'DetailedHTMLProps<AnchorHTMLAttributes<HTMLAnchorElement>, HTMLAnchorElement>'.ts(2322)\nPeek Problem\nNo quick fixes available\n```\n\nOh no! No quick fixes available??\n\nLies.\n\n## The Quick Fix\n\nHere's the fix. Add a [TypeScript ambient declaration](https://www.typescriptlang.org/docs/handbook/modules.html#ambient-modules) - basically a `anyname.d.ts` file anywhere in your project, assuming default `tsconfig` settings and add this:\n\n```ts\n// anyname.d.ts - place this anywhere in your project\ndeclare namespace React {\n  interface HTMLAttributes<T> {\n    // Preact supports using \"class\" instead of \"classname\" - need to teach typescript\n    class?: string;\n  }\n}\n```\n\nAnd now I can write `class` in my React code!\n\n**If that's all you came to this blogpost for, then we're done**. I'm just going to discuss how I made my way to this solution as an intermediate TypeScript user, since this is a learning opportunity for broader TypeScript use.\n\n> ⚠️ If you're also using Tailwind with Next.js, there is one more issue to resolve - disabling the styled-jsx plugin. More details here: https://github.com/zeit/next.js/issues/11675\n\n## Appendix: Declaration Merging to Patch Definitions\n\nOne thing I solidified when I read [Boris Cherny's TypeScript book](https://mobile.twitter.com/swyx/status/1135525665971695617) is the power of [Declaration Merging](https://www.typescriptlang.org/docs/handbook/declaration-merging.html) to fix issues with official typings. I even included this as part of [the Troubleshooting Handbook in the React TypeScript Cheatsheet](https://github.com/typescript-cheatsheets/react-typescript-cheatsheet#troubleshooting-handbook-bugs-in-official-typings).\n\nBut I'd only patched libraries before, never patched something in the core behavior of JSX itself.\n\nWhen I googled how to do this, I found [this unhelpful Stackoverflow answer](https://stackoverflow.com/questions/51835611/specify-specific-props-and-accept-general-html-props-in-typescript-react-app) and eventually this [Stackoverflow answer](https://stackoverflow.com/questions/40093655/how-do-i-add-attributes-to-existing-html-elements-in-typescript-jsx):\n\n```ts\ndeclare module 'react' {\n     interface HTMLProps<T> {\n        block?:string;\n        element?:string;\n        modifiers?:string;\n    }\n}\n```\n\nBut when I tried it, it didn't work.\n\nI ultimately resorted to looking up [the React typings in DefinitelyTyped itself](https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/types/react/index.d.ts), and realizing that React was exported as a `namespace`. Since Namespaces are a more arcane feature of TypeScript, I've never really used them in application code. \n\nBut it was enough to go on - I swapped `declare module 'react'` with `declare namespace React` and prayed that declaration merging worked.\n\nIt did. And now you know too."
  },
  {
    "slug": "spot-the-difference",
    "data": {
      "title": "Want Better Design/CSS Skills? Spot the Difference!",
      "description": "A quick exercise to improve your attention to detail when implementing/designing with CSS.",
      "tag_list": [
        "css",
        "design",
        "tailwindcss",
        "tailwind"
      ]
    },
    "content": "\nCan you spot the differences between Exhibit A:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/l60af0mjgrlf3unoe6do.png)\n\nAnd Exhibit B:\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/np9d3b9ihvc30r6lzgqd.png)\n\nWhich one looks subjectively better?\n\nWould it help to know *how many* differences to look for?  I'll tell you down below, but if you like the challenge, give this a try :)\n\n## Design Eye\n\nI think a key difference between designers and frontend developers is \"Design Eye\". \n\nI can't tell you how many times I've gotten a spec from a designer, implemented it, and handed it back only for them to completely tear it apart pointing out the small details I missed.\n\nOr maybe you know the feeling when you design something and it kinda sorta looks fine, but then you put it up next to something a designer made and it looks like crap. *And you can't even explain why* - because you don't even have the vocabulary!\n\nThat's \"Design Eye\".\n\nI mean, I like to believe I'm detail oriented where it counts, e.g. if a comma goes where it shouldn't in code, you bet I'm all over that. But I'm nothing compared to designers. Heck, their leading industry podcast is even *called* [Design Details](https://designdetails.fm/)!\n\nSo I'd like to train my \"Design Eye\".\n\n## Spotting the Differences\n\nThere are **3** differences between Exhibit A and B above. They range from Easy, Medium, and *Hard*. If you didn't get the 2nd or 3rd one, I don't blame you.\n\n> 💁‍♂️ Here's your chance to scroll up and look again if you'd like to test yourself.\n\nI've discussed the Ben Franklin method before in my post on [How To Learn In Private](https://www.swyx.io/writing/learn-in-private/#getting-more-out-of-what-goes-in#getting-more-out-of-what-goes-in). Basically, you look at something you want to make, then try to make it to the best of your ability, and then get an expert (or yourself) to critique the difference.\n\nToday I realized I could apply the Ben Franklin method to training my \"Design Eye\". I saw a request to code up this screenshot:\n\n![https://pbs.twimg.com/media/EUvfoJ7UEAErtuJ?format=jpg&name=medium](https://pbs.twimg.com/media/EUvfoJ7UEAErtuJ?format=jpg&name=medium)\n\nWhich I then turned into **Exhibit A** - not a faithful reproduction, but \"Good Enough\", I thought, and certainly better than anything I've done on my own before:\n\n{% codepen https://codepen.io/swyx/pen/QWbRYZL default-tab=result %}\n\nI then [tweeted it out](https://twitter.com/swyx/status/1246335697016913920), and immediately [Alex Clark](https://twitter.com/AlexClark_NZ/status/1246360474901340160) and [Darian Moody](https://twitter.com/djm_/status/1246361505995644929) found problems with it 😂 [Cunningham's Law](https://meta.wikimedia.org/wiki/Cunningham%27s_Law) never fails!\n\nSo I asked Darian to teach me, and [he obliged by forking my Codepen](https://twitter.com/djm_/status/1246398388389756929) and producing **Exhibit B**:\n\n{% codepen https://codepen.io/djm_/pen/poJmmxJ default-tab=result %}\n\nAnd now time to learn by spotting the difference!\n\n## Difference 1: Line Height\n\nThis is the \"Easy\" one. Line Heights should be adjusted relative to the size of the text - a rare instance when too much spacing is not better when it comes to design.\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/5qhxc5cswyh9mq85bzdo.png)\n\nIn Tailwind this maps to the `leading-tight` class.\n\n## Difference 2: Letter Spacing\n\nThis is \"Medium\" - but also very very hard for me. But for some it's just obvious that the letters are too far apart:\n\n![spotthedif2](https://user-images.githubusercontent.com/6764957/78453793-a6d52000-7683-11ea-8d22-7eafc098df22.gif)\n\nIn Tailwind this maps to the `tracking-tight` class.\n\n## Difference 3: Text Color\n\nText colors should match their surroundings to provide a gentler visual transition. This is \"Hard\" - even when you tell me its there, I struggle to see it, but it's obvious for designers:\n\n![spotthedif3](https://user-images.githubusercontent.com/6764957/78453915-409ccd00-7684-11ea-9e27-66057a02fd0b.gif)\n\nIn Tailwind this maps to the `text-{color}-{weight}` class.\n\nThis is the kind of extreme detail Steve Schoger teaches in [his talks](https://www.youtube.com/watch?v=7Z9rrryIOC4) and in [Refactoring UI](http://refactoringui.com/), but it really takes practice to see it.\n\n## Conclusion\n\nI learned a few new tricks going through this exercise today, and I imagine I will need to do a few more rounds of this for it to really stick.\n\nDo you want to try learning how to refactor your own UI's this way? Maybe play a few games of \"Spot the Difference\"!\n\nSure, these details aren't as important as getting the big picture right. But I think attention to detail is what distinguishes the premium apps from the merely OK.\n"
  },
  {
    "slug": "tailwind-unreset",
    "data": {
      "title": "How and Why to Un-Reset Tailwind's CSS Reset",
      "description": "Tailwind CSS comes with a great CSS Reset, called Preflight. It starts with the awesome Normalize.css...",
      "tag_list": [
        "tailwind",
        "css",
        "nextjs",
        "tailwindcss"
      ]
    },
    "content": "\nTailwind CSS comes with a great CSS Reset, called [Preflight](https://tailwindcss.com/docs/preflight/#app). It starts with the awesome [Normalize.css project](http://necolas.github.io/normalize.css/), and then nukes all default margins, styling, and borders for every HTML element. This is so that you have a consistent, predictable starting point with which to apply your visual utility classes separate from the semantic element names.\n\nThat's great. So why would you want to _unreset_ a CSS Reset?!\n\n> [Note from Adam Wathan](https://twitter.com/adamwathan/status/1246426888454750209?s=20) - they are working on a [Tailwind plugin](https://pensive-agnesi-f6bc76.netlify.com/) to do this for you *properly* rather than my janky unofficial solution. See also [his talk on Tailwind CSS Best Practices](https://www.youtube.com/watch?v=J_7_mnFSLDg&feature=youtu.be&t=727).\n\n## The Why\n\nWithout unresetting, here's how this post you're reading would look on [my own demo site](https://github.com/sw-yx/smaller-safer-serverless-starter):\n\n![image](https://user-images.githubusercontent.com/6764957/78451345-a84b1c00-7674-11ea-9e72-672bb7d295f5.png)\n\nThis is because this blogpost is authored in Markdown, processed through [JDown](https://github.com/DanWebb/jdown), and then rendered into our Next.js app with [React's `dangerouslySetInnerHTML` API](https://reactjs.org/docs/dom-elements.html#dangerouslysetinnerhtml). Despite the name, this is the main way to inject externally generated HTML into Preact/React pages:\n\n```jsx\n<div dangerouslySetInnerHTML={{ __html: post.contents }} />\n```\n\nHowever, this content comes _without_ any Tailwind classes, and because I'm writing this in Markdown, there's really [no way to add the Tailwind classes in to each element](https://www.swyx.io/writing/markdown-mistakes/#4-no-syntax-for-adding-classes) - nor really would you want to.\n\nThe solution, [prescribed on the Tailwind docs](https://tailwindcss.com/docs/adding-base-styles/), is to Add Base Styles for the components you want to render like \"normal\". Effectively doing a CSS \"Un-reset\"!\n\n## The How\n\nI'm not the first to this idea - and a quick trip to Google yielded this [Unreset.scss](https://raw.githubusercontent.com/ixkaito/unreset-css/master/_unreset.scss) file. Assuming you're using SASS, you could basically copy it over to your `tailwind.scss` and namespace it under an `unreset` class:\n\n```scss\n.unreset {\n  // paste unreset.scss here!\n}\n```\n\nAnd then in your JSX, you can add that `unreset` class in:\n\n```jsx\n<div className=\"unreset\" dangerouslySetInnerHTML={{ __html: post.contents }} />\n```\n\nJob done! Right?\n\n## The How - Tailwind style!\n\nNah, I still don't love it. First of all, any customization to colors and margins that you've done won't be respected, because you've \"Un-Reset\" them to [magic numbers](https://twitter.com/adamwathan/status/939843271768948736). Your blog content will look inconsistent from the rest of your site 💩\n\nThe Tailwind Way™ to do it would be to go through all the `unreset.css` styles and translate them to Tailwind classes as far as possible, so that they will conform as close as possible.\n\n```scss\n.unreset {\n  a {\n    @apply text-blue-700 underline;\n  }\n  p {\n    @apply my-4;\n  }\n  // etc...\n}\n```\n\nSince [Tailwind's Preflight CSS Reset](https://unpkg.com/tailwindcss@1.2.0/dist/base.css) doesn't reset absolutely _everything_, you should really diff Preflight against `unreset.css` in order to figure out what to unreset, so as not ship excess CSS.\n\nSound like a chore? It is. **[I've done it for you here!](https://gist.github.com/sw-yx/28c25962485101ca291ec1947b9d0b3e)**. You can also [see it in action on this demo](https://github.com/sw-yx/smaller-safer-serverless-starter).\n\n## Video Demo\n\nI recorded this as a video for my ongoing Dev.to CMS project!\n\n- Youtube Link: https://youtu.be/iLEYtgBezhs\n- Dev.to Embed: {% youtube iLEYtgBezhs %}\n\n## Conclusion\n\nHere's my demo site after unresetting:\n\n![image](https://user-images.githubusercontent.com/6764957/78451324-82be1280-7674-11ea-8921-c3d298c8ef64.png)\n\nOf course, like everything Tailwind, feel free to customize to match your exact usecase. I just helped you get started.\n\nThat's it for this blogpost, thanks for reading!"
  },
  {
    "slug": "instant-graphql-onegraph",
    "data": {
      "title": "Instant GraphQL with OneGraph",
      "description": "Here's a recorded screenshare chat I had with Sean Grove (https://twitter.com/sgrove), Cofounder of O...",
      "tag_list": [
        "screenshares"
      ]
    },
    "content": "\nHere's a recorded screenshare chat I had with Sean Grove (https://twitter.com/sgrove), Cofounder of OneGraph! We swapped out the Next.js API Routes that I manually set up in my livestreaming, to the premade GraphQL integration that OneGraph has with Dev.to. As a bonus, this takes care of user authentication so it doesn't rely on getting Dev.to API keys!\n\n- Youtube Link: https://youtu.be/4NaRQ3B-3fg\n- Dev.to Embed: {% youtube 4NaRQ3B-3fg %}\n\nSource: https://github.com/sw-yx/dev-to-cms/commit/150ce146e54f9b63911917b83b24c993341f9496\nLive Demo: https://dev-to-cms.now.sh/\n\n\n## Catch up on the Dev.to CMS LiveStream!\n\n- [Day 1 - Setup Next.js and Tailwind UI, list posts through API routes](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-1-2ad1) - 90 mins\n- [Day 2 - setting up a Markdown Editor with Next.js, Tailwind UI, Highlight.js, React Hook Form, and React Query](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-2-6mf) - 3 hours\n- Quick Fix - [How To Add Monaco Editor to a Next.js app](https://dev.to/swyx/how-to-add-monaco-editor-to-a-next-js-app-ha3) - 18 mins\n- [Day 3 - Refactoring to Edit Existing Posts](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-3-c65) - 3 hours\n- [Day 4 - Polish Day! Implementing Notifications, Markdown preview, and programmatic Redirects, and Using Web Components in Next.js](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-4-6em) - 3 hours\n- [Instant GraphQL with OneGraph - Screenshares in Public with Sean Grove](https://dev.to/swyx/instant-graphql-with-onegraph-4hi8) - refactoring handrolled Dev.to API access with OneGraph and GraphQL\n- [How and Why to Un-Reset Tailwind's CSS Reset](https://dev.to/swyx/how-and-why-to-un-reset-tailwind-s-css-reset-46c5)"
  },
  {
    "slug": "how-to-use-web-components-with-next-js-and-typescript-4gg1",
    "data": {
      "title": "How to use Web Components with Next.js and TypeScript",
      "description": "In my livestream today I had the need to bring in a spinner component to show work in progress in my...",
      "tag_list": [
        "webcomponents",
        "nextjs",
        "typescript",
        "usetheplatform"
      ]
    },
    "content": "In [my livestream today](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-4-6em) I had the need to bring in a spinner component to show work in progress in my app. However found that existing React spinners were too heavy. That's when I had the idea to use web components in my Next.js (React/Preact) app for the first time ever!\n\nWe'll use https://github.com/craigjennings11/wc-spinners/ as a case study.\n\n## Web Components 101\n\nNormally to use a webcomponent in a project you just import it like a library:\n\n```js\n  import 'wc-spinners/dist/atom-spinner.js';\n```\n\nand then use it in our JSX/HTML:\n\n```html\n<div> \n    <atom-spinner/> Loading\n</div>\n```\n\nThe web component will encapsulate behavior and styles without the weight of a framework, which is very nice for us.\n\nHowever when it comes to Next.js and TypeScript, we run into some problems.\n\n## TypeScript and Web Components\n\nWhen you use TypeScript with JSX, it tries to check every element you use to guard against typos. This is a problem when you've just registered a component that of course doesn't exist in the normal DOM:\n\n```\nProperty 'atom-spinner' does not exist on type 'JSX.IntrinsicElements'.ts(2339)\n```\n\nThe solution I got from [this guy](https://wespeter.com/posts/consuming-web-component-react-typescript/) is to use [declaration merging](https://www.typescriptlang.org/docs/handbook/declaration-merging.html) to extend TypeScript:\n\n```ts\n// declarations.d.ts - place it anywhere, this is an ambient module\ndeclare namespace JSX {\n  interface IntrinsicElements {\n    \"atom-spinner\": any;\n  }\n}\n```\n\n## Next.js and Web Components\n\nThe next issue you run into is server side rendering.\n\n```\nReferenceError: HTMLElement is not defined\n```\n\nWC's famously don't have a great SSR story. If you need to SSR WC's, it seems the common recommendation is to use a framework like Stencil.js. I have no experience with that.\n\nSince in my usecase I only needed the WC clientside, I found that I could simply defer loading the WC:\n\n```js\nfunction Component() {\n  React.useEffect(() => import(\"wc-spinners/dist/atom-spinner.js\")\n  , [])\n  return (<div>\n        // etc\n        <atom-spinner />\n        </div>)\n}\n```\n\nAnd that's that! Enjoy using the platform!"
  },
  {
    "slug": "make-your-own-dev-to-cms-livestream-part-4-6em",
    "data": {
      "title": "Make your own Dev.to CMS livestream - Part 4",
      "description": "This is day 4 of my livecode Dev.to CMS.  Source: https://github.com/sw-yx/dev-to-cms Demo: https://d...",
      "tag_list": [
        "livecoding",
        "preact",
        "typescript",
        "nextjs"
      ]
    },
    "content": "This is day 4 of my livecode Dev.to CMS.\n\nSource: https://github.com/sw-yx/dev-to-cms\nDemo: https://dev-to-cms.now.sh\n\n## Things covered\n\n- [x] Redirect to edit after posting\n- [x] Markdown preview with toggles\n- [x] Async status -> nicer UX when clicking submit\n    - [x] Disable submit button\n    - [x] show spinner while inflight - web component!! https://github.com/craigjennings11/wc-spinners/\n    - [x] Show error! - using notifications as a hook!\n\n## Video\n\n\nHere is [the YouTube livestream (3 hours)](https://www.youtube.com/watch?v=ONVp8QivDv0&feature=youtu.be).\n\nDev.To embed:\n\n{% youtube ONVp8QivDv0 %}\n\nToday we fixed some bugs in the codebase and found new ones!\n\n## Catch up on the Dev.to CMS LiveStream!\n\n- [Day 1 - Setup Next.js and Tailwind UI, list posts through API routes](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-1-2ad1) - 90 mins\n- [Day 2 - setting up a Markdown Editor with Next.js, Tailwind UI, Highlight.js, React Hook Form, and React Query](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-2-6mf) - 3 hours\n- Quick Fix - [How To Add Monaco Editor to a Next.js app](https://dev.to/swyx/how-to-add-monaco-editor-to-a-next-js-app-ha3) - 18 mins\n- [Day 3 - Refactoring to Edit Existing Posts](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-3-c65) - 3 hours\n- [Day 4 - Polish Day! Implementing Notifications, Markdown preview, and programmatic Redirects, and Using Web Components in Next.js](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-4-6em) - 3 hours\n- [Instant GraphQL with OneGraph - Screenshares in Public with Sean Grove](https://dev.to/swyx/instant-graphql-with-onegraph-4hi8) - refactoring handrolled Dev.to API access with OneGraph and GraphQL\n- [How and Why to Un-Reset Tailwind's CSS Reset](https://dev.to/swyx/how-and-why-to-un-reset-tailwind-s-css-reset-46c5)"
  },
  {
    "slug": "a-world-without-plugins-cig",
    "data": {
      "title": "A World Without Plugins",
      "description": "What happens if we did away with plugins altogether? The case for Imperative Recipes",
      "tag_list": [
        "programming",
        "apis",
        "plugins"
      ]
    },
    "content": "\nDevelopers have a complex relationship with plugins. We love making and using them - until things go wrong. Then they can ruin our day.\n\nWhat happens if we did away with plugins altogether?\n\n## What are Plugins?\n\nPlugins mostly take the form of encapsulated code. This code then takes a few lines of **declarative** configuration (sometimes point-and-click, which is great for non-technical users) to set up. The main platform exposes interfaces, like \"hooks\" and \"lifecycle methods\", that then call the plugin's code in (hopefully well documented and predictable) order. This saves the user from writing a lot of code themselves, which saves both the work to write that code *and* the knowledge needed to write it.\n\nFor developers, most plugin configuration looks pretty much the same:\n\n```js\n// my random config file\nconst MyAwesomePlugin = require('my-awesome-plugin');\nmodule.exports = {\n  // etc\n  plugins: [\n    new MyAwesomePlugin({myPluginOptions: 'foobar'}),\n    // etc\n  ]\n};\n```\n\nAnd we go back and forth on whether it should be static (JSON or YAML or TOML) or dynamic (JS or TS or [Dhall](https://github.com/dhall-lang/dhall-lang) or [Prolog](https://yarnpkg.com/features/constraints/)). (PS: [here's a poll I did](https://mobile.twitter.com/swyx/status/1181961991335940096) to show you how opinionated people are about this!)\n\nMost plugin systems also offer \"local\" plugins - plugins that you write yourself without having to be published to a plugin registry. Some platforms are \"self hosting\" - their own first party functionality also uses the same plugin system that they ask everyone else to use.\n\nSome plugins offer a lot of value for you - they handle truly gnarly edge cases and save you thousands of lines of code. Others are merely thin wrappers over existing code, done for the sake of vanity or marketing or piggy backing on an existing popular ecosystem. The user would be better off just writing the code themselves.\n\n## Everybody Gets A Plugin!\n\nWebpack has [plugins](https://webpack.js.org/concepts/plugins/). Babel has [plugins](https://babeljs.io/docs/en/plugins/), while also (confusingly) commonly being used as a plugin to Webpack. Meteor has [plugins](https://docs.meteor.com/api/packagejs.html#build-plugin-api). Apollo Server has [plugins](https://www.apollographql.com/docs/apollo-server/integrations/plugins/). Next.js (may) have [plugins](https://github.com/zeit/next.js/issues/9133). Gatsby has [plugins](https://www.gatsbyjs.org/plugins/) that have [plugins](https://www.gatsbyjs.org/tutorial/remark-plugin-tutorial/) and [plugins that have UI](https://www.gatsbyjs.org/docs/themes/). Redux has [middleware](https://redux.js.org/advanced/middleware). Express has [middleware](https://expressjs.com/en/guide/using-middleware.html).  VSCode has [extensions](https://code.visualstudio.com/docs/editor/extension-gallery). Zsh has [plugins](https://github.com/ohmyzsh/ohmyzsh/wiki/Plugins). Vim has [plugins](https://vimawesome.com/).\n\nI'm just naming stuff in my part of the world. I'm sure you could name more.\n\n## Why Everybody Loves (and Hates) Plugins\n\nEvery platform wants a plugin system. It helps people adjust the platform to their needs without forcing the core platform to develop and maintain that functionality. It helps third parties offer a faster onboarding to their tool with an existing popular platform.\n\nCynically speaking, it helps offload responsibility of the core platform to third party developers, while also making it stickier to users. More positively, it unleashes the creativity and resources of third party developers to scale the platform to usecases that that platform owners didn't even think of. \n\nA lot of professional app development starts with sticking together the right set of plugins to get desired features out of the box. If you know your tools well and the plugin ecosystem is mature, you can get up and running extremely fast.\n\nAt least, that's the promise. **In practice**, plugin quality varies widely, conflicts between plugins arise due to poor scoping, and having an easy resolution when things go wrong is rare. Unless you're churning out the same app again and again, you're always going to be running into and needing or writing some new plugin or other. Nobody is immune.\n\nBtw, good luck if the platform decides it needs to make breaking changes to plugin APIs. You end up having to promise heaven (in features) or hell (in vulnerabilities) to end users in order to justify the development cost of the upgrade - it is not good enough to merely upgrade with no visible change on the surface. And because plugin authors are usually unpaid, the plugins are poorly maintained, and everyone is unhappy.\n\nAt large enough scale, even bugfixes are breaking changes. [Hyrum's Law](https://www.hyrumslaw.com/) says that \"With a sufficient number of users of an API, it does not matter what you promise in the contract - all observable behaviors of your system will be depended on by somebody.\" Even behavior you didn't intend will eventually be relied on, and therefore you can't break (eg. [CSS](https://wiki.csswg.org/ideas/mistakes)).\n\nIs the best developer experience just configuring plugin after plugin after plugin until we quit the industry to a life of peaceful goat farming? Is this the best humanity can do?\n\nWhat would happen if we just didn't have plugins?\n\n## If We Didn't Have Plugins\n\nIf we didn't have plugins:\n\n- The platform would be forced to support more things out of the box (eg [Parcel](https://parceljs.org/))\n- The platform would be forced to document itself better, rather than rely on plugin authors and plugin users to find and support each other (eg [Next.js](https://nextjs.org/docs/) vs [Gatsby](https://www.gatsbyjs.org/plugins/))\n- The platform would be forced to more heavily consider the exposed API surface area (eg [React](https://www.youtube.com/watch?v=4anAwXYqLG8), [Prettier](https://prettier.io/))\n- More developers would learn how to use the platform directly instead of relying on probably poorer supported code and docs from plugin authors. **This results in more transferable knowledge.**\n\nThese all sound like good things. But what do we expect devs to use if we don't have plugins? We can't have everybody reinventing the wheel every time. \n\n## My Proposal: Recipes\n\nI think the answer is Recipes. I think that plugins are a step too far on the \"Declarative is always better than Imperative\" mindset.\n\nRecipes - to be clear - are like plugins in that you can look up a directory of what you want to do and copy and paste some code. They differ in that the code would be imperative instead of declarative. You no longer declare the name of the plugin and hope that it works - *you copy and paste the actual code that will be executed.*\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/6lbbq7l6zfqskivl844f.png)\n\nThis unlocks a few benefits:\n\n- Updating a Recipe is lower effort (a docs update) than Updating a Plugin (code publishing/signing/testing)\n- the developer learns the underlying system\n- the developer can use their knowledge of the language and associated tooling to debug. \n- the developer can modify the recipe for whatever weird use case they have without foisting that weight on the plugin author AND all other users.\n\nAnd let's face it - most of us have to look up docs to copy and paste declarative plugin config anyway. Recipes simply acknowledge that fact. In fact you're more likely to *not* need to look up docs after copying a Recipe.\n\n## Possible Objections\n\n> *But hold on, that's only for the trivial plugins. Do you expect us to copy and paste thousands of lines of codes for the big plugins?*\n\nNo. In fact that's perhaps the biggest benefit of all - I expect the substantial code to be packaged up and distributed as libraries - **independent and agnostic of the platform**! This reduces the developer's work back to copying simple recipes that use that substantial code. \n\nIf the plugin's code is substantial, going beyond being a mere adaptor, it should be published as a standalone library. This library can then be reused beyond just the current platform. This benefits the library author, who doesn't have to write for every platform, and benefits the developer, whose knowledge of the library is now transferable to the next platform (Every platform wants you to believe it is the last platform you'll ever need).\n\nIf we denote the total number of platforms as `M`, and the total number of plugins/libraries (for each usecase) as `N`, then it seems evident that a world with plugins will require knowledge/maintenance of `M * N` platforms with their plugins, whereas a world with recipes will only require knowledge/maintenance of `M + N` platforms and libraries.\n\n> *But hold on, there's a reason we want declarative plugins - we can statically analyze them!*\n\nYeah. I like static analysis too. But a) most plugin systems don't really do much with it, and b) there's nothing stopping Recipes from including some static data.\n\n## Conclusion\n\nTo be clear, all discussion in this essay only applies to developers. When it comes to nontechnical users, I agree there is clear value in writing plugins that people can click to install. \n\nBut for developers, I simply don't agree that the value afforded by plugins is worth the cost of abstraction. We turn our brains off when we see declarative code - we just apply them as instructed and hope it works, and are at a total loss when it doesn't. When the platform relies on us turning off our brains, that's when we have lost.\n\nI've been thinking about this ever since [Andrew Clark commented](https://twitter.com/acdlite/status/1073401531070767104):\n\n> All plugin systems are bad. I have never met a good one.\n\n**Imagine there's no plugins. It's easy if you try.**\n\n![https://res.cloudinary.com/practicaldev/image/fetch/s--SvTJCo1M--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/i/bsfdn4r2gp1kzxu2yfz3.jpg](https://res.cloudinary.com/practicaldev/image/fetch/s--SvTJCo1M--/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000/https://dev-to-uploads.s3.amazonaws.com/i/bsfdn4r2gp1kzxu2yfz3.jpg)\n\n## Further Reading\n\n- https://johno.com/guessable/\n- https://www.marvel.com/articles/comics/today-in-marvel-history-no-more-mutants\n- https://www.kebabshopblues.co.uk/2015/01/12/zero-config/"
  },
  {
    "slug": "tiago-forte-second-brain",
    "data": {
      "title": "10 Principles I Learned from Tiago Forte's Building a Second Brain",
      "description": "10 Principles I Learned",
      "tag_list": [
        "secondbrain",
        "learninpublic"
      ]
    },
    "content": "\nWe live in an Information Age. **Our quality of life, income, success, are all tied to our fluency with information**: Our ability to capture it and share it with the world. \n\nTiago Forte's [Building a Second Brain](https://www.buildingasecondbrain.com/) idea has been very influential for me to formalize my own [Learn in Public](https://www.swyx.io/writing/learn-in-public) and [Mise en Place Writing](https://www.swyx.io/writing/writing-mise-en-place) concepts. I was delighted to learn [he released a podcast season today](https://fortelabs.co/blog/basbpodcast) with 10 short episodes. I'm going to take notes from the podcast here, and then update this post with more as I go through the BASB course.\n\nThese 10 principles are basically different aspects of a system/approach to knowledge work that helps us **reduce stress, produce more, and live a life of creative joy**:\n\n1. **Borrowed Creativity**: Stand on the shoulders of giants.\n2. **The Capture Habit**: Outsource memory to devices.\n3. **Idea Recycling**: Reuse ideas repeatedly.\n4. **Projects over Categories**: Don't silo insights - organize them into projects you are working toward, *right now*.\n5. **Slow Burns**: Not everything has to be a Heavy Lift. You can accumulate in the background.\n6. **Start With Abundance**: Don't start from a blank canvas.\n7. **Intermediate Packets**: Break down work into manageable projects.\n8. **You Only Know What You Make**: Taking action is the best way to discover what you don't know. \n9. **Make Things Easier for your Future Self**: Package up things for your future self to use.\n10. **Keep Your Ideas Moving**: You never need to be stuck.\n\n---\n\n## 1. **Borrowed Creativity** \n\n- There are no new ideas in the world - all creative output is remixed from constant creative input. What you see when people output great accomplishments and idea is only the end result of a lot of processing and storing of inspirations. \n- Your output is limited to the quality of your inputs. Thus if you want better output, change the way you consume. When you consume with a view to producing something, you have higher standards, you are much less willing to waste time.\n- Dealing with information overload: When you start being more discerning, you start filtering a lot more of the noise. You start seeing how rare it is to see truly good content/ideas out there.\n- When you have a well organized second brain of notes, you start from a higher starting point than when you are only drawing from your best thinking in the moment. You need to be able to pull on accumulated wisdom from yourself and others over a long span of time. It feels like you're skipping steps, because you're borrowing others' thinking. (swyx note: of course - Don't plagiarize; attribute freely)\n\n## 2. **The Capture Habit**\n\n- **Write It Down**. Any insight, any possibly relevant content, before you even know you're going to use it.\n- this habit comes from [David Allen's Getting Things Done](https://gettingthingsdone.com/). Your mind has limited working memory. You alone can't hold on to the best ideas and information AND also keep looking out and processing new ones.\n- People don't do this because they don't value their own ideas. \"Someone must have thought of this before\". \"I'll never be able to act on it\". This is a self fulfilling prophecy if you dont take the basic action of capturing. You have no idea where it's going to end up - [You can only connect the dots backwards](https://news.stanford.edu/2005/06/14/jobs-061505/).\n- Listen to your intuition on what to capture - physical responses - heartbeat, eyes dilation, excitement, energy. Believe that your ideas and perspective matters.\n- When you create content, it's much harder to sit down and just pour out great insight after great insight. It's much easier to collect and store this over time and then organize them later.\n- When you have all these raw materials taken care of by your notetaking system, you start being to do higher level work - spontaneity, creativity, adaptation. Offload mundane details to computers.\n\n\n## 3. **Idea Recycling**\n\n- Similar to **Borrowed Creativity**, but instead you are borrowing from your past self.\n- You don't really remember what your past self knew.\n- You don't really know what your future self will want.\n- Only thing you can do is pass ideas through time. Most ideas start extremely simple - email, tweet, text message. Take the same idea and recycle it through various contexts and grow it each time. Invest more in the 10% that gets traction. It's probably something that is obvious to you but amazing to others.\n- Unlike physical things, Ideas get *better* when you recycle them.\n- It's too risky to build everything from scratch every time with no priors. The only way to get around this is to keep outputting and testing in small ways.\n- Build a compounding asset of intellectual capital that will last your lifetime.\n- Productivity: Never do the same work twice. Most things you do are comprised of components - reuse them. Over time, more and more of what you do can be reused templates from the past. (swyx note: Not all of it, of course)\n\n## 4. **Projects over Categories**\n\n- Knowledge is a Factory, not a Library.\n- A Factory is linear - things come in, are processed, and then output is produced. When you have real intellectual output, people can't take that away from you.\n- Libraries try to categorize everything into the Dewey Decimal System. It works for that - anyone can find things through that system. But the more you put into one category, the more there's just a ton of stuff in that category and it is overwhelming and useless.\n- We aren't optimizing for everyone - we are just working to improve *Personal* Knowledge Management.\n- Projects are the best unit of measurement for your output as a Knowledge Worker. Not Ideas - not very concrete. Not Goals - too long term. Projects are more medium term, specific, more concrete, and you can check it off and remove/archive it so you can take on something new.\n- Categories are consumption oriented, Projects are production oriented. You want to organize your knowledge according to projects. So you should know your current project list and store ideas directly into that current project. This way, when you're in the mood/time to produce, you can start right away, instead of going to look for things.\n\n## 5. **Slow Burns**\n\n- Heavy Lifting is when you block off a week or a month and just blast through your project. Over time, that time available for Heavy Lifts gets scarcer - kids, marriage, employees, other responsibilities.\n- Slow Burns is the opposite. You collect things in the background. You can even make wedding planning stress free this way.\n- Intellectual work can be spread out over time. It allows it to be more enjoyable, spontaneous, creative, critical (editing with some distance) etc. But primarily less stress :)\n- Analogy - Predators need to eat now - they work fast and intensely with scarcity. Scavengers work in abundance - we are living in a sea of creative inputs - dont have to look very hard around you to find inspiration. \n\n## 6. **Start With Abundance**\n\n- Most people start with scarcity - a blank canvas\n- Refuse to start a project until you have everything you need already assembled.\n- But it also means you have to store notes on everything to give yourself the best shot. This is how Big Breaks happen - they don't wait for you to be ready. They tend to happen at the worst time. But at least you will have given yourself the best starting point when it comes.\n- You can decide to be wealthy in the world of ideas. All you need is some intentionality to cultivate them. Ideas are free, plentiful, and you can store them forever and nobody can take them away from you.\n\n## 7. **Intermediate Packets**\n\n- Intermediate Packets are parts of your work. Concrete parts of your work. Every piece of your work is built out of parts - so break down your work. Instead of writing a full email, write the outline of your email. Instead of writing a full book, you get an agent, get an editor, get a contract, get an outline for a book proposal, on and on. It becomes a lot less intimidating when you break it down. \n- It's really hard to fail at Intermediate Packets (aka Projects). Hack your own motivation by breaking things down.\n- Take some effort to make each piece consumable by your future self. Turn perishable things into longer lasting ones by taking the time to save them down in the right place and adding metadata like titles, quotes, page numbers.\n\n## 8. **You Only Know What You Make**\n\n- Think of Learning and Working as the same thing. You're not done learning when you start work - but CV's look like that. Instead, think that the best way to learn is making something.\n- When you make things, all the practical difficulties and holes in your knowledge come to life.\n- Example - [Book Summaries](https://fortelabs.co/blog/category/types/book-summary/) - instead of reading a book and putting it down, save notes, dive into ideas, and write summaries, and try to apply the ideas in a book summary. Don't pride yourself on quantity of books read. Read less, but really immerse in the ideas of each book. Add your own interpretations and metaphors. Become an expert on the book. Side benefits - Form a relationship with the author, build an audience, etc.\n\n## 9. **Make Things Easier for your Future Self**\n\n- Some people over time work harder and harder, some others work less and less. People live completely different lives as they get older - some super busy, some produce results with much less effort.\n- The difference is leverage. Either day by day you are building some sort of accumulating leverage - blogs, savings, health. \n- Intellectual leverage compounds over time. Makes it easier for future selves.\n- Treat your future self as if that person was real. Impacts the way you eat, sleep, workout, and learn. It helps you act with more direction in future life.\n\n## 10. **Keep Your Ideas Moving**\n\n- Don't get frustrated by being stuck. Move on. Let it stew in the back of your mind, as a Slow Burn. Often you will be working on it subconsciously.\n- You will get stuck - so the solution is always have multiple things cooking. Translating ideas from one domain to the other.\n- Problem with multitasking: you lose progress because you can't keep everything in your head. When you think and work externally it reaps the benefits of multitasking without the downsides.\n- More important to stay in flow (maximum enjoyment, creativity, immersion) than have any particular outcome. Flow requires movement. Keep moving, and save things how you left it.\n\n"
  },
  {
    "slug": "make-your-own-dev-to-cms-livestream-part-3-c65",
    "data": {
      "title": "Make your own Dev.to CMS livestream - Part 3",
      "description": "This is day 3 of my livecode Dev.to CMS.  Source: https://github.com/sw-yx/dev-to-cms Demo: https://d...",
      "tag_list": [
        "livecoding",
        "react",
        "typescript",
        "nextjs"
      ]
    },
    "content": "This is day 3 of my livecode Dev.to CMS.\n\nSource: https://github.com/sw-yx/dev-to-cms\nDemo: https://dev-to-cms.now.sh\n\n## Things covered\n\n- How to lose a lot of time with React Query: https://github.com/tannerlinsley/react-query\n- Tailwind UI: https://tailwindui.com/\n\n## Video\n\nHere is [the YouTube livestream (3 hours)](https://www.youtube.com/watch?v=ONVp8QivDv0&feature=youtu.be).\n\nDev.To embed:\n\n{% youtube ONVp8QivDv0 %}\n\nToday we refactored the editor to a standalone component, and then added the ability to edit posts! It now works\n\n\nWe methodically worked thru all issues:\n\n- splitting out an editor component\n- creating api endpoint to fetch (GET) a single existing article\n    - this later required fetching unpublished articles as well\n- creating api endpoint to update (PUT) a single existing article\n- wiring it up to the Dashboard (list of articles) and to existing editor pages.\n\n\n## Catch up on the Dev.to CMS LiveStream!\n\n- [Day 1 - Setup Next.js and Tailwind UI, list posts through API routes](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-1-2ad1) - 90 mins\n- [Day 2 - setting up a Markdown Editor with Next.js, Tailwind UI, Highlight.js, React Hook Form, and React Query](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-2-6mf) - 3 hours\n- Quick Fix - [How To Add Monaco Editor to a Next.js app](https://dev.to/swyx/how-to-add-monaco-editor-to-a-next-js-app-ha3) - 18 mins\n- [Day 3 - Refactoring to Edit Existing Posts](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-3-c65) - 3 hours\n- [Day 4 - Polish Day! Implementing Notifications, Markdown preview, and programmatic Redirects, and Using Web Components in Next.js](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-4-6em) - 3 hours\n- [Instant GraphQL with OneGraph - Screenshares in Public with Sean Grove](https://dev.to/swyx/instant-graphql-with-onegraph-4hi8) - refactoring handrolled Dev.to API access with OneGraph and GraphQL\n- [How and Why to Un-Reset Tailwind's CSS Reset](https://dev.to/swyx/how-and-why-to-un-reset-tailwind-s-css-reset-46c5)"
  },
  {
    "slug": "how-to-add-monaco-editor-to-a-next-js-app-ha3",
    "data": {
      "title": "How To Add Monaco Editor to a Next.js app",
      "description": "Bottom Line Up Front   I use a slightly modified version of the steps mentioned in this GitH...",
      "tag_list": [
        "nextjs",
        "monaco",
        "javascript"
      ]
    },
    "content": "## Bottom Line Up Front\n\nI use a slightly modified version of the steps mentioned in [this GitHub comment](https://github.com/react-monaco-editor/react-monaco-editor/issues/271#issuecomment-601835902). Modifications were necessary because I use TailwindCSS with Next.js.\n\n- [YouTube recording (18 mins)](https://youtu.be/13UVFrGe80o)\n- Dev.to Embed:\n\n{% youtube 13UVFrGe80o %}\n\n## Motivations\n\nMonaco Editor is [the open source editor]\n(https://github.com/microsoft/monaco-editor) used in [VS Code](https://github.com/microsoft/vscode), which itself is open source. I used to write my blogposts in VS Code, and as I [make my own Dev.to CMS](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-1-2ad1), I wanted to have all the familiar trappings of Monaco to help me out while I write.\n\n## Problems\n\nHowever there are some issues we have to deal with:\n\n- Monaco is framework agnostic, so it requires writing some React bindings.\n  - You could do it yourself, but also you could just skip that and use https://github.com/react-monaco-editor/react-monaco-editor\n- Monaco is written for a desktop Electron app, not for a server-side rendered web app.\n  - This is solved by using `import dynamic from \"next/dynamic\"` and making Monaco a dynamic import.\n- Monaco also wants to offload syntax highlighting to web workers, and we need to figure that out\n- Next.js [doesn't want any dependencies importing CSS](https://github.com/zeit/next.js/issues/10975) from within `node_modules`, as this assumes a bundler and loader setup (e.g. webpack) and can have unintentional global CSS side effects (all global CSS is intended to be in `_app.js`).\n  - we can re-enable this with [`@zeit/next-css`](https://www.npmjs.com/package/@zeit/next-css) and [`next-transpile-modules`](https://www.npmjs.com/package/next-transpile-modules)\n\nWe can solve this with [a solution worked out by Elliot Hesp on GitHub](https://github.com/react-monaco-editor/react-monaco-editor/issues/271#issuecomment-601835902) and [a config from Joe Haddad of the Next.js team](https://gist.github.com/Timer/2ed585ff0db6b885a7f7dd738a982b13).\n\n## Solution\n\nThe solution I use is informed by my usage of Tailwind CSS, which requires a recent version of PostCSS, which `@zeit/next-css` only has at 3.0 (because it is deprecated and not maintained).\n\nI also use TypeScript, which introduces a small wrinkle, because Monaco Editor attaches a `MonacoEnvironment` global on the `window` object - I just `@ts-ignore` it.\n\n```js\n// next.config.js\n\nconst MonacoWebpackPlugin = require(\"monaco-editor-webpack-plugin\");\nconst withTM = require(\"next-transpile-modules\")([\n  // `monaco-editor` isn't published to npm correctly: it includes both CSS\n  // imports and non-Node friendly syntax, so it needs to be compiled.\n  \"monaco-editor\"\n]);\n\nmodule.exports = withTM({\n  webpack: config => {\n    const rule = config.module.rules\n      .find(rule => rule.oneOf)\n      .oneOf.find(\n        r =>\n          // Find the global CSS loader\n          r.issuer && r.issuer.include && r.issuer.include.includes(\"_app\")\n      );\n    if (rule) {\n      rule.issuer.include = [\n        rule.issuer.include,\n        // Allow `monaco-editor` to import global CSS:\n        /[\\\\/]node_modules[\\\\/]monaco-editor[\\\\/]/\n      ];\n    }\n\n    config.plugins.push(\n      new MonacoWebpackPlugin({\n        languages: [\n          \"json\",\n          \"markdown\",\n          \"css\",\n          \"typescript\",\n          \"javascript\",\n          \"html\",\n          \"graphql\",\n          \"python\",\n          \"scss\",\n          \"yaml\"\n        ],\n        filename: \"static/[name].worker.js\"\n      })\n    );\n    return config;\n  }\n});\n```\n\nand then in your Next.js app code:\n\n```tsx\nimport React from \"react\";\n// etc\n\nimport dynamic from \"next/dynamic\";\nconst MonacoEditor = dynamic(import(\"react-monaco-editor\"), { ssr: false });\n\nfunction App() {\n  const [postBody, setPostBody] = React.useState(\"\");\n  // etc\n  return (<div>\n  {/* etc */}\n    <MonacoEditor\n      editorDidMount={() => {\n        // @ts-ignore\n        window.MonacoEnvironment.getWorkerUrl = (\n          _moduleId: string,\n          label: string\n        ) => {\n          if (label === \"json\")\n            return \"_next/static/json.worker.js\";\n          if (label === \"css\")\n            return \"_next/static/css.worker.js\";\n          if (label === \"html\")\n            return \"_next/static/html.worker.js\";\n          if (\n            label === \"typescript\" ||\n            label === \"javascript\"\n          )\n            return \"_next/static/ts.worker.js\";\n          return \"_next/static/editor.worker.js\";\n        };\n      }}\n      width=\"800\"\n      height=\"600\"\n      language=\"markdown\"\n      theme=\"vs-dark\"\n      value={postBody}\n      options={{\n        minimap: {\n          enabled: false\n        }\n      }}\n      onChange={setPostBody}\n    />\n  </div>)\n}\n```\n\nSince I'm using Tailwind, I'm also using PostCSS, which also [tries to eliminate Monaco's CSS](https://dev.to/jaakkolantero/comment/n7hl). You have to tell it to ignore that:\n\n```js\n// postcss.config.js\nconst purgecss = [\n  \"@fullhuman/postcss-purgecss\",\n  {\n    // https://purgecss.com/configuration.html#options\n    content: [\"./components/**/*.tsx\", \"./pages/**/*.tsx\"],\n    css: [],\n    whitelistPatternsChildren: [/monaco-editor/], // so it handles .monaco-editor .foo .bar\n    defaultExtractor: content => content.match(/[\\w-/.:]+(?<!:)/g) || []\n  }\n];\n```\n\n\n## Catch up on the Dev.to CMS LiveStream!\n\n- [Day 1 - Setup Next.js and Tailwind UI, list posts through API routes](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-1-2ad1) - 90 mins\n- [Day 2 - setting up a Markdown Editor with Next.js, Tailwind UI, Highlight.js, React Hook Form, and React Query](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-2-6mf) - 3 hours\n- Quick Fix - [How To Add Monaco Editor to a Next.js app](https://dev.to/swyx/how-to-add-monaco-editor-to-a-next-js-app-ha3) - 18 mins\n- [Day 3 - Refactoring to Edit Existing Posts](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-3-c65) - 3 hours\n- [Day 4 - Polish Day! Implementing Notifications, Markdown preview, and programmatic Redirects, and Using Web Components in Next.js](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-4-6em) - 3 hours\n- [Instant GraphQL with OneGraph - Screenshares in Public with Sean Grove](https://dev.to/swyx/instant-graphql-with-onegraph-4hi8) - refactoring handrolled Dev.to API access with OneGraph and GraphQL\n- [How and Why to Un-Reset Tailwind's CSS Reset](https://dev.to/swyx/how-and-why-to-un-reset-tailwind-s-css-reset-46c5)"
  },
  {
    "slug": "make-your-own-dev-to-cms-livestream-part-2-6mf",
    "data": {
      "title": "Make your own Dev.to CMS livestream - Part 2",
      "description": "This is day 2 of my livecode Dev.to CMS. See Day 1 here  Source: https://github.com/sw-yx/dev-to-cms...",
      "tag_list": [
        "livecoding",
        "react",
        "typescript",
        "nextjs"
      ]
    },
    "content": "This is day 2 of my livecode Dev.to CMS. See [Day 1 here](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-1-2ad1)\n\nSource: https://github.com/sw-yx/dev-to-cms\nDemo: https://dev-to-cms.now.sh\n\n## Things covered\n\n- Syntax Highlighted Editors (Monaco, Prism.js, finally Highlight.js)\n- React Hook Form: https://react-hook-form.com/\n- React Query: https://github.com/tannerlinsley/react-query\n- Tailwind UI: https://tailwindui.com/\n\n## Video\n\nHere is [the YouTube livestream (3 hours)](https://youtu.be/SmpsdMZxy8I).\n\nDev.To embed:\n\n{% youtube SmpsdMZxy8I %}\n\nFor the 1st half, it was pretty easy going. I was mostly pulling components off of Tailwind UI and assembling an editor!\n\nFor the 2nd half, I mainly faced issues with trying to integrate Monaco and then Prism.js. Finally landed on Highlight.js.\n\n\n## Catch up on the Dev.to CMS LiveStream!\n\n- [Day 1 - Setup Next.js and Tailwind UI, list posts through API routes](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-1-2ad1) - 90 mins\n- [Day 2 - setting up a Markdown Editor with Next.js, Tailwind UI, Highlight.js, React Hook Form, and React Query](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-2-6mf) - 3 hours\n- Quick Fix - [How To Add Monaco Editor to a Next.js app](https://dev.to/swyx/how-to-add-monaco-editor-to-a-next-js-app-ha3) - 18 mins\n- [Day 3 - Refactoring to Edit Existing Posts](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-3-c65) - 3 hours\n- [Day 4 - Polish Day! Implementing Notifications, Markdown preview, and programmatic Redirects, and Using Web Components in Next.js](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-4-6em) - 3 hours\n- [Instant GraphQL with OneGraph - Screenshares in Public with Sean Grove](https://dev.to/swyx/instant-graphql-with-onegraph-4hi8) - refactoring handrolled Dev.to API access with OneGraph and GraphQL\n- [How and Why to Un-Reset Tailwind's CSS Reset](https://dev.to/swyx/how-and-why-to-un-reset-tailwind-s-css-reset-46c5)"
  },
  {
    "slug": "dev-to-cms-stream-1",
    "data": {
      "title": "Make your own Dev.to CMS livestream - Part 1",
      "description": "Livecoding my own Dev.to CMS with Next.js + Preact + TypeScript + Tailwind UI",
      "tag_list": [
        "livecoding"
      ]
    },
    "content": "\nAs someone who uses [Dev.to as a headless CMS](https://www.swyx.io/writing/devto-cms/), I feel a lot of personal pain points with the very simple default editor we get in Dev.to. I figured I would try a personal project with all new tech that I know some things about but haven't built anything serious with:\n\n- Preact\n- Next.js\n- TailwindCSS\n\nAnd done in TypeScript because that's how I roll.\n\nSource: https://github.com/sw-yx/dev-to-cms\nDemo: https://dev-to-cms.now.sh\n\n[Here is my first livestream of that!](https://www.youtube.com/watch?v=dR2xZbViyks&feature=youtu.be) Had a slow start but got a lot done in 90 minutes.\n\nDev.To embed:\n\n{% youtube dR2xZbViyks %}\n\n\n## Catch up on the Dev.to CMS LiveStream!\n\n- [Day 1 - Setup Next.js and Tailwind UI, list posts through API routes](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-1-2ad1) - 90 mins\n- [Day 2 - setting up a Markdown Editor with Next.js, Tailwind UI, Highlight.js, React Hook Form, and React Query](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-2-6mf) - 3 hours\n- Quick Fix - [How To Add Monaco Editor to a Next.js app](https://dev.to/swyx/how-to-add-monaco-editor-to-a-next-js-app-ha3) - 18 mins\n- [Day 3 - Refactoring to Edit Existing Posts](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-3-c65) - 3 hours\n- [Day 4 - Polish Day! Implementing Notifications, Markdown preview, and programmatic Redirects, and Using Web Components in Next.js](https://dev.to/swyx/make-your-own-dev-to-cms-livestream-part-4-6em) - 3 hours\n- [Instant GraphQL with OneGraph - Screenshares in Public with Sean Grove](https://dev.to/swyx/instant-graphql-with-onegraph-4hi8) - refactoring handrolled Dev.to API access with OneGraph and GraphQL\n- [How and Why to Un-Reset Tailwind's CSS Reset](https://dev.to/swyx/how-and-why-to-un-reset-tailwind-s-css-reset-46c5)"
  },
  {
    "slug": "computer-history-museum-youtube",
    "data": {
      "title": "The Computer History Museum YouTube Channel",
      "description": "I have been enjoying the CHM's YouTube Channel.",
      "tag_list": [
        "faves"
      ]
    },
    "content": "\nI've been going down a bit of a rabbit hole recently. For a few months I've been thinking about how services like Netlify and Zeit function like \"operating systems\" over new computing primitives provided by major public clouds. Then [I heard Pulumi's Joe Duffy](https://twitter.com/swyx/status/1241533362319364096) mention that Azure used to be codenamed \"Red Dog\", and then [found out](https://twitter.com/swyx/status/1241537049955622913) that Dave Cutler literally worked on it as a \"Cloud Operating System\" and then found out that [he was responsible for Windows NT and all its variants](https://news.microsoft.com/features/the-engineers-engineer-computer-industry-luminaries-salute-dave-cutlers-five-decade-long-quest-for-quality/)...\n\n...and that is what led me to [his autobiographical interview on the Computer History Museum](https://www.youtube.com/watch?v=29RkHH-psrY) YouTube Channel.\n\nThis channel is the motherlode for history buffs. I wouldn't call myself a \"History Buff\", but I certainly care a good deal about how the things we have now came to be, and what we can learn from that. \n\nI also love [the Lindy Effect](https://en.wikipedia.org/wiki/Lindy_effect) as a heuristic for worthwhile content - if this thing recorded 10 years ago is still as relevant today, then it will be as relevant 10 years from now, and therefore is a good use of time. If you switch your content consumption from short lived ephemera to time-tested stuff, your knowledge compounds. Plain and simple.\n\nHere are things that jump out at me from this incredible channel:\n\n- [Secret History of Silicon Valley](https://www.youtube.com/watch?v=ZTC_RxWN_xo): One of the best talks I've ever seen. Not that many histories of SV *END* with William Shockley. It elevates Fred Terman's role in bringing talent and military funding from the east coast, and blessing entrepreneurship, and definitively traces exact cause of *why* Silicon Valley is what it is today.\n- [The Origins of Linux](https://www.youtube.com/watch?v=WVTWCPoUt8w) from Linus Torvalds\n- [The Design of C++](https://www.youtube.com/watch?v=69edOm889V4) from Bjarne Stroustrup\n- [Steve Jobs Introduces the Macintosh](https://www.youtube.com/watch?v=1tQ5XwvjPmA) - full 90 minute premiere from 1984 in shockingly good quality.\n- [Elizabeth Holmes interview](https://www.youtube.com/watch?v=uJDc4tOU3zo) - I havent seen this yet but 🍿\n- [Morris Chang interviewd by Jensun Huang](https://www.youtube.com/watch?v=u-x7PdnvCyI&t=164s) - I love it when CEOs interview CEOs. Here's Jensen Huang (Nvidia) interviewing Morris Chang (TSMC), two Taiwanese-American semiconductor titans, a generation apart but both visionaries in their fields.\n- [Oral History of Guido van Rossum](https://www.youtube.com/watch?v=Pzkdci2HDpU)\n\nI have not vetted everything and to be sure the quality varies widely. But there are certainly some original-source gems to be had, and hours of highly curated direct source content to crawl through here. It's like being able to visit the CHM from your own home!\n\nDo you see an overlooked CHM interview that you really enjoyed? Let me know!"
  },
  {
    "slug": "career-ladders",
    "data": {
      "title": "Every Public Engineering Career Ladder",
      "description": "A list of the public engineering career ladders I've found",
      "tag_list": [
        "career"
      ]
    },
    "content": "\n\"What will it take to get to the next level?\"\n\nOf course this is a very open ended question, but it can be nice to set some guide rails around what expectations are in a company. As an individual contributor, career ladders tell me what the company ostensibly values (and, by omission, what it doesn't value).\n\nI recently compiled this list of career ladders and figured I would share it here. Some ladders include nontechnical jobs, I will just look at engineering and engineering management.\n\n- **[Fog Creek](https://www.joelonsoftware.com/2009/02/13/fog-creek-professional-ladder/)**: from 2009, but obviously Joel's thinking has been very influential. Focus is on growing ownership, ability to write production code independently, shipping experience, and at senior levels, design/planning/architecture. Teamwork, self study, mentorship, and impact are all key, as well as the Joel Test.\n- **[Rent the Runway](https://dresscode.renttherunway.com/blog/ladder)** ([spreadsheet](https://docs.google.com/spreadsheets/d/1k4sO6pyCl_YYnf0PAXSBcX776rNcTjSOqDxZ5SDty-4/edit?usp=sharing)): from 2015. Takes a fun D&D inspired Dex/Str/Wis/Cha stats based evaluation, corresponding to technical skill, productivity, impact, and communication/leadership. Management track is also included, with more focus on architecture, hiring, organizational skills, and leadership/salesmanship.\n- **[Artsy](https://artsy.github.io/blog/2015/04/03/artsy-engineering-compensation-framework/)**: inspired by Rent the Runway\n- **[CapGemini UK](https://capgemini.gitbooks.io/grade-ladder/content/)**: also inspired by Rent the Runway\n- **[Basecamp](https://basecamp.com/handbook/appendix-05-titles-for-programmers)**: pretty simple.\n- **[Thumbtack](https://engineering.thumbtack.com/how-we-built-an-engineering-job-ladder-from-the-ground-up/)** ([spreadsheet](https://docs.google.com/spreadsheets/d/15ACBs-crUHnqf1wANUQwX9oZIDOi5tvJJXGWpKTcf00/edit#gid=0)): from 2019. Breaks down technical skills into **code quality/testing**, **debugging**, and **scoping/project design**, and nontechnical factors to **collaboration, citizenship, leadership, and impact**. Leadership is interestingly broken down into **Autonomy, Judgement, Initiative, and Consensus Building**.\n- **[CircleCI](https://circleci.com/blog/why-we-re-designed-our-engineering-career-paths-at-circleci/)** ([spreadsheet](https://docs.google.com/spreadsheets/d/131XZCEb8LoXqy79WWrhCX4sBnGhCM1nAIz4feFZJsEo/edit#gid=0)): one of the most well known ladders, detailed but not overwhelming. NOtably, one of the values assessed is Security.\n- **[Envoy](https://github.com/envoy/Engineering/blob/master/engineering_bands.md)**: pretty simple ladder list, by a hot company.\n- **[Financial Times](https://medium.com/ft-product-technology/improving-our-career-map-for-engineers-4210185c6246)** ([webapp](https://engineering-progression.ft.com/)). Even has an [API](https://engineering-progression.ft.com/docs/api/)! lol. Only 4 areas across Technical, Leadership, Delivery, and Communication are assessed. Feels manageable.\n- **[Meetup](https://github.com/meetup/engineering-roles/)**: splits roles into Makers and Managers. \n  - Makers focus on Architecture & framework & software development, Best practices & architecture & code reviews, Technical skills evaluation & mentoring, Introduce new engineering tools & mentor adoption, and Recommend process improvements & support adoption. \n  - Managers focus on Performance evaluation, Career growth, Recruiting & resourcing, Rollout process improvements & adoption of engineering tools and Organization & administrative.\n- **[Socialbakers](https://github.com/Socialbakers/engineeringroles)**: seems pretty similar to Meetup's ladder\n- **[Medium](https://medium.com/s/engineering-growth-framework/engineering-growth-tracks-b1fad620787e)** ([gist](https://gist.github.com/david206/b8dceddd687bb2c60805c9669cc89eaa)): has tracks for Mobile, Servers, Foundations, Web client, Project Management, Communication, Craft, Initiative, Org design, Accomplishment, Wellbeing, Career development, Evangelism, Community, Recruiting, and Mentorship! Phew!\n- **[Starling Software](http://www.starling-software.com/employment/programmer-competency-matrix.html)**: Uses Big O notation to denote levels, which is a fun shibboleth. Skills are broken out to Computer Science, Software Engineering, Programming, Experience, and Knowledge. This one has a long record on [Hacker News](https://news.ycombinator.com/item?id=4626695) but is a good map of things that we can work on.\n- **[Kickstarter](https://gist.github.com/jamtur01/aef437a79fee5a9cefdc)**:  basically a bunch of job descriptions, including Data careers and CTO.\n- **[Brandwatch](https://docs.google.com/document/d/1PCj7f-91G6dGdIlfDpCHcm68L80hRELk00eO-gZvh1o/edit)**: explains levels at a high level, and then breaks it out for [IC's and Management in this spreadsheet](https://docs.google.com/spreadsheets/d/1weNoeSUfHYOHy9sHKad_ax6qrVXNFlP4ieTwaRll70M/edit#gid=0). A total of 15 attributes to work on!\n- **[Spotify](https://labs.spotify.com/2016/02/15/spotify-technology-career-steps/)**: famous for its \"Squad/Tribe\" structure - emphasizes \"Steps\", with a simple list of five sets of behaviors they want:\n   - Values team success over individual success\n   - Continuously improves themselves and team\n   - Holds themselves and others accountable\n   - Thinks about the business impact of their work\n   - Demonstrates mastery of their discipline\n- **[Chuck Groom](https://blog.usejournal.com/the-software-engineering-job-ladder-4bf70b4c24f3)** - this is unusual - personal thoughts on a Job Ladder, though the author is a senior engineering leader at VTS. Good discussions on how having ladders helps, as well as descriptions of Anti-patterns. I love the Principal Engineer antipatterns:\n \n  >  Over-emphasis on scaling or high availability far beyond business needs. Spends too much time chasing the newest “shiny” technology. Doesn’t collaborate or ask questions. Condescending. Has “pet” agenda. Pisses off senior leadership.\n\n- **[Buffer](https://buffer.com/resources/career-framework/)** has a Maker (IC) and a Manager track, spelling out 6 levels from Entry level to Principal. They make interesting use of a \"rope\" analogy to describe the skill differences.\n\n## Further Reading\n\n- More ladders (non engineering) are available at https://www.progression.fyi/\n- http://engineering.khanacademy.org/posts/career-development.htm\n- https://progression.monzo.com/engineering/web\n- https://squeakyvessel.com/2016/07/11/engineering-ladders-links-elsewhere/\n- https://patreonhq.com/how-patreon-levels-engineers-a28a3491ae6a\n- https://developer.squareup.com/blog/squares-growth-framework-for-engineers-and-engineering-managers/\n- http://www.elidedbranches.com/2015/11/truth-and-consequences-of-technical.html\n- https://mcfunley.com/thoughts-on-the-technical-track\n- https://github.com/vijayvenkatesh/engineering_ladders\n- https://www.cnblogs.com/dhcn/p/10729002.html\n- https://circleci.com/blog/how-to-successfully-work-from-home-strategies-for-remote-work/\n- Management vs Engineering: https://charity.wtf/2019/01/04/engineering-management-the-pendulum-or-the-ladder/"
  },
  {
    "slug": "writing-mise-en-place",
    "data": {
      "title": "Mise en Place Writing",
      "description": "How to write more, faster, and better by decoupling writing from pre-writing",
      "tag_list": [
        "reflections",
        "writing"
      ]
    },
    "content": "\nOn my [mailing list](https://tinyletter.com/swyx), [Joe](http://twitter.com/jsjoeio/) writes:\n\n> I want you to write about writing and how you've made it a habit.\n> - why do you do it?\n> - what's the sudden shift in doing it more recently?\n> - what's your process? idea -> draft -> review? -> publish\n> - how do you chose what to write about?\n\nThanks Joe! I think the How is a lot easier to answer than the Why, so I will just focus on that here and discuss \"Why I Write\" in a future piece. As for the How - I call it **Mise en Place Writing**.\n\nTL;DR - I haven't made writing a habit. I've made *pre-writing* a habit. **By decoupling writing from pre-writing, I can write more, faster, and better.**\n\n## Mise en Whatnow?\n\nWikipedia defines [Mise en place](https://en.wikipedia.org/wiki/Mise_en_place) as:\n\n> a French culinary phrase which means \"putting in place\" or \"everything in its place\". It refers to the setup required before cooking, and is often used in professional kitchens to refer to organizing and arranging the ingredients (e.g., cuts of meat, relishes, sauces, par-cooked items, spices, freshly chopped vegetables, and other components) that a cook will require for the menu items that are expected to be prepared during a shift\n\nThe idea is that cooking is faster, easier and more enjoyable if you have all the stuff you'll need to do prepared beforehand. It's basically all the fun parts of cooking separated from the boring parts. If you've ever had the chance to do a cooking class in a foreign country (A great way to appreciate local cuisine in personal travel or team retreats), notice that it's fun because all the prep is done for you. \n\nOf course, for chefs, typically *you're* also the one doing the prep so you're not really *saving* any work. But it can be nice to break work up into more palatable (pun intended?) chunks of \"prep\" vs \"cook\" instead of \"Prep & Cook\". In fact, if you prep early, you also get a chance to spot missing ingredients that you'll need, and be able to run out and get them, rather than discovering that you don't have them while cooking (!).\n\n## Writing isn't Just Writing\n\nWhat's true for chefs is even more true for writers. The actual act of writing is only the final assembly stage of a longer, more involved process. Instead of grocery shopping (or stocking up), chopping, measuring, cutting, peeling, slicing, we have ideation, research, peer review, audience testing, reframing, illustration, organization, and more.\n\n**The key insight of Mise en Place Writing is that we should decouple writing from pre-writing**.\n\nWriting is an intensive, focused process. It takes me anywhere from 1-5 hours to pump out an average article - in that time I am doing nothing else. \n\nThat's a lot of continuous time dedicated to just one thing - rare in today's attention economy. If I were to add to that ideation and research and organization and so on, it'd take even longer, and I'd do a poorer job of it. Ironically, this is the stuff that actually has high leverage on what readers will take away from what I write. So I should spend more time on *that*.\n\n## Components of Pre-Writing\n\nPre-writing is low intensity and often serendipitous, dependent on thoughts flowing into you rather than you putting thoughts out. Here is a nonexclusive list of things that can be realistically counted as pre-writing:\n\n- **💡Ideation**: Literally, what are possible things you could be writing about? Raw Idea Velocity is the focus here - remember you're not signing up to actually deliver the thing - but if you had a flash of inspiration or insight sometime somewhere, write it down. It's not uncommon for me to watch a good talk and come out of it with 2 ideas of things to write about.\n  - If you feel like you lack ideas for things to write about, your bar is too high. Even if it's been definitively explained elsewhere, you can still get value [learning in public](https://www.swyx.io/writing/learn-in-public) by explaining in your own words to others who think like you. \n  - If you have too MANY ideas - I can sometimes relate - run it by a mental filter of What People Want. You'll want to take note of what people are interested in. Or, y'know, just ask them what they want.\n- **📚Research**: Links, facts, quotes, stories, demos, repos, tweets, talks, podcasts, timelines, histories, taxonomies. Your research journey starts immediately after the Idea. When you get the Idea Flash, definitely note down its origin. That's research. This is the \"meat\" of many genres of blogposts - concrete pieces of information that readers can take away. Can you imagine doing comprehensive research just before you write? You might never end up writing! You're collecting all the stuff you're going to use to write in future, except you're doing it as you come across it.\n- **👀Peer Review**: Research will be a lot easier and faster with Peer Review. This is an advantage of having a network - a small group of people who know more than you, that you can tap on to get more research direction and to get a feel of immediate objections to address.\n- **👨‍👩‍👧‍👦Audience Testing**: This is a little wider net than Peer Review - instead of consulting people who know more than you, you're now testing the messaging on the people you're writing this for. This is an extra step I rarely do for my blogposts, but, for example, I'll do this when chatting with developers in conferences and meetups to gauge reaction. Broadway shows have the concept of [Workshopping](https://en.wikipedia.org/wiki/Workshop_production) in a smaller audience, with the understanding that everything from plot to props to cast can be changed based on feedback, before the show actually launches live. For more highly produced pieces of content, like talks, workshops, or books, this is worth investing in so that you have the impact you hope for.\n- **🖼️Reframing**: The initial idea might not be what you actually end up writing. Whether it is due to audience feedback or delayed inspiration, you might find a more interesting angle to approach the topic, or find a better analogy. It's cheaper to pivot the entire focus of your writing when you haven't written it yet. Just by illustration - this post you're reading was originally titled \"How and Why I Write\" - good, but not as interesting and memorable. I found a better framing, and reframed.\n- **🧱Organization**: Deciding on a structure for your article sets it apart from a stream-of-consciousness rant. It lets people zoom in and out of your thoughts by skimming or diving in as needed. If you really need to deliver a message, use the [Tell them what you will tell them, Tell them, and Tell them what you just told them](https://www.inc.com/john-baldoni/deliver-a-great-speech-aristotle-three-tips.html) metastructure, that highlights the structure itself. I don't always do that because it can feel repetitive. But structure choice is important. See, for example, that I've brought you along this list in roughly chronological order.\n- **🎨Illustration**: A picture speaks a thousand words. Think about how you can help the reader give a visual reference for what you will describe - as a bonus, it makes your thesis a lot more sharable. Some things might be harder to illustrate - [Maggie Appleton has great ideas on how to illustrate the invisible](https://illustrated.dev/drawinginvisibles1). Mental imagery can work too - You'll notice I don't use many visuals here - but I am invoking a cooking analogy that you already have entrenched in your head.\n\nAlright, alright, I couldn't resist:\n\n![mise en place2](https://user-images.githubusercontent.com/6764957/77262518-e1a38500-6c8d-11ea-802a-2f34263c10b1.png)\n\nAs you can see, there's a lot that you can do to improve your writing before you write. We should have a separate workflow for pre-writing.\n\n## The Pre-Writing Workflow\n\nThe idea of grooming a backlog is key to productive writing. For the past few months I have been accumulating a backlog of ideas that could be interesting topics to write about. As of right now it stands at around 50-70 topics. \n\nFirst, make sure you have a cross platform note-taking tool - I evaluated Evernote, Notion, Roam Research, and SimpleNote, but currently am using OneNote because it is free forever with the backing of Microsoft, and has good offline support. I might move to Joplin in future, or write my own. It's not really about the tool - at my scale, it's trivial to switch tools - but the feature set needs to support the workflow. \n\nAnd the workflow is this - anytime you have any content idea anywhere - reading something, watching a talk, listening to a podcast, having a conversation with a friend, thinking to yourself in a shower - you need to note it down in a searchable place. If it attaches to some other relevant piece of thought, you collect these together in a growing list of ideas, quotes, soundbites, links, talking points and so on.\n\nIt has to be *easy* and *fast*. I don't know about you but I can forget ideas in minutes. If I don't write it down it might be gone forever. I've been known to jump out of the shower dripping wet just to go write something down. \n\nHuman Brains are great at abstract thinking and creative inspiration. Computers are great at storage and search. Use them as your [second brain](https://www.buildingasecondbrain.com/).\n\nEach note you make is a little kitchen, and you're assembling the pieces in place for a future you to come in and just get to cooking.\n\n## The Infinite Kitchen\n\nWhen practicing Mise en Place, Chefs are limited by available kitchen space-time. Yes, I am invoking space-time equality. You not only have raw table square footage limits, but food laid out must be consumed within a certain time as well.\n\nWriters have no such restriction. \n\nYou, the writer, have an **Infinite Kitchen in the Cloud**. \n\nScrew struggling to come up with 1 blogpost a month. You're now simultaneously preparing **50 blogposts at once**. You're adding ingredients as they come in, you're chopping them up, rearranging, taste testing, noting what you're still missing, throwing entire kitchens out, smashing two kitchens together. \n\nHave fun! Get messy! Get weird! No one else is watching!\n\nAnd then you write.\n\n## Writing\n\nI'm currently writing one post a day. It's a high bar - most people don't have the luxury. It also means the quality of my writing is pretty much limited to what I can do in 1-5 hours - not anywhere close to what people can achieve with a sustained 20-100 hour effort on a monster [skyscraper](http://blog.exitbee.com/skyscraper-content-business-can-use-rapid-growth/) or even a book. So I don't intend to keep it up forever.\n\nBut the process of writing is the same whether I write once a day or I am trying to create something more involved. I still separate pre-writing from writing.\n\nEach day, I go through my growing list and think about what is the most interesting thing I could write about that day. The nice thing about having a groomed backlog is that all the related links and notes and thoughts from myself over time is collected in a list ready for me to flesh out into a full piece. It's just less intimidating that way. But there's also a little dopamine hit from this concentrated dosage of inspiration from all my past selves who have sent this message into the future.\n\nYou don't have to write once a day, it's a stupid goal to aim for and I don't really know why I do it apart from the challenge. But you can probably see how this workflow adapts to writing at any frequency. Once again, **the key insight of Mise en Place Writing is that we should decouple writing from pre-writing**. With that, our writing improves, as well as our enjoyment of writing.\n\n## Improvisation is OK\n\nWith all that work done pre-writing, I might be giving you the impression that I am strictly separating everything all the time and I don't deviate from the plan once I enter Writing Mode. Not true. Writing Mode is another opportunity to re-experience the pre-writing and the writing job all at the same time, just with the benefit of some extra prep that I've done before hand.\n\nImprovisation is OK.\n\n## Editing\n\nA lot of people put heavy emphasis on editing. I agree that it can add a lot of value. But I don't do a lot of it. For one thing, I don't have a ton of time. For another, I find it often doesn't add as much value as hoped. You can run things by [Hemingway](http://www.hemingwayapp.com/) or Grammarly but they are no substitutes for human judgment. Finding a good editor is harder than just grabbing a coworker, but that's what a lot of people do and accordingly they don't get the same value as a good editor would.\n\nI wish I had more insights here but I just don't have a ton of experience with editing. I will say that I edit my posts a lot *after* publication - they are all intended to be [living documents in a Digital Garden](https://www.swyx.io/writing/digital-garden-tos/) - so if something doesn't read quite right or someone chimes in with a crucial thing I overlooked, I will go change it. But usually, it's more important to err on the side of getting it out there rather than be bogged down in heavy editing, especially when it can be edited post-publication.\n\n## Some Context\n\nI don't have an idea for a strong ending, so I'll just end with some notes.\n\nI am strongly influenced by the thinking of [Tiago Forte's Building a Second Brain](https://www.buildingasecondbrain.com/) and [David Perell's thoughts on Writing](https://www.perell.com/blog/the-ultimate-guide-to-writing-online).\n\nI'm not trying to build a brand about anything, which is a weakness and an advantage at the same time. \n\n- I'm interested in many things, so I cast a wide net, therefore I can write about whatever I want on any particular day. \n- The downside is that people who follow me or subscribe to my newsletter don't really know what they're getting on any particular day. Therefore my efforts do not compound. \n\nIn terms of [Learning Gears](https://www.swyx.io/writing/learning-gears/), I'm very much in Explorer mode right now. But when I need to switch to Connector and Miner, I know how to pivot.\n\nBut you might see how the Mise en Place idea can work even if you focus on one particular topic and are trying to build anything from a personal brand to a full fledged content marketing practice in that area.\n\n\n"
  },
  {
    "slug": "markdown-mistakes",
    "data": {
      "title": "6 Things Markdown Got Wrong",
      "description": "John Gruber's Markdown is almost a perfect content authoring format. Here are 6 things it got wrong.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nJohn Gruber's Markdown is almost a perfect content authoring format. Here are 6 things it got wrong (in my opinion, of course).\n\n*This post is an [expanded tweet](https://twitter.com/swyx/status/1240719259505963010)*\n\n## 1. Lazy List Numbering\n\nIn Gruber's original spec, this was the way he explained it:\n\n> If you instead wrote the list in Markdown like this:\n> \n> ```md\n> 1.  Bird\n> 1.  McHale\n> 1.  Parish\n> ```\n>\n> or even:\n> \n> ```md\n> 3. Bird\n> 1. McHale\n> 8. Parish\n> ```\n>\n> you’d get the exact same HTML output. The point is, if you want to, you can use ordinal numbers in your ordered Markdown lists, so that the numbers in your source match the numbers in your published HTML. But if you want to be lazy, you don’t have to.\n\nThis laziness is good for, well, the lazy. I know some people love this feature. But we read markdown more than we write it - even as the people writing it! It is a [design philosophy of Markdown](https://daringfireball.net/projects/markdown/syntax#philosophy). In practice, more people will reorder their numbered lists just to make it look right *in markdown* (either manually, or with tooling). \n\nI can understand *why* Gruber went for it: HTML itself is pretty lazy. Normal `li` elements don't care about what order you leave them in:\n\n```html\n<ol>\n    <li>Bird</li>\n    <li>McHale</li>\n    <li>Parish</li>\n</ol>\n```\n\nBut given that in Markdown we're already typing numbers, having numbers be order independent kind of loses the meaning of using numbers at all.\n\nWell, ok. Let's say order matters in Markdown. Does that give us any value in HTML? \n\nActually, yes:\n\n```html\n<ol>\n    <li value=\"1\">Bird</li>\n    <li value=\"2\">McHale</li>\n    <li value=\"3\">Parish</li>\n</ol>\n```\n\nAnd if the author were to need out-of-order numbers for whatever reason, that would be supported out of the box as well. Pretty sweet!\n\nAs a further thought exercise, we could support lazy numbering with some *other* character than a 0-9 number:\n\n```md\n300. Bird\n100. McHale\n*. Parish - this would show as \"101\"\n```\n\nThis would be even easier to remember had Gruber not chosen to allow `*` for unordered lists instead of the pretty-much-universally-used `-` (and the never-used `+`) - but it's common for lexers to recognize sets of two characters (`*.`) over one (`*`).\n\n## 2. Code Blocks\n\nCode Blocks probably aren't necessary, and overlap with 2 important commonly used (but not headline) Markdown syntax features.\n\nJust to be clear what we're talking about - Code Blocks are how you indicate code (with 4 spaces), whereas Code Fences are the thing more people probably use today (with triple backticks).\n\nHere's how Gruber [explained them in the spec](https://daringfireball.net/projects/markdown/syntax#precode):\n\n> Pre-formatted code blocks are used for writing about programming or markup source code. Rather than forming normal paragraphs, the lines of a code block are interpreted literally. Markdown wraps a code block in both `<pre>` and `<code>` tags.\n> \n> To produce a code block in Markdown, simply indent every line of the block by at least 4 spaces or 1 tab. For example, given this input:\n> \n> ```md\n> This is a normal paragraph:\n> \n>     This is a code block.\n> ```\n> \n> Markdown will generate:\n> \n> ```html\n> <p>This is a normal paragraph:</p>\n> \n> <pre><code>This is a code block.\n> </code></pre>\n> ```\n\nIt's maaaybe possible to see why Gruber picked 4 spaces/1 tab (let's not have THAT debate) to indicate a codeblock. It's convenient to toggle what is or is not a code block just by tabbing content in and out.\n\nBut of course, code is code, and words are words - we don't just switch them back and forth willy nilly, they're drastically differnt types of content.\n\nThe problem with Code Blocks is twofold. First, they weren't designed with developer tooling in mind. Code fences have room to let you indicate the language of the code content, and have even been extended to offer other metadata:\n\n```md\n```js\nalert('this is definitely javascript!');\n```js\n```\n\nAdding [code titles](https://www.gatsbyjs.org/packages/gatsby-remark-code-titles/) and [line highlighting](https://www.gatsbyjs.org/packages/gatsby-remark-prismjs/#line-highlighting) was probably a step too far for Gruber's goals, but the core idea is that code fences are more extensible than code blocks - and, it turns out - useful for readers of the unprocessed markdown too! This blogpost you just read would be *unreadable* - and therefore *unwritable* - without code fences.\n\nCode Blocks have another related, smaller problem - they coincide with hanging list indents. Here's how Gruber described [list indents in the spec](https://daringfireball.net/projects/markdown/syntax#list): \n\n> To make lists look nice, you can wrap items with hanging indents:\n> \n> *   Lorem ipsum dolor sit amet, consectetuer adipiscing elit.\n>     Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,\n>     viverra nec, fringilla in, laoreet vitae, risus.\n> *   Donec sit amet nisl. Aliquam semper ipsum sit amet velit.\n>     Suspendisse id sem consectetuer libero luctus adipiscing.\n\nand\n\n> List items may consist of multiple paragraphs. Each subsequent paragraph in a list item must be indented by either 4 spaces or one tab:\n> \n> 1.  This is a list item with two paragraphs. Lorem ipsum dolor\n>     sit amet, consectetuer adipiscing elit. Aliquam hendrerit\n>     mi posuere lectus.\n> \n>     Vestibulum enim wisi, viverra nec, fringilla in, laoreet\n>     vitae, risus. Donec sit amet nisl. Aliquam semper ipsum\n>     sit amet velit.\n> \n> 2.  Suspendisse id sem consectetuer libero luctus adipiscing.\n\nNice - and extremely handy - except it is now pretty confusing whether or not a code block is at the top level or inside a list item.\n\nIf you reserve indents for cosmetic and list item membership purposes then you have no such ambiguity. Code fences (or maybe more accurately, the backtick <code>`</code>) tell you what is or isn't code. That's it.\n\nOf course, today Code Fences, with language tags for syntax highlighting are in very widespread usage. I'm not sure who invented them, but I definitely know that it is widely used because [GitHub Flavored Markdown](https://twitter.com/swyx/status/1099801305990590469) made it so and I am grateful for that.\n\n## 3. Markdown in HTML in Markdown\n\nA genius decision by Gruber was to have Markdown be a superset of HTML. He called it [Inline HTML](https://daringfireball.net/projects/markdown/syntax#html) - but you can remember it as HTML-in-Markdown:\n\n\n> For example, to add an HTML table to a Markdown article:\n> \n> This is a regular paragraph.\n> \n> ```html\n> <table>\n>     <tr>\n>        <td>Foo</td>\n>     </tr>\n> </table>\n> ```\n> \n> This is another regular paragraph.\n\nThis is pretty handy for the many cases where Markdown just doesn't give you enough control (more on that later). But sometimes you just want to define some wrapper components, and then use Markdown for the rest:\n\n```html\n<div class=\"myContainerClass\">\n\n    ## My Request\n    \n    I want to use *Markdown* here, not HTML!\n\n</div>\n```\n\nIn other words, we want Markdown in HTML in Markdown.\n\nOnce again, [GitHub Flavored Markdown](https://twitter.com/swyx/status/1099801305990590469) to the rescue. This is EXTREMELY useful for the one stateful UI element offered to you by GitHub - the details/summary tag:\n\n```md\n\n## Docs for my new startup\n\nThis is my startup! Short and sweet pitch here.\n\n<details>\n    <summary>If you'd like to read our in-depth vision, click here</summary>\n\n    ## Our vision\n\n    Alpha ecosystem user experience. Hackathon incubator business-to-consumer assets focus. Termsheet stealth first mover advantage scrum project client long tail business-to-business user experience entrepreneur backing product management rockstar venture. Business-to-business analytics market disruptive crowdsource creative paradigm shift infographic metrics network effects lean startup accelerator stealth customer. Vesting period growth hacking partnership scrum project validation interaction design handshake sales assets business-to-consumer. Ownership early adopters graphical user interface funding influencer A/B testing user experience. Return on investment infographic investor partnership growth hacking business plan user experience deployment churn rate assets first mover advantage buyer startup. Non-disclosure agreement investor sales angel investor. Mass market creative angel investor freemium network effects investor business-to-consumer supply chain bootstrapping twitter hypotheses early adopters. Traction MVP paradigm shift series A financing virality market alpha.\n\n</details>\n\n```\n\n[John Gruber actually saw this criticism](https://twitter.com/gruber/status/1240888155307495426) and replied: \n\n> `markdown=“1”` as a tag attribute is the way to solve #4 properly. But there’s a question of how you should handle block vs span elements.\n\nSo this is a solution I wasn't aware of, but it seems it is a [nonstandard attribute](https://stackoverflow.com/questions/57120694/markdown-1-not-working-inside-the-p-tag) so it is not in wide use.\n\nThe next 2 points are common usecases that necessitate \"de-opting\" from Markdown, causing the need for Inline HTML in the first place - if we just had these 2 things we would be able to stay in Markdown a lot longer.\n\n## 4. No Syntax for Adding Classes\n\nThere are only 2 mentions of classes in the entire Markdown spec. That's probably not a surprise - [as it says](https://daringfireball.net/projects/markdown/syntax#html):\n\n> The idea for Markdown is to make it easy to read, write, and edit prose. HTML is a publishing format; Markdown is a writing format. Thus, Markdown’s formatting syntax only addresses issues that can be conveyed in plain text.\n\nUltimately, I think, Markdown cannot be fully orthogonal to HTML, and it doesn't really try to be. It isn't a design goal of Markdown to let you do everything you can do with HTML, but still it is essentially a language that *compiles down* to HTML. \n\nAdditionally, normally everything Markdown does is visible to the end user - it doesn't even have comments! Yet, classes are a weird feature of HTML. Are they visible to the end user? By default, no. But by all practical purposes they are the primary hooks CSS uses to modify how things appear to users. \n\nUsing wrapper divs is often a great way to manage layout and spacing and other important design considerations. As discussed above, because there is no Markdown-in-HTML-in-Markdown, any child content of a wrapper div then has to become HTML too, and you lose the benefit of using Markdown at all.\n\nUnfortunately my case is diminished greatly by the lack of constructive suggestions to do this well. [Axel Rauschmayer chimed in](https://twitter.com/swyx/status/1241011813766107139) to offer [Pandoc's format](https://pandoc.org/MANUAL.html) - as a Markdown flavor, I'd wish for something that degrades gracefully to common Markdown viewers, like code fences do. If Gruber had designed this into Markdown from the get-go, whatever the syntax, we would not have this problem or this objection.\n\n## 5. No ID's in Headers\n\nThis is important for URL accessibility. Here we have a normal markdown header:\n\n```md\n# My Title\n```\n\nand this compiles to:\n\n```html\n<h1>My Title</h1>\n```\n\nbut do the same in GitHub, and this results:\n\n```html\n<h1><a id=\"user-content-mytitle\" class=\"anchor\" aria-hidden=\"true\" href=\"#mytitle\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>mytitle</h1>\n```\n\nThis lets GitHub offer its nice hover effect to remind you you can copy anchor links, but you don't really need an `<a>` tag for an anchor link - you could just compile to:\n\n```html\n<h1 id=\"my-title\">My Title</h1>\n```\n\nAnd that would work.\n\nThere's nothing I hate more than reading a good article with headers, wanting to link someone directly to the section relevant to them, and then not having an `id` to use to send them straight there.\n\n> 💁‍♂️ Side note: I LOVE the [Display #Anchors](https://chrome.google.com/webstore/detail/display-anchors/poahndpaaanbpbeafbkploiobpiiieko?hl=en) chrome extension for toggling this on and off!\n\nOf course, offering ID's for headers would also mean taking a stance on how to slug-case content. How would you slug case this?\n\n```md\n# Something: with Punctuation? (Part II - Dealing with Emojis 🤯) \n```\n\nThere are also quibbles about case sensitivity and hyphens vs underscores. But I don't really care - pick one, let me link directly to content!!\n\nOf course, GitHub Flavored Markdown deals with this with GitHub's postprocessing and you can add this feature in widely used Markdown processors. But it would have been nice to be in the spec.\n\n## 6. No Option for Metadata\n\nThis is related to the \"no way to specify non-visible things\" complaint above, but on a whole-document level instead of a per-element level. Sometimes you want to offer Markdown processors some extra data about how *this specific Markdown document* should be handled differently than others, or just specify other non-HTML data for use by other toolchains like a blogging engine (e.g. [specify what layout format to use](https://www.11ty.dev/docs/layouts/), or what categories the blogpost belongs to, and so on).\n\nInstead of having to specify this data in a separate file, it'd be nice to colocate this metadata alongside the content. \n\nThere's a reason this probably wasn't offered by Gruber - it involves another language that isn't Markdown and isn't HTML. I don't know this for a fact but it seems like Jekyll was the first to introduce [Front Matter](https://jekyllrb.com/docs/front-matter/) as the widely accepted Markdown metadata format - and it uses YAML, which predates Markdown, but only by 3 years. \n\nYAML itself has plenty of vocal detractors, so it isn't perfect. But *something* for Metadata would have been really, really nice.\n\n## Conclusion\n\nOf course, Gruber got everything else right, which is why we're even talking about it today. I don't even want to know all the competing alternatives present at the time and have to pick between them. Sometimes, [Worse is Better](https://en.wikipedia.org/wiki/Worse_is_better). \n\nAnd in the end, Markdown was successful enough. I am writing this blogpost in Markdown, and since I write every day, that means I write Markdown every day. I also build tools that consume Markdown, and live primarily on the web, and thus I feel the painpoints more intimately than others might.\n"
  },
  {
    "slug": "farewell-netlify",
    "data": {
      "title": "Farewell, Netlify",
      "description": "On leaving Netlify",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nToday is my last day at Netlify. It is a bittersweet moment, but the end of a formative period of my career that I will always be grateful for.\n\nNetlify is the first job I ever got through [Learning in Public](https://www.swyx.io/writing/learn-in-public). I remember that day well. In fact the whole concept was [less than a week old](https://twitter.com/swyx/status/1009174159690264579?lang=en) to me when [Matt Biilmann](https://twitter.com/biilmann) reached out. He bet *early* on me - super early. I guess that's what founders do? I had yet to give [my first conference talk](https://www.swyx.io/speaking/react-not-reactive/), and I had less than a year of experience as a card carrying web developer (although I had coded as part of my job in my finance career). I had never done developer relations professionally. I had no track record of working remotely - and yet the entire interview process was remote, and the first time I physically met any coworkers was after I had joined the company. A lot of firsts. Everyone who interviewed me took a leap of faith that I could do the job, and I am equally impressed at that as I am thankful.\n\nNetlify is the first high growth startup I've been a part of, from [Series B to C](https://twitter.com/swyx/status/1235246651553517568). At none of my previous jobs have I counted growth in X's instead of percentages, and it strikes me that this is actually pretty rare outside of my VC-fueled bubble. I think Netlify users are vaguely aware it is growing, but it must seem smooth from the outside. I think it's a little like a duck swimming - the visible part is gliding serenely above water, and underneath is some rather ungainly paddling.\n\nA lot of companies aspire to category creation, but Netlify actually *did it*. In the past two years [JAMstack](http://jamstack.org/) went from a sorta kinda weird niche term to something *other* companies have adopted, to the point of copying everything from talking points to app design to literal marketing copy. The flattery is sincere. Meanwhile Netlify went from 1 conference in our first year to 3 the next. [Books](https://www.netlify.com/oreilly-jamstack/). [Workshops](https://frontendmasters.com/workshops/jamstack/). [FreeCodeCamp courses](https://www.youtube.com/watch?v=A_l0qrPUJds). [Entire](https://www.netlify.com/blog/2019/12/16/introducing-scully-the-angular-static-site-generator/) [frameworks](https://redwoodjs.com/). More I can't think of right now. It warms the cold dead heart of natural cynics like me. It didn't Just Happen, it was the result of a tremendous amount of hard work by the people in Netlify (and allies outside). Too many to name here for fear of leaving someone important out. But I will carry what they taught me everywhere I go.\n\nSome people diss JAMstack as frontend developers trying to be full-stack. I get what they mean, but also there's a very real way it is not a diss. JAMstack related technologies and services reduce (not eliminate) a lot of steps that stand in the way of frontend developers getting fast and secure sites out into the world, for less. For those who still don't quite get it, [Chris Coyier describes it best in this talk](http://full-stack.netlify.com/) - all else is tiresome debates over semantics. For customers, I have joked in the past that Netlify is a secret hack around nonproductive and nonexistent platform teams ([Reverse Conway's Law](https://twitter.com/conways_law/status/1241037003552182279)) - for product engineers and [indie hackers](https://www.indiehackers.com/article/jamstack-for-indie-hackers-b07f7a943d) alike. For me the diss is true in a more literal sense - I went from 100% frontend product engineer to being able to run workshops on [serverless tech](https://www.youtube.com/watch?v=PoqWF9BKtOE) and [CLIs](https://egghead.io/courses/build-custom-cli-tooling-with-oclif-and-typescript), and having informed opinions about adjacent spaces. This job is a Mario Mushroom for hungry people who learn in public.\n\nAbove all, I think, Netlify is a great group of people. Netlify showed me how you can hire extremely competent people who are also *nice*. Nice, diverse, inclusive, funny, self aware, humble, transparent, I could go on. Of course there doesn't have to be a tradeoff between being nice and being a great developer. But it's good to have proof.\n\n> See also: [Netlify Year One](https://www.swyx.io/writing/netlify-year-one/)"
  },
  {
    "slug": "netlify-build-plugins",
    "data": {
      "title": "What You Can Do with Netlify Build Plugins",
      "description": "Netlify Build Plugins are here! Here's a smattering of thoughts on what you can do with them.",
      "tag_list": [
        "netlify",
        "plugins"
      ]
    },
    "content": "\n[Netlify Build is coming](https://github.com/netlify/build)! It is in private beta now, and will go into public beta soon (and you can try it out as part of [the RedwoodJS tutorial](https://redwoodjs.com/tutorial/deployment.html#deployment)).\n\nThere are a number of new capabilities Netlify Build will enable, but probably the one you'll hear about most is the new plugin system. [Chris Coyier](http://twitter.com/chriscoyier), of course, [already got the scoop on this](https://css-tricks.com/netlify-build-plugins-announcement/) when it was [announced at JAMstack Conf](https://www.youtube.com/watch?v=4m6Hi4_qEVE), [Sarah Drasner](http://twitter.com/sarah_edo) has written about [creating your first Netlify Build Plugin](https://www.netlify.com/blog/2019/10/16/creating-and-using-your-first-netlify-build-plugin/) (note: APIs have changed) and I've recently written about how to [write quality plugins with Unit and Integration testing](https://www.swyx.io/writing/testing-plugin-authors).\n\nI've spent the past couple months dabbling with Build Plugins, and I thought I should share what I've worked on, thought about, and seen done by others.\n\n> ⚠️ Note: That Beta label is Beta for a reason - Netlify reserves the right to change these APIs as it observes real usage. So we will focus on general principles over specific APIs.\n\n## Capabilities\n\nWhat you can do as part of plugins is circumscribed by A) what the build system does and B) exposes to you, the plugin author/user. So we should understand both.\n\n### What A Build Does\n\nI'll adapt from [the excellent docs](https://github.com/netlify/build/blob/master/README.md#background):\n\n1. Netlify clones your repo\n2. If not cached, dependencies are installed in the project. If the cache is still valid (based on a manifest like `package.json`), it simply restores the cache\n3. Netlify runs your build command\n4. Files & dependencies are cached\n5. Netlify runs proprietary [post processing](https://docs.netlify.com/site-deploys/post-processing), including detecting [Forms](https://docs.netlify.com/forms/setup/#html-forms), [Functions](https://docs.netlify.com/functions/overview/), and Redirects\n6. Functions are deployed, if detected\n7. Your site is deployed\n\nI'm omitting a lot. For Netlify to be as low-config as it is, and still support every ecosystem from PHP to Dotnet Core, there is a *ton* to get right. For the Docker-literate, you can see the exact, nonsimplified steps of Netlify's Build Image (aka Buildbot) in [its Dockerfile](https://github.com/netlify/build-image/blob/xenial/Dockerfile). [I'd recommend Brian Holt's Intro to Docker](https://dev.to/swyx/three-jobs-of-containers-436p) for those who aren't.\n\nPreviously, the only user-configurable section was Step 3, the build command. If you wanted to, for example, do stuff between steps 1 and 2, or steps 3 and 5, you'd have to write scripts in a variety of places, like [npm scripts](https://docs.npmjs.com/misc/scripts), or a Webpack plugin, or a site generator specific lifecycle. And what if you needed to pass information between steps? Fuhgeddaboudit!\n\n### What Netlify Build Exposes\n\nWith Netlify Build, the potential to programmatically control steps 1 - 7 and more is there (but has not yet been promised so don't hold them to it). For now, Netlify Build exposes a subset of that functionality (note, I expect this list to change over time):\n\n\n\n| Event   | Description                        |\n| :------------------------------------------------------------------- | :--------------------------------- |\n| onInit | Runs before anything else          |\n| ‏‏‎onPreBuild | Before build commands are executed |\n| ‏‏‎onBuild | Build commands are executed        |\n| ‏‏‎onPostBuild | After Build commands are executed  |\n| ‏‏‎onSuccess | Runs on build success              |\n| ‏‏‎onError | Runs on build error                |\n| ‏‏‎onEnd | Runs on build error or success     |\n\nSo you don't have access to *everything*, but you have more than you had with a simple build command.\n\n## Plugin Categories\n\nIf I were to map a build system like this to tools I'm already familiar with, it'd be a combination of CI/CD tool (like, say, Jenkins or GitHub Actions) and static site generator/bundler lifecycles (which somehow superceded [old school task runners](https://blog.logrocket.com/node-js-task-runners-are-they-right-for-you-bb29ea30b7fa/)). So you'd expect the possible plugins to be a superset of that.\n\n> Note: these are by no means formal categories endorsed by Netlify, this is just me randomly coming up with classifications. Don't let this limit your thinking.\n\n### Notifiers\n\nThe lightest weight plugins (that would, for example, have the least amount of config and the lowest impact on workflow and build times) are ones that simply dispatch information about the build after a build is done.\n\n- [netlify-build-plugin-speedcurve](https://github.com/tkadlec/netlify-build-plugin-speedcurve) notifies Speedcurve to run async tests\n- You can [notify your boss via Twilio](https://www.netlify.com/blog/2019/10/16/creating-and-using-your-first-netlify-build-plugin/#writing-our-simple-twilio-build-plugin) or [Slack](https://www.netlify.com/blog/2016/07/18/shiny-slack-notifications-from-netlify/) when you're done deploying!\n- [netlify-plugin-checklinks](https://github.com/munter/netlify-plugin-checklinks) - check that your links are valid!\n\nMost notifications can be enhanced with some progressive sense of history - so if you make it easy to access the cache, you can, for example, warn or block on regressions in performance, as I did in this [unpublished Lighthouse plugin](https://github.com/sw-yx/netlify-plugin-lighthouse).\n\n### Cache/Asset Optimization\n\nFaster builds, and faster sites.\n\n- [netlify-plugin-image-optim](https://github.com/chrisdwheatley/netlify-plugin-image-optim) - Optimizes PNG, JPEG, GIF and SVG file formats.\n- [netlify-plugin-subfont](https://github.com/munter/netlify-plugin-subfont) - analyses your usage of web fonts, then reworks your webpage to use an optimal font loading strategy for the best performance. [By the guy who made subfont, Munter!](https://github.com/Munter/subfont)\n- [netlify-plugin-cache-nextjs](https://github.com/pizzafox/netlify-cache-nextjs#readme) - caching Next.js builds\n- [netlify-plugin-gatsby-cache](https://github.com/jlengstorf/netlify-plugin-gatsby-cache#readme) - caching Gatsby builds (see a pattern?)\n\n### Deploy Blockers\n\nYou could, abstractly, block a deploy if some precondition were not met. This could be viewed as similar to CI, except that this could happen *after* integration, and can block *deploys* instead of just *builds*, on a wider set of conditions. It is common to `throw new Error` to block a build, but this bears no distinction from *real* errors which you should pay attention to - therefore, Netlify [offers some utils](https://github.com/netlify/build/blob/master/docs/creating-a-plugin.md#error-reporting) like `failBuild`, `failPlugin`, and `cancelBuild` to indicate whether something is wrong or working as intended.\n\n- [netlify-plugin-no-more-404](https://github.com/sw-yx/netlify-plugin-no-more-404) is a plugin I worked on that checks you don't break the web by removing a page without also accounting for it with a [redirect](https://docs.netlify.com/routing/redirects/)!\n- [netlify-deployment-hours-plugin](https://github.com/neverendingqs/netlify-deployment-hours-plugin) - Block deployment on Fridays? Non working hours? sure!\n- [netlify-plugin-a11y](https://github.com/sw-yx/netlify-plugin-a11y) - You could ensure you never deploy anything that fails accessibility checks!\n\n### Generate Source Files\n\nSometimes part of your content lives elsewhere other than git. Netlify Build can pull it down for you before you run your build.\n\n- [netlify-plugin-fetch-feeds](https://github.com/philhawksworth/netlify-plugin-fetch-feeds) - source content from remote feeds including RSS and JSON\n- [netlify-plugin-yield-data-for-eleventy](https://github.com/philhawksworth/netlify-plugin-yield-data-for-eleventy) - expose data collected in the Netlify build cache for Eleventy\n- [netlify-plugin-ghost-markdown](https://github.com/daviddarnes/netlify-plugin-ghost-markdown) - Generates posts and pages from a Ghost publication as markdown files\n\n### Generate Build Artefacts\n\nBefore deploying, you can statically analyze your build, and then build atop your build! This is most often used to create static build artefacts, but because Netlify has an integrated serverless solution, you can generate that too. (My hope is that [all first party products](https://www.netlify.com/products/) are up for programmatic provisioning in future)\n\n- [netlify-plugin-rss](https://github.com/sw-yx/netlify-plugin-rss) - generate an RSS feed!\n- [netlify-plugin-sitemap](https://github.com/netlify-labs/netlify-plugin-sitemap) - generate a sitemap!\n- [netlify-plugin-search-index](https://github.com/sw-yx/netlify-plugin-search-index) - generate a static JSON blob or serverless endpoint to search your own site!\n\n### Weird Combos\n\nWhat you can do is really limited by your imagination. (Cue cliché: \"We can't wait to see what you come up with!\") Here's a weird one of mine that actually came up due to real life needs - [netlify-plugin-encrypted-files](https://github.com/sw-yx/netlify-plugin-encrypted-files). It helps you partially obscure files (names and contents) in git repos, so that you can partially open source your site, while still being able to work as normal on your local machine and in your Netlify builds.\n\n### BYOP\n\nYou don't have to use plugins others write, either. Since Netlify Build allows local plugins, creating local forks and adjustments is mostly a copy-and-paste job (as is the reverse workflow of develop-locally-then-publish-separately - this is how I do local testing for [netlify-plugin-rss](https://github.com/sw-yx/netlify-plugin-rss)).\n\n## Conclusion\n\nIt's still early days yet but I hope to have spelled out a little of how *I* personally place a plugin system in context of something like Netlify.\n\nEvery platform eventually aspires to have plugins - a very challenging tradeoff because it makes users' lives easier and helps third parties integrate/become more discoverable, and yet the security and maintenance concerns are high. You can't just ban or artificially constrain [Remote Code Execution](https://searchwindowsserver.techtarget.com/definition/remote-code-execution-RCE), especially if your whole job is to run other people's code. (If you run other people's code *on other people's browsers*, you should read Figma's writeup **[How to build a plugin system on the web and also sleep well at night](https://www.figma.com/blog/how-we-built-the-figma-plugin-system/)**).\n\nBut also, every pluginized platform eventually figures out that it, itself, can be composed of a bunch of plugins too. This was [webpack's realization](https://webpack.js.org/api/plugins/), and it is comparable to the [Lean Core](https://github.com/facebook/react-native/issues/23313) idea from React Native. It turns out that forcing yourself to be a first class consumer of your own plugins makes both the plugins and your system more maintainable and reliable, just like how it is a good idea to [only consume internal services through public APIs.](https://gist.github.com/chitchcock/1281611)\n\nThere's a ton more things to think about, which is what makes this an exciting engineering challenge. Should plugins have plugins? Will you allow point-and-click configuration through UI? How does that translate to a local dev setup? How can you expose more core capability without overwhelm?\n\nIt will be exciting to watch Netlify's plugin ecosystem come to life. You can learn more on [the Netlify Build Repo](https://github.com/netlify/build)."
  },
  {
    "slug": "twitter-metacommentary",
    "data": {
      "title": "Twitter as Universal Meta-Commentary Layer",
      "description": "Musing about how Twitter can enhance your Internet reading experience",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nTwitter has enhanced my Internet experience in 2 similar ways recently - **Webmentions** and **Twitter Links**. I think these things point toward something we want as part of the permanent fabric of the social Internet.\n\n## Webmentions\n\nI've written previously about [the Indieweb Webmentions movement](https://www.swyx.io/writing/clientside-webmentions/), which you can see at the bottom of most posts on [my site](https://www.swyx.io). \n\nWith Webmentions, you let people comment on domains they control, and weave them together with your own content so that people who read it can easily discover reactions from other people, free from walled-garden platforms like Facebook and Linkedin, or proprietary account systems like Disqus or Wordpress. \n\nThe [Indieweb](https://indieweb.org/) movement has a strong \"stick it to the man\" vibe, but unfortunately it takes a lot more work for a lot poorer quality experience. So a compromise is to use Twitter as a basic auth/microblogging source that everybody can use. \n\nSo far, [the Bridgy service](https://brid.gy/) has had some bugs and [doesn't support backfill](https://twitter.com/swyx/status/1226742112626057216), but otherwise, I think it's great! It lets me, the author, add some social proof and interaction, but lets readers and commenters post on something they \"own\" and the conversation easily continues off-page for me and other readers. In a very small way, it lets my blogposts come alive.\n\n## Twitter Links\n\nMore recently, I've been enjoying Internet-friend [Maxim Leyzerovich](https://twitter.com/round)'s [Twitter Links Chrome Extension](https://github.com/round/Twitter-Links-beta). It's so simple - while you're reading something interesting, you can hit the extension and it puts that URL into Twitter search! \n\nThis lets you a) easily find the creator if they are on Twitter, or b) see what others are saying about it.\n\n![image](https://user-images.githubusercontent.com/6764957/77026532-4008f480-696a-11ea-9881-b6cb6e04f4f5.png)\n\nThis is like Webmentions on demand, with no crawling service needed and built in social interaction.\n\n## Why Meta-Commentary Helps\n\nIf you've read my thoughts on [First Principles Thinking](https://www.swyx.io/writing/first-principles-approach), it might seem surprising that I also am a fan of Meta-Commentary. \n\nIt's true, there's probably too much of \"let the reactions tell me what to think\" on the Internet. But that's valid when I don't have the context and knowledge to know any better myself.\n\nAdditionally - once I've read something and have a First Principles™ take - it's *still* helpful to see reactions of others, read anything else they might bring up, and interact with them directly.\n\nWhat is unique about this is that this is uniquely human. There are no personalized algorithms for you to fall down your own personal rabbit hole. You *have* to engage with real people, on their terms, and put your own identity on the line as you go.\n\n[Visa Veerasamy made Twitter his entire notetaking system](https://www.youtube.com/watch?v=yQHT73b5CtE) - see how he reads books in a threaded style, and indexes his own Tweets.\n\n## Twitter and the Metaverse\n\nNot everyone is familiar with the idea of [the Metaverse](https://en.wikipedia.org/wiki/Metaverse) - because it's mostly used in the context of virtual reality - but my broad interpretation of its essence is that it is the ideal form of a decentralized Internet - where you can seamlessly hop from world to world to world, instead of having everyone being centralized on one of 3-5 platforms.\n\nOf course, the irony of using Twitter, a centralized platform, to achieve a decentralized goal of the Metaverse, is not lost on me. However, the experience is simply better than all decentralized alternatives (like [Mastodon](https://joinmastodon.org/)) right now (since [the primary feature of any social network is the people already on it](https://stratechery.com/2016/the-audacity-of-copying-well/)). My bet is that [Twitter's own decentralization](https://www.theverge.com/2019/12/11/21010856/twitter-jack-dorsey-bluesky-decentralized-social-network-research-moderation) will succeed.\n\nBut the core reason why Twitter works as the base layer for Universal Meta-Commentary is because the format is so constrained. It is unstructured text, but you can build the Metaverse atop linked URLs - [the coolest feature of the web](https://www.w3.org/Provider/Style/URI) - and lets you add some images and text, and that's it. If you want more, you'll have to thread it, or blog off-site. In the meantime, you're forced to crystallize your thoughts to a short statement. [Twitter is AMP for thoughts](https://twitter.com/swyx/status/1163443579051753472).\n\nI think we can build a lot more Meta-commentary and Metaverse tooling with a Twitter or Twitter-like layer as the natural link between worlds."
  },
  {
    "slug": "frontend-observability",
    "data": {
      "title": "Observability for Frontend Developers",
      "description": "Some thoughts on how frontend developers can also embrace instrumenting their apps for observability",
      "tag_list": [
        "observability",
        "webdev"
      ]
    },
    "content": "\n[Observability](https://docs.honeycomb.io/learning-about-observability/intro-to-observability/) is a hotly debated topic in the backend world, and I accidentally got involved when I tried to match up my knowledge of open source JS tools to the Monitoring pillars of [Metrics, Logs and Traces in a prior blogpost](https://www.swyx.io/writing/js-tools-metrics-logs-traces/). Charity Majors did everyone a great service by [schooling me on what Observability really meant](https://charity.wtf/2020/03/03/observability-is-a-many-splendored-thing/) and [Chase Adams had the awesome idea](https://twitter.com/chaseadamsio/status/1231945577748819972?s=20) that she and I should have a Real Chat™ about how frontend developers can embrace Observability too.\n\nWe did chat ([recorded screenshare here!](https://www.youtube.com/watch?v=QyME7OSLd1Y&feature=youtu.be)), and this post is the result! \n\n## Bottom Line Up Front\n\nI see 2 opportunities for Frontend Developers to instrument for Observability - their clientside apps, and their developer tooling. The former has much more established thinking than the latter.\n\n[I have added Honeycomb to Excalidraw in this proof of concept](https://github.com/sw-yx/frontend-observability), a popular open source React sketch drawing app, so you could envision what the code might look like in practice (of course in real life it would be much more heavily instrumented).\n\nThe key here is to shield the Honeycomb API key in a serverless function (here we use [a simple Netlify Function](https://github.com/sw-yx/frontend-observability/blob/swyx/addHoneycomb/functions/honeycomb/honeycomb.js)), and to pipe events through it. In production, you might add a check for authentication before sending over the events. You can also append serverside information to your clientside event (including user metadata, for debugging) before sending it in to your event store.\n\nBut this only tells you the **how** - let's talk about the **why**.\n\n## The Core Problem\n\nWhat do we want with observability, and why should we frontend developers care?\n\nI'll quote from [Charity's tweetstorm](https://twitter.com/mipsytipsy/status/1231833240433917952):\n\n> Can you understand what is happening inside the system, can you understand ANY internal state the system may get itself into, simply by asking questions from the outside?\n>\n> At its core, observability is about these unknown-unknowns.\n>\n> Plenty of tools are terrific at helping you ask the questions you could predict wanting to ask in advance.  That’s the easy part.  “What’s the error rate?”  “What is the 99th percentile latency for each service?”  “How many READ queries are taking longer than 30 seconds?”\n>  - Monitoring tools like DataDog do this — you predefine some checks, then set thresholds that mean ERROR/WARN/OK.\n>  - Logging tools like Splunk will slurp in any stream of log data, then let you index on questions you want to ask efficiently.\n>  - APM tools auto-instrument your code and generate lots of useful graphs and lists like “10 slowest endpoints”.\n>\n> But if you *can’t* predict all the questions you’ll need to ask in advance, or if you *don’t* know what you’re looking for, then you’re in o11y territory.\n\nIn my own words, it's about giving yourself/your team the power to answer open ended questions about how your app behaves in production. Not just obvious stuff like when errors occur, but also subtle UX-research-ey stuff like \"How do employees and users use our tool differently and why?\" - a great story covered in [Emily Nakashima's talk at O11yCon](https://youtu.be/VA0b6v9vaEM). I won't go further into how Observability differs from Monitoring and the Metrics/Logs/Traces data types - [Charity definitively covered that in her post](https://charity.wtf/2020/03/03/observability-is-a-many-splendored-thing/).\n\nObservability, as described, is a high bar - higher than we're used to. But notice none of these issues are technically restricted to the backend.\n\n## Tooling\n\nProbably some tools come to mind - [LogRocket](https://logrocket.com/), [Sentry](http://sentry.io/) and [BugSnag](https://www.bugsnag.com/) mainly because of their generous sponsorship of frontend dev podcasts. For other frontend devs, maybe you're told to send events and logs to Google Analytics or Mixpanel or report metrics to Datadog or Splunk or New Relic. For UX research you might use [FullStory](https://www.fullstory.com/) or [Heap](https://heap.io/) or [Hotjar](https://www.hotjar.com/). \n\nMaybe you pay for multiples of these, but thats kind of the problem - you're paying to store disparate bits of the same data in multiple places, all disconnected to each other and unhelpfully siloed.\n\nHoneycomb doesn't have a monopoly on Observability, but it certainly is investing heavily in customer education around the idea that you should have a **single source of truth** for all these events, and to aggregate these events into metrics, logs or traces on demand instead of pre-aggregating and losing all granularity.\n\nIt's difficult for me to comment on all these tools mainly because I don't have experience with most of them. It really depends on what kind of company you work at and what you already use. \n\n**But we want to go beyond tools**.\n\n## Fight Log-o-phobia\n\nObservability is more about the mindset that instead of setting alerts to tell you when stuff you expect to go wrong goes wrong, you should instead **be in constant conversation with your code**, getting a good gut feel for how it works in production, and instrumenting so that you can *answer questions you don't even have yet*.\n\nIf you've ever felt **Log-o-phobia** (the fear of looking at logs, which I just totally made up), it could be because your systems don't help give you enough information to reconstruct the sequence of events after the fact. What if instead of being afraid of your tools, you got a dopamine rush from having everything you need to figure it out fast? \n\nBut enough about tools - what concrete things can we as frontend engineers report?\n\n## Opportunity 1: Instrumenting Frontends for Observability\n\nMost of this is straight up verbatim from [Emily's talk at O11yCon](https://youtu.be/VA0b6v9vaEM), which you should definitely go watch, but I'm just listing them here for the sake of completeness.\n\nYou could send an event per thing of interest, for example:\n\n- On page load ([Emily has a great blogpost on this](https://www.honeycomb.io/blog/instrumenting-browser-page-loads-at-honeycomb/))\n- On SPA navigation\n- On significant user actions\n- On error\n- On page unload\n\nI think there is room to evolve here. As we discussed in our chat, Frontend Developers think more in terms of Sessions. We had some debate over what a session is - In my prior experience I've defined them as contiguous 30 minute blocks, whereas Charity was keen on much more tightly scoped sessions, between 1-10 minutes, since a lot of tracing happens during that session and it gets very wide.\n\nThere's no hard and fast rule for what you put in an event - at the end of the day you're just trying to give your future self/your team the best possible chance of figuring out the internal state of your app when the observed behavior is going on. But, for example, you might imagine it is useful to tie together page load and unload events, or to track what A/B test bucket the user is in, what permissions they have, what browser they use, and so on.\n\nHere's a full list so you get an idea:\n\n- App specific\n  - Page type\n  - User id (is employee?)\n  - Ab testing groups\n- Performance/environment\n  - Load time\n  - Resource count\n  - Asset version\n  - Request id\n- Capabilities\n  - Screen height/width\n  - User agent\n  - Window height/width\n  - Color depth\n  - Feature support\n- Others?\n  - installed fonts\n  - browser language\n  - online/offline status\n  - Geolocation?\n  - Page visibility?\n  - Zoom level? \n  - Font size?\n\nThat's just the baseline - you can go on to create custom derived metrics out of your events to figure out things like how long it takes your users to do certain key actions and why they fail (see [Rachel Fong's great story in the O11yCon talk](https://youtu.be/VA0b6v9vaEM?t=1497)). You can create great metrics like detecting Refreshes or [Rage clicks](https://blog.fullstory.com/rage-clicks-turn-analytics-into-actionable-insights/). \n\nI will say this is where Honeycomb doesn't do a great job yet - it is common for frontend-focused tools like [LogRocket](https://logrocket.com/) to offer Rage click detection out of the box, whereas in Honeycomb you'll both have to instrument and devise a query for detecting this. But once you make a query, it's trivial to share and reuse it, and I'm sure Honeycomb will look at building it in someday. \n\n## See this in action\n\nHere is where you should really check out [my Proof of Concept](https://github.com/sw-yx/frontend-observability) if you'd like to see what the code looks like. All this work takes place in two commits:\n\n- [Adding a serverless function and calling it from inside the React app](https://github.com/sw-yx/frontend-observability/commit/5be976f9177ddbb412ea35357e54480b0c251084)\n- [Adding browser load/unload tracking](https://github.com/sw-yx/frontend-observability/commit/e4c10b6ea8708bcc97e3ed753ba39ffd467c9b18)\n\nYou can read a detailed code discussion in the [README](https://github.com/sw-yx/frontend-observability/blob/swyx/addHoneycomb/README.md).\n\n## A quick note on Privacy\n\nOf course, with this much data collection, we need to be careful to be sensitive around privacy, especially with Personally Identifiable Information, or HIPAA/PCI and GDPR sensitive information. It seems the practice here is to hash/encode that data before sending it over to your event store. Honeycomb lets you run a proxy server that they interface with, so that sensitive info only lives on a server you control. \n\nPersonally I like the idea that most of the time you just work with an anonymized/double blind user ID to do your work, and all the deanonymizing information is kept far away under someone else's lock and key.\n\n## Opportunity 2: Instrumenting JavaScript Developer Tools\n\nAs someone who works on and uses JS tools, and thinks [JavaScript Tooling Sucks](https://www.swyx.io/writing/js-tooling/), this one really strikes home as an area of opportunity.\n\nHow many times have we had a build go bad and had no idea why? How many times does Gatsby or Sapper or Docusaurus or Webpack or Parcel or whatever else we use plain not work the way we wanted it to, whether it was through a misconfigured plugin or a misnamed file or (yes, this just happened to me) a corrupted binary? How many times have we had to close issues because we \"cannot reproduce\" them, or had trouble getting users to report the right information for us to help them?\n\nI think we could do a lot to make our CLIs and browser extensions and language servers and dev servers and everything else more observable. The key difference is we're exposing this data to the users, rather than just the developers of the tools. They can then choose to send this data to us alongside bug reports or we can provide them tools to figure out their own problems on their own from the events logged therein.\n\nIn my chat with Charity, she mentioned that Intercom's engineers had instrumented their own CI/CD pipeline this way, to figure out why their builds can go wrong. I think this is an excellent idea and we'll likely need different thinking and data structures to tackle this user-facing observability.\n\n## Watch our discussion\n\nIf you'd like to hear me grill Charity more on Frontend Observability, have a watch!\n\n[![Screenshares with Swyx (3)](https://user-images.githubusercontent.com/6764957/76673365-a1038780-657a-11ea-91ad-1e2ddbc5726e.png)](https://www.youtube.com/watch?v=QyME7OSLd1Y&feature=youtu.be)\n"
  },
  {
    "slug": "webperf-tests",
    "data": {
      "title": "Every Web Performance Test Tool",
      "description": "Check your site's speed quickly with a battery of tests",
      "tag_list": [
        "webdev"
      ]
    },
    "content": "\n*Canonical URL: https://www.swyx.io/writing/webperf-tests/*\n\nHere is every Web Performance Test Tool I know of to help identify issues in your site/get you some key speed metrics. The idea is you think of a feature you want, e.g. \"TTFB\" or \"locations\" or \"waterfall\" and just Cmd + F on this page to find a tool that helps you with that thing.\n\nPlease let me know if you have other tools that belong here.\n\n## Comprehensive Tools\n\nTools that try to give you the whole kitchen sink including waterfalls\n\n- [PageSpeed Insights](https://developers.google.com/speed/pagespeed/insights/) - The original PageSpeed test from Google, aka Lighthouse run on the web - gives you FCP, FID, TTI, screenshots, opportunities, and Diagnostics. You can run this programmatically for free with Chrome's [Crux API](https://web.dev/chrome-ux-report-api/) \n- [WebPageTest](https://webpagetest.org) - This seems the most highly regarded in the webperf circles I know about. You pick a location and a browser. Gives you 3 runs, shows waterfall, and a bunch of stats with letter grades for compression/TTFB, FCP, Document Complete and Fully Loaded times.\n- [GTMetrix](https://gtmetrix.com/) - Also super highly regarded. Gives you PageSpeed and YSlow recommendations, waterfall, historical graphs, and (for logged in users) filmstrips\n- [Dareboost](https://www.dareboost.com/) - looks simple, but actually *does* have filmstrip and waterfall and recommendations!!\n\n## Open Source, Self Hosted\n\n- [Perfume.js](https://github.com/Zizzamia/perfume.js)\n- [Falco](https://getfal.co/) - Open Source, self-hosted WebPageTest runner. automatically run audits & see the evolution of key performance metrics to easily spot regressions & audit the performance of individual URLs or entire user journeys\n- [Sitespeed](https://www.sitespeed.io/) - also Open Source, self hosted with monitoring :)\n\n## Services with Paid offerings\n\n- [PerfBeacon](https://perfbeacon.com/) - runs Google Lighthouse/Pagespeed insights at a scheduled interval + when triggered by API calls. Price plans from 2.5k - 50k tests per month, 14 day free trial\n- [PageSpeedPlus](https://pagespeedplus.com/) - Google PageSpeed Insights Monitoring, Lighthouse Data - Lab & Field, Web Vitals, URL History, Email & Slack Alerts, Competitor Comparison, Full Site Scans\n- [Calibre](https://calibreapp.com/) - set performance budgets from various worldwide locations, receive regular reports, and get [performance changes in Pull Requests](https://calibreapp.com/pull-request-reviews). Price plans from 5k - 50k tests per month, 15 day free trial\n- [Amazon Cloudwatch Synthetics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Synthetics_Canaries.html)\n- [Simpleops.io](https://simpleops.io/) \n\n## Improvement Recommendations\n\nThese offer improvements but didn't quite make my cut for \"Comprehensive Tools\" - but are still noteworthy, esp Geekflare's security tools.\n\n- [Geekflare](https://gf.dev) - Geekflare's set of 25 tools giving you everything from a Website Performance Audit to TTFB and Traceroute tests and other security related tests. The Audit gives a nice score and a lot of improvement recommendations.\n- [Pingdom Tools](https://tools.pingdom.com/) - you pick a location, it gives you some load time/page size stats, improvement opportunities, and a waterfall with a nice filter\n- [RequestMap from Webperf.Tools](http://requestmap.webperf.tools/) - [Simon Hearnes' request map generator](https://simonhearne.com/2015/find-third-party-assets/) which focuses on visualizing Third Party Assets. Pick a location, and a device.\n- [ReportURI Security Headers](https://securityheaders.com/) - quick and easy way to check if sites have security headers like CSP and HSTS enabled\n- [Mozilla Observatory](https://observatory.mozilla.org/) - teaching developers, system administrators, and security professionals how to configure their sites safely and securely.\n\n## Test Banks\n\nTools which ping your site from multiple locations to give you a global view.\n\n- [Sucuri Performance](https://performance.sucuri.net) - pings your site from 16 locations worldwide and gives you connection and TTFB times.\n- [Website Speed Test from Dotcom Tools](https://dotcom-tools.com/website-speed-test.aspx) - ping 20+ locations and pick a browser, it shows you load times for first visit and repeat visit!\n- [KeyCDN Perf Tools](https://tools.keycdn.com/performance) - pings your site from 14 locations worldwide and gives you DNS/Connect times\n- [Latency.apex.sh](https://latency.apex.sh/) - pings two URLs to compare latency from 13 locations. Good for comparing sites with and without a CDN.\n- [Lighthouse Metrics](https://lighthouse-metrics.com/) - ping Lighthouse from multiple locations. Made by [Chris](https://twitter.com/chriswdmr).\n- [FastOrSlow](https://www.fastorslow.com/) - FastOrSlow does actual browser simulations from around 12 locations around the world. When we first architected it, we wanted to provide as many locations as possible including places like South Africa. That meant in some cases we needed actual bare-metal servers. So we built it on bare metal in data centers around the world. ([Source](https://news.ycombinator.com/item?id=23119701))\n\n## Simple Tools\n\nTools with simplicity as the appeal - great first checks.\n\n- [TestMySite from Netlify](https://testmysite.io/) - Netlify's tool which checks TTFB and HTTPS and gives a score out of 100\n- [TestMySite from Google](https://www.thinkwithgoogle.com/feature/testmysite/) - mobile speed report from Google - lets you generate a report with recommendations for your team\n- [Bytecheck.com](https://www.bytecheck.com) - gives you a simple waterfall breaking down Redirects + DNS + Connect + SSL + Send + Wait + Receive\n- [YellowLab.tools](https://yellowlab.tools/) nice letter grades per section\n- [Pingdom Tools](https://tools.pingdom.com/) - you pick a location, it gives you some load time/page size stats, improvement opportunities, and a waterfall with a nice filter\n- [24x7 Page Analyzer](https://www.site24x7.com/web-page-analyzer.html) - you pick a location, it shows you a waterfall of requests with a PageSpeed score\n- [KeyCDN Speed Tool](https://tools.keycdn.com/speed) - gives you per-asset download breakdowns - a little TOO detailed to be useful\n- [Butterfly](https://getbutterfly.com/) - I havent looked into this too much - Unlimited Automated Page Speed Monitoring & Tracking, Security Tracking & Performance Timing - founder is [Ciprian on Twitter](https://twitter.com/getButterfly)\n\n"
  },
  {
    "slug": "gatsby-remark-essential-plugins",
    "data": {
      "title": "Essential Plugins for Gatsby Remark",
      "description": "Gatsby-Remark is one of those fun plugins that have their own plugins - but there are a lot of them. Here's a list I wrote down a few months ago of plugins I think everyone should use.",
      "tag_list": [
        "react",
        "gatsby"
      ]
    },
    "content": "\n[Gatsby-Remark](https://www.gatsbyjs.org/packages/gatsby-transformer-remark/) is one of those fun plugins that have their own plugins - but there are a lot of them! (Because [Remark](https://remark.js.org/) has a lot of [plugins](https://github.com/remarkjs/remark/blob/master/doc/plugins.md#list-of-plugins))\n\nHere's a list of plugins I think everyone should use, and what they do.\n\n## Bottom Line Up Front\n\nI'd recommend a `gatsby-config.js` that looks like:\n\n```js\nplugins: [\n    {\n      resolve: `gatsby-transformer-remark`,\n      options: {\n        plugins: [\n          'gatsby-remark-autolink-headers',\n          'gatsby-remark-prismjs',\n          'gatsby-remark-copy-linked-file',\n          'gatsby-remark-external-links',\n          'gatsby-remark-images',\n          'gatsby-remark-numbered-footnotes',\n          'gatsby-remark-social-cards',\n          'gatsby-remark-embedder'\n      }\n    }\n  ]\n```\n\n(Note I've omitted all options for these plugin-plugins, but you're probably going to want to specify some options for some of these)\n\n## Plugins\n\n## `gatsby-remark-autolink-headers`\n\n> [Link to docs](https://www.gatsbyjs.org/packages/gatsby-remark-autolink-headers/?=headers)\n>\n> Adds GitHub-style hover links to headers in your markdown files when they’re rendered.\n\nThis one is first because it is SO important to user experience. I link to anchor tags all the time (using the [Display Anchors](https://chrome.google.com/webstore/detail/display-anchors/poahndpaaanbpbeafbkploiobpiiieko?hl=en) browser extension), and it is a pain to try to link to a specific part of a long blog post with a header that *doesn't have an ID* or a handy link for the user to copy! So [remark-autolink-headings](https://github.com/remarkjs/remark-autolink-headings) adds the ID and link tags:\n\n```markdown\n# Lorem ipsum 😪\n## dolor—sit—amet\n### consectetur &amp; adipisicing\n#### elit\n##### elit\n```\n\nto\n\n```html\n<h1 id=\"lorem-ipsum-\"><a href=\"#lorem-ipsum-\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Lorem ipsum 😪</h1>\n<h2 id=\"dolorsitamet\"><a href=\"#dolorsitamet\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>dolor—sit—amet</h2>\n<h3 id=\"consectetur--adipisicing\"><a href=\"#consectetur--adipisicing\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>consectetur &#x26; adipisicing</h3>\n<h4 id=\"elit\"><a href=\"#elit\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>elit</h4>\n<h5 id=\"elit-1\"><a href=\"#elit-1\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>elit</h5>\n```\n\nThe Gatsby version of this plugin also adds some nice hover styling with a nicer link icon, which you can see in [the Kitchen Sink demo](https://using-remark.gatsbyjs.org/hello-world-kitchen-sink/). Note that GitHub works like this by default.\n\nThis quite frankly was a straight up design flaw in Markdown and I flatly refuse to write any Markdown content without these enhancements.\n\n### `gatsby-remark-prismjs`\n\n> [Link to docs](https://www.gatsbyjs.org/packages/gatsby-remark-prismjs/?=gatsby-remark-prismjs)\n>\n> Adds syntax highlighting to code blocks in markdown files using [PrismJS](http://prismjs.com/).\n\nThis one is key for developer blogs. As a developer, you can pry syntax highlighting from my cold, dead, carpal tunnel ridden hands. Please don't make me read your blog without syntax highlighting.\n\nNote, however, that PrismJS highlighting is done clientside, which will add ~19kb to your JS bundle so that you can do *dynamic* highlighting (i.e. if you need your reader to edit code and the highlighting to change accordingly). If you only need *static* highlighting, then you could look into only doing it at build time and sending *no* JS down the wire. [I have used `shiki` from the Vue ecosystem](https://www.swyx.io/writing/svelte-static/), but [gatsby-remark-shiki](https://www.gatsbyjs.org/packages/gatsby-remark-shiki/?=shiki) seems less popular.\n\nHowever, this tradeoff is not free, because the syntax highlighted HTML that gets generated is bulkier, and you lose some very nice features like [line highlighting](https://www.gatsbyjs.org/packages/gatsby-remark-prismjs/#optional-add-line-highlighting-styles), hence I continue to recommend Prism.js.\n\n## `gatsby-remark-copy-linked-file`\n\n> [Link to docs](https://www.gatsbyjs.org/packages/gatsby-remark-copy-linked-files/?=gatsby-remark-copy-linked-file)\n>\n> Copies local files linked to/from Markdown (`.md|.markdown`) files to the root directory (i.e., `public` folder).\n\nThis one is important because it lets you colocate your markdown with other resources, for example static files and images, instead of splitting them up into a \"content\" folder and a \"static\" and a \"images\" folder - resulting in an append-only folder of jumbled content where you don't know what belongs to what.\n\nSo instead of this:\n\n```\n/content\n  /my-blog-post.md\n/static\n  /images\n    /my-blog-post-image1.png\n  /brochure.pdf\n```\n\nYou get:\n\n\n```\n/content\n  /my-blog-post\n    /index.md\n    /my-blog-post-image1.png\n    /brochure.pdf\n```\n\nAnd Gatsby copies files out to the appropriate folder at build time.\n\n## `gatsby-remark-images`\n\n> [Link to docs](https://www.gatsbyjs.org/packages/gatsby-remark-images/?=gatsby-remark-images)\n>\n> Processes images in markdown so they can be used in the production build.\n\nWe all know and love the benefits of [Gatsby Image](https://using-gatsby-image.gatsbyjs.org/). Related to the above, when you reference an image from your markdown, you don't just want to load a simple image, you want to run it through [Gatsby Sharp](https://www.gatsbyjs.org/packages/gatsby-plugin-sharp/?=gatsby%20sharp) image processing to take advantage of the blur-up performance benefits.\n\n## `gatsby-remark-external-links`\n\n> [Link to Docs](https://www.gatsbyjs.org/packages/gatsby-remark-external-links/?=gatsby-remark-external-links)\n>\n> Adds the target and rel attributes to external links in markdown.\n\nThis one is pretty simple - by default, Markdown links just translate to `<a href=\"https://mylink.com\">` links which cause people to navigate off your site. For some people this is desired behavior, but if you want Remark to automatically add `target=\"_blank\"` and `rel=\"nofollow noopener noreferrer\"` ([for security](https://developers.google.com/web/tools/lighthouse/audits/noopener)), then this plugin does that for you.\n\n\n## `gatsby-remark-numbered-footnotes`\n\n> [Link to docs](https://www.gatsbyjs.org/packages/gatsby-remark-numbered-footnotes/?=gatsby-remark-numbered-footnotes)\n>\n> This is a plugin for `gatsby-transformer-remark` that converts footnote reference links to sequential numbers.\n\nFootnotes are great! They let you add extra context without cluttering your message. You can write footnotes in Markdown like so:\n\n```markdown\nThis is normal body copy.[^also] It includes a couple footnotes.[^thing]\n\n[^also]:\n  This is a footnote.\n\n[^thing]:\n  This is another footnote.\n```\n\nAnd it looks like this (note I don't have this setup on my personal site yet):\n\n\nThis is normal body copy.[^also] It includes a couple footnotes.[^thing]\n\n[^also]:\n  This is a footnote.\n\n[^thing]:\n  This is another footnote.\n\nPretty nice to read!\n\n## `gatsby-remark-social-cards`\n\n> [Link to Docs](https://www.gatsbyjs.org/packages/gatsby-remark-social-cards/?=gatsby-remark-social-cards)\n>\n> `gatsby-remark-social-cards` iterates over your markdown files and automatically generates graphical representations of the frontmatter data! It’s highly customizable and can help increase your click rates.\n\nAs I blogged recently, [OG Images are your site's calling card](https://www.swyx.io/writing/jamstack-og-images/). Plain and simple, people read your social cards way more than your post content, so it ought to be appealing and informative instead of repetitive. \n\n![https://i.imgur.com/VByhlyE.jpg](https://i.imgur.com/VByhlyE.jpg)\n\nThis plugin is well tested and has every feature you could want to transform Markdown frontmatter to your social unfurl card of choice.\n\n## `gatsby-remark-embedder`\n\n> [Link to Docs](https://www.gatsbyjs.org/packages/gatsby-remark-embedder/?=gatsby-remark-embedder)\n>\n> Gatsby Remark plugin to embed well known services by their URL.\n\nI'll just let them explain:\n\n> Trying to embed well known services (like CodePen, CodeSandbox, GIPHY, Instagram, Lichess, Pinterest, Slides, SoundCloud, Spotify, Streamable, Twitter or YouTube) into your Gatsby website can be hard, since you have to know how this needs to be done for all of these different services.\n> \n> `gatsby-remark-embedder` tries to solve this problem for you by letting you just copy-paste the link to the pen/player/sandbox/tweet/video you want to embed right from within your browser onto a separate line (surrounded by empty lines) and replace it with the proper embed-code.\n\nIt's been a pleasure watching this plugin grow - the maintainer [Michael](https://github.com/MichaelDeBoey) is pretty diligent about adding more and more content types like SoundCloud and CodePen. These are simple components that we should not have to rewrite every time, and help make our blogposts a lot more interactive so that people don't have to leave our site to enjoy non-simple-text content.\n\n## Conclusion\n\nYou can create really great reading experiences with these plugins, and get a lot of mileage out of remark. They seem like relatively conservative choices which, should you have to move on from Gatsby or Remark in future, you could adapt and make work again without heavy rewriting of content. This is the promise of Markdown, after all.\n\nI do wish more of these were framework agnostic, because all this work going into `gatsby-remark` plugins could have just been [`remark` plugins](https://github.com/remarkjs/remark/blob/master/doc/plugins.md#list-of-plugins) and therefore usable by others. But of course there are some Gatsby specific concerns and opportunities that these plugins can take advantage of. But I worry that the community is unneccesarily splintered as a result.\n\nWhat other Gatsby Remark plugins do you particularly like? Let me know in replies/comments!"
  },
  {
    "slug": "formats",
    "data": {
      "title": "Formats over Functions",
      "description": "Why we should focus less on Implementation and more on Standardization",
      "tag_list": [
        "tech"
      ]
    },
    "content": "\n## The Thesis\n\nWhen we focus our efforts on Formats over Functions, we focus less on **implementation** and more on **standardization**. **This is the key to longer lasting code.**\n\n## Why\n\n\"Functions\" are selfish - they assume all code is written for them and them alone, and as a result the API bleeds all over the place. \"Formats\" are more generous - they only assume conventions - and the library/framework code lives *outside* the file so that other libraries/frameworks can write to it as well.\n\nWhen we write imperative code in Functions, there are a lot of ways we can shoot ourselves in the foot, and it tends to be more verbose. This is why our code tends towards more declarative over time as we better understand our problem space. When we are confident we understand the problem well, we should move this to Formats, which are the ultimate declarative form with no implementation detail involved.\n\nFormats deal mainly in simple values, whereas functions have to be evaluated. Here I am very inspired by Rich Hickey's [Value of Values](https://www.youtube.com/watch?v=-6BsiVyC1kM) talk. The idea is that values have a transitive property - every nested property of a value is itself a value - therefore it is completely static and reliable and can be statically extracted without evaluation, or copied and modified without side effects.\n\n## Examples\n\nHere are examples of successful Formats over Functions:\n\n- `package.json`: Multiple tools can read package.json without getting in each others' way. There is no spec for it though, which is occasionally a problem, but there are [attempts](https://github.com/pkg-json/package-json) and unofficial [guides](https://flaviocopes.com/package-json/).\n- Storybook's [Component Story Format](https://storybook.js.org/docs/formats/component-story-format/) frees Stories from the prison of Storybook's proprietary functions, enabling it to be consumed by design tools and other tooling.\n- [xState](https://xstate.js.org/docs/) moves state management out of proprietary library/framework functions into the tried-and-tested [SCXML specification](https://www.w3.org/TR/scxml/).\n- [Infrastructure as Code](https://en.wikipedia.org/wiki/Infrastructure_as_code) replaces human functions in the loop ([ClickOps](https://m.subbu.org/clickops-3cf0e5bc5ecf)) with code formats - at first proprietary per cloud, and then [agnostic across clouds](https://serverless.com/) (debatable if multicloud IaC is valuable)\n- The success of GraphQL: When [Lee Byron and co open sourced GraphQL](https://www.youtube.com/watch?v=783ccP__No8), they didn't open source the FQL/Ent implementation that was used in Facebook. That wouldn't have been very useful to others who don't use XHP/PHP. They *rewrote it from scratch* and chose to solidify a GraphQL spec first, and then write a JS *reference implementation*. This choice helped it stay agnostic of implementation details, and allowed both server and clientside implementations of it to proceed independently, leading to the exponential success of GraphQL as a protocol.\n- [libc (thanks b0rk)](https://twitter.com/b0rk/status/1217890878674624513?s=20) is another example of a command standard/interface. Because everyone speaks libc but doesn't have to use the same libc implementation, you can use DNS and it works the same in every language and platform.\n- CPU [Instruction Set Architectures](https://en.wikipedia.org/wiki/Instruction_set_architecture) - any two CPU's that implement the same ISAs will be able to run the same code, regardless of their underlying implementation.\n- Email. Email works everywhere regardless of implementation detail.\n- [Wikipedia has more standards](https://en.wikipedia.org/wiki/List_of_international_common_standards#Reference_for_information): in particular, [BGP](https://www.swyx.io/writing/networking-essentials-routing-5gb7/), Barcodes, ISBNs, QR codes, Uniform resource locators, [vCards](https://en.wikipedia.org/wiki/VCard)\n\nMore examples of ongoing movement:\n\n- [React Components -> Redwood Cells](https://www.swyx.io/writing/react-sfcs-here)\n- Blockchain development moving from Protocols > Platforms https://medium.com/@lightcoin/from-platforms-to-protocols-c5fe0bdd0fc7\n- [Twitter is looking at becoming a decentralized format](https://www.theverge.com/2019/12/11/21010856/twitter-jack-dorsey-bluesky-decentralized-social-network-research-moderation) (actual success to be seen)\n\n## Epistemic Disclosures\n\n- **Epistemic status**: This is a general idea I've increasingly been referring to as a programming principle/design pattern - but I don't pretend to have good definitions and examples locked down yet. But I'm pretty sure the thesis is right - I just have to scope it down to the [boldest correct form of the argument](http://paulgraham.com/useful.html).\n- **Epistemic effort**: I've had [a stub post](https://twitter.com/swyx/status/1210298953276645376) on this since October but I figure I better flesh it out a bit - but be warned I know it is a little vague still."
  },
  {
    "slug": "js-20-years",
    "data": {
      "title": "JavaScript: the First 20 Years by Allen Wirfs-Brock and Brendan Eich",
      "description": "A link to the 190 page history of JS by its original creator and the editor of ES6.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nAllen Wirfs-Brock (editor of ES6) and Brendan Eich (you know what he did) have published [the definitive 20 year history of JavaScript](https://zenodo.org/record/3707008). It is a 190 page treatment for [The History of Programming Languages Conference](https://hopl4.sigplan.org/), which only occur once every 13-15 years (1978, 1993, 2007, 2020). [It took them 3 years to write the final manuscript.](http://www.wirfs-brock.com/allen/posts/866)\n\nRead it: https://zenodo.org/record/3707008\n\n**If you have any interest in programming languages at all, you should read it.** Modern JavaScript, warts and all, is one of the biggest achievements of humanity, especially in the Information Age and the democratization of computing.\n\nHistory won't stop here - HOPL-V in ~2035 might be more about visual programming languages and \"[no code](https://webflow.com/blog/no-code-is-a-lie)\" than it is traditional text editing. Every time programming gets easier, people get more infuriated, and yet the world keeps turning toward enabling more programmers. [The Future of Programming](https://www.youtube.com/watch?v=8pTEmbeENF4) is clear, even if we're never quite there yet.\n\nThat's it, that's pretty much all I want to blog today. I did write 1700 words on another blogpost, but that's currently in review."
  },
  {
    "slug": "lampshading",
    "data": {
      "title": "The Power of Lampshading",
      "description": "How to turn Ignorance into Power",
      "tag_list": [
        "advice"
      ]
    },
    "content": "\nWe are often told that **Knowledge is Power**. This is mostly true - except for at least two points in your career.\n\nHave you thought about how **Ignorance can be Power** too? I can think of at least two stages in a career when you can wield lack of knowledge as a form of power (in the neutral, *ability to influence others to do what you need* sense, not in the petty *dominating over others* sense). \n\nAnd we'll talk about how you can wield ignorance throughout the rest of your career too - with **Lampshading**!\n\n## When you're very senior\n\nFirst, when you're in **senior management**, typically at least a couple layers removed from individual contributors. Beyond a certain level you are not being paid to have the right answers - that's what your reports are for. It's your job to ask the right *questions*, and to enable your team to figure out how to get the answers. \n\nIn my career so far I've often noticed that it is *senior management*, not middle or junior people, that are most likely to say \"Whoa, whoa, whoa. I don't understand what's going on here. [Can you explain like I'm five?](https://www.dictionary.com/e/slang/eli5/)\" Done right, it can expose weak reasoning and bust bullshit, particularly when framed with the (mostly correct) belief that [\"If you can't explain it simply, you don't understand it well enough.\"](https://www.passiton.com/inspirational-quotes/3363-if-you-cant-explain-it-simply-you-dont).\n\nNote I'm not absolving incompetent management of the need to know domain knowledge necessary to be an effective leader. I'm simply observing that at senior levels you are *not* expected to know everything, and that's an interesting violation of \"Knowledge is Power\" you have probably experienced.\n\n## When you're new\n\nSecond, **when you're new**, typically entry level in a career or a new joiner to a community or company. At this level, again, nobody expects you to know *anything*. Sure, you needed to know *enough* to fool someone into hiring you. But so long as you never lied or lied-by-omission, nobody is going to turn around and fire you for having holes in your knowledge.\n\nOf course, there are cases where this doesn't apply. Junior talent are the most expendable, and some companies don't have a healthy attitude to mentorship and hiring. But in general, I find the tech industry a lot better for mentoring than, say, finance. Tech companies generally place explicit responsibility on seniors/team leads to mentor juniors, especially as part of their career progression goals.\n\nYou might imagine, having restarted my career 2-4 times depending how you count it, that I have a good deal of personal experience with being a total newbie. \n\n## Storytime!\n\nHere I can tell you about my first day on my new dev job, fresh out of bootcamp.\n\nMy team all joined on the same day - 3 new hires (me and 2 more experienced devs). My new boss, a confident senior dev who had had a long tenure with the firm, was walking us through our tech stack. All of a sudden he paused, and said, \"oh by the way, we're going to use TypeScript. You all know TypeScript, right?\". Coworker 1 nodded, Coworker 2 nodded. There was that unspoken sense of *duh, we're all professionals here, of course we use TypeScript*. \n\nAnd then all eyes were on me.\n\nI don't do well with peer pressure. In Gretchen Rubin's [4 Tendencies](https://gretchenrubin.com/2015/01/ta-da-the-launch-of-my-quiz-on-the-four-tendencies-learn-about-yourself/) model, I'm an Obliger - I like to please people and put my own concerns aside. Of course my bootcamp hadn't taught TypeScript, we'd only had 3 months to learn fullstack JS! And of course I wanted to say yes!\n\nI had a probably visible moment of panic, before going with \"no I don't know TypeScript.\" My boss simply nodded, saying, \"you can learn on the job\", and moved on.\n\nI think in my first few months I probably had a dozen little tests like that. Did I know how to do professional code reviews? (No) Did I know how to do BEM naming? (No, and I proudly still don't) Did I know what Clean Code was? (eh.. nope).\n\nEvery time I confessed ignorance, they gave me what I needed to learn, and I caught up. If I made a mistake, they taught me what I did wrong. What were they gonna do, fire me? They knew what they were doing when they hired me out of bootcamp.\n\n## Lampshading\n\nSo we see that confessing ignorance works at both the senior and junior stages of careers. But it also works in isolated situations as well, for example when you caveat what you don't know while [learning in public](https://www.swyx.io/writing/learn-in-public).\n\nGiven my casual interest in creative writing, I often compare this technique with [Lampshading](https://tvtropes.org/pmwiki/pmwiki.php/Main/LampshadeHanging). To quote from TV Tropes:\n\n> Lampshade Hanging (or, more informally, **Lampshading**) is the writers' trick of dealing with any element of the story that threatens the audience's Willing Suspension of Disbelief, whether a very implausible plot development, or a particularly blatant use of a trope, **by calling attention to it and simply moving on**.\n\nApplied to real life: You call out your own weakness, so that others can't. \n\nIn fact, by most functional team dynamics, others are then obliged to help you fix your weakness. This is [soft power](https://en.wikipedia.org/wiki/Soft_power).\n\nMany Americans (of a certain age) immediately sympathize with this by linking it to [the final battle in 8 Mile](https://www.youtube.com/watch?v=sHE0wmgljco). In it, Eminem kicks off by naming every single weakness of his that his opponent Papa Doc was going to, literally stealing all the words from Papa Doc's mouth and turning himself into a sympathetic character. **Weakness is strength** here purely because of lampshading.\n\n## The Stupid Question Safe Harbor\n\nIn real life, I often lampshade by invoking the \"Stupid Question Safe Harbor\" (SQSH).\n\nA \"[Safe Harbor](https://en.wikipedia.org/wiki/Safe_harbor_(law))\" is a legal idea that explicitly okays some behavior that may be in a grey zone due to unclear rules. So I use it as an analogy for how we act when someone says \"I have a Stupid Question\". \n\nWhen we invoke the \"Stupid Question Safe Harbor\", we are acknowledging the question is potentially stupid, AND that we all know that there's not really such a thing as a stupid question, but we'll just get it out of the way to ask something really basic - because getting mutual understanding is more important than saving face.\n\nThe trick here is you actually are saving face - now you've invoked the SQSH, people understand you’re roleplaying, you're explicitly invoking a well understood mode of conversation, and you're not *ACTUALLY* that stupid. Right? Right?? *nervous laughter*\n\nWhen you are in a group scenario, the SQSH has positive externalities. There may be multiple people wondering the same thing, but only one person has to \"take the hit\" of asking the \"stupid question\", and yet all benefit. I like performing this role of [Stupid Questions as a Service](https://twitter.com/swyx/status/1096645037788618752).\n\n## Advanced Lampshading\n\nOnce you learn to look out for **Lampshading**, you may see powerful users of it out there in the wild who use it to Learn in Public:\n\n- Kyle Simpson famously was told \"You Don't Know JS\" in an interview, and turned that into his [primary claim to fame](https://github.com/getify/You-Dont-Know-JS), controversial title and all.\n- I eventually took my own TypeScript learnings, explicitly lampshaded that I was learning, and put them online and that became the [React TypeScript Cheatsheets](https://github.com/typescript-cheatsheets/react-typescript-cheatsheet/)\n- I *frequently* lampshade my mistakes during my talks. Clicker not working? Call attention to it. Joke didn't land? Call myself out. Self aware, self deprecating humor is always appreciated by the audience, and fills dead air. But you can also use it to set up [a Pledge, in advance of a Turn and the final Prestige](https://www.goodreads.com/quotes/91029-every-great-magic-trick-consists-of-three-parts-or-acts), which is [exactly how I set up my own livecoding talks](https://www.youtube.com/watch?v=KJP1E-Y-xyo).\n- [This woman lampshading to ward off all trolls](https://twitter.com/Nicolemens/status/1229610008167383040?s=20)\n- *who else lampshades very well? let me know*"
  },
  {
    "slug": "testing-plugin-authors",
    "data": {
      "title": "Unit and Integration Testing for Plugin Authors",
      "description": "Some thoughts on how to set up testing with plugins",
      "tag_list": [
        "testing",
        "node"
      ]
    },
    "content": "\nI've just completed work on [Netlify-Plugin-No-More-404](https://github.com/sw-yx/netlify-plugin-no-more-404) - a [Netlify Build](https://github.com/netlify/build) plugin to guarantee you preserve your own internal URL structure between builds. But I'm not here to plug my plugin or Netlify - I just think I had a small realization on plugin testing strategy which I would like to share with you.\n\nMost projects want to be platforms, and most platforms want to have plugins to extend functionality and eventually create mutually beneficial business relationships. Gatsby has plugins, Next.js has plugins, Shopify has plugins, Wordpress has plugins, everybody gets a plugin! If you're successful enough [even your plugins have plugins](https://using-remark.gatsbyjs.org/)! Figma has [written some great stuff about the engineering challenges behind plugins](https://www.figma.com/blog/how-we-built-the-figma-plugin-system/) - not least of which is API design, permissions, and security, and I'd highly recommend their writing on this. I have a future blogpost that I hope to do on \"how to do plugin systems right\", because [all plugins system suck](https://twitter.com/acdlite/status/1073401531070767104?s=20) in some way.\n\nThe scope of this blogpost is much smaller than that - it's just about setting up testing as a plugin author. I think plugin authors should set up:\n\n- **unit tests around their business logic**\n- **integration tests around their plugin interface**\n\n## First, a talk on Boundaries\n\n[Gary Bernhardt's Boundaries talk](https://www.destroyallsoftware.com/talks/boundaries) is really influential to my thinking. As it says on the tin: \n\n> This talk is about using simple values (as opposed to complex objects) not just for holding data, but also as the boundaries between components and subsystems. \n\nA plugin is a component connecting to a subsystem. Once we think about it this way, it greatly clarifies both the code as well as how to test it. You don't need to watch the talk to understand the rest of this post, but I highly recommend it anyway.\n\n## A mental model for plugin authoring\n\nYou can view the relationship of a plugin and its core as some overlapping boxes:\n\n![image](https://user-images.githubusercontent.com/6764957/76472148-14ff2d80-63cb-11ea-913a-49bb47482fe1.png)\n\nSeems simple enough. You can then break it down into business logic and plugin interface:\n\n\n![image](https://user-images.githubusercontent.com/6764957/76473579-4548cb00-63cf-11ea-8e6f-0f87c4f30c01.png)\n\n\nNote that by Business logic, I mean everything that the core has no knowledge of - something domain specific to what your plugin is trying to do. \n\nBy plugin interface, I mean everything imposed on you by the core system: all the settings, utilities, and lifecycles specified by them - and therefore you're writing glue code between your business logic and how the plugin API wants you to expose your work.\n\n**The core proposal of this blogpost** is that you should first write your business logic via unit tests (fast tests with simple values, ideally with no I/O), and then test your plugin interface code by writing integration tests (slower tests, mocking APIs where needed, with I/O). \n\nMost people will think of Martin Fowler's [Test Pyramid](https://martinfowler.com/bliki/TestPyramid.html) or Kent C Dodds' [Testing Trophy](https://twitter.com/kentcdodds/status/960723172591992832): \n\n![https://miro.medium.com/max/1600/0*KVKUJFHMJoxOHM0g](https://miro.medium.com/max/1600/0*KVKUJFHMJoxOHM0g)\n\nBut those are generalized testing philosophies. I think for plugin systems, you can let the core system be responsible for end-to-end success, and you get the most bang for your buck with unit and integration tests.\n\nIf that sounds obvious, I can say that as a plugin author I didn't really think about it while diving in headfirst, and I paid the price in rewrites today.\n\n## Testing the business logic\n\nI think the key here is to design your business logic code as a single function or module with [as small an API surface area as possible](https://www.youtube.com/watch?v=4anAwXYqLG8) for you to get the job done. If your function takes 5 parameters but could take 3 instead if you derive the final 2, then take 3. [I'm a fan of argument objects, by the way](https://twitter.com/swyx/status/1198632709834326021).\n\n![image](https://user-images.githubusercontent.com/6764957/76474812-36641780-63d3-11ea-827c-93d6dae81a24.png)\n\nIdeally your business logic doesn't really care about what the core system's plugin API looks like, although of course if there are special requirements for idempotence or side effects those concerns will leak through down to how you write your business logic. But ultimately you want to stay as agnostic of plugin API as possible. This serves two benefits: \n\n- it is easier to test, since you will be passing in simple values, and \n- it is also easier to copy your logic over to other plugin systems, which you *will* be doing!\n\nBecause unit tests are meant to be light and deterministic, you should create as many variations of them as to form a [minimum spanning tree](https://en.wikipedia.org/wiki/Minimum_spanning_tree) of what your users could realistically give your code.\n\n## Testing the plugin interface\n\nNow that you are happy with your business logic, you can write your integration with the plugin API with high confidence that any errors are due to some mistake with the API itself, not anything to do with the business logic.\n\n![image](https://user-images.githubusercontent.com/6764957/76474931-aa9ebb00-63d3-11ea-9c06-372794d7d281.png)\n\nI don't have a lot of wisdom here - you will be mocking your system's provided core APIs (if you're lucky, they will provide well documented local testing utilities for you, but its also not a heavy lift to write your own as you learn about what the APIs do), and you will have to set up and tear down any files on the filesystem for these effectful integration tests.\n\nI find myself writing less of these integration tests, since I already did the test-all-variations stuff at the unit test level. At the plugin interface level, I merely need to test that I'm relaying the right information to the business logic properly. \n\nI also set these things up as \"fixtures\" rather than solid tests - which to me means that it is a test I can quickly manually futz around to reproduce or investigate user reported bugs.\n\n## Secret Developer Flags\n\nI also find myself adding two secret developer-experience-focused boolean flags to my business logic, both defaulting to `false`:\n\n- `testMode`: Inside business logic, plugins should surface helpful warnings and logs and errors to the user; however this can be a little annoying when running tests, so your unit tests can pass `testMode: true` to silence those logs.\n  - Of course, this isn't perfect - you should also be testing for regressions against expected warnings and errors *not* showing up - but my project was not ready for that level of sophistication yet.\n- `debugMode`: When the plugin is shipped and run live inside the production system, it will still have bugs due to APIs not behaving as you expected. So adding a `debugMode` flag helps you log out diagnostic information helpful to tell you, the plugin developer, how the real life system differs from your locally tested code. Additionally, if the plugin user is reporting issues, you can also easily tell them to turn on `debugMode` and send over the resulting logs to help you figure out what they have going wrong. \n  - Of course, it doesn't just have to be a boolean flag - you can [use log levels and match against feature strings](https://www.swyx.io/writing/js-tools-metrics-logs-traces/) for more complex systems - but a boolean keeps things simple.\n\n## Other Tips\n\nI like using colocated READMEs in each folder to document what tests should do. The markdown format syntax highlights nicely and it shows up on GitHub. Just a personal preference.\n\nany other tips? reply and I'll write them here with acknowledgement!\n\n## Go Slow to Go Far\n\nA final word on the value of testing for plugin developers. \n\n- When I first started doing plugins I (of course) didn't write any tests - I think the cool kids now say they \"test in production\" now. This is fine - until you start to rack up regressions when you try to fix one thing and something else breaks. \n- Additionally, most of the time this won't be your main job, so you will only infrequently visit this codebase and the context switch will be annoying to the point of discouraging further development. \n- What helps future you also helps other plugin developers, if you are working in a team or open source.\n- And when you eventually need to refactor - to swap out underlying engines, or to add new features or redesign internals for scale, the extra sprint effort due to lack of tests may discourage refactors and so cap the useful life of your plugin.\n\nI kind of visualize it like this in my head:\n\n![image](https://user-images.githubusercontent.com/6764957/76457985-24bd4880-63b0-11ea-94eb-805b5e1861b4.png)\n\nTests hold the line, and that's a powerful thing for sustained progress over your code's (hopefully long) life."
  },
  {
    "slug": "react-sfcs-here",
    "data": {
      "title": "React Single File Components Are Here",
      "description": "React has long eschewed convention in favor of the extreme flexibility of JS. It is time for the next level in React authorship formats.",
      "tag_list": [
        "react"
      ]
    },
    "content": "\nThe [launch of RedwoodJS](https://news.ycombinator.com/item?id=22537944) today marks a first: **it is the first time React components are being expressed in a single file format with explicit conventions**.\n\nI first talked about this in relation to [my post on React Distros](https://www.swyx.io/writing/react-distros/), but I think React SFCs will reach standardization in the near future and it is worth discussing now as a standalone post.\n\n## Context\n\nIt is a common joke that you can't get through a React conference without reference to React as making your view layer a pure function of data, `v = f(d)`. I've [talked before about the problems with this](https://www.swyx.io/speaking/react-not-reactive/), but basically React has always done a collective `¯\\_(ツ)_/¯` when it comes to having a satisfying solution for actually fetching that all-important data. [React Suspense for Data Fetching](http://reactjs.org/concurrent) may be a nice solution for this in the near future.\n\nHere's a typical React \"single file component\" with [Apollo Client](https://www.apollographql.com/docs/react/why-apollo/), though it is the same with React Query or any data fetching paradigm I can imagine:\n\n```js\n// Data Example 1\nexport const QUERY = gql`\n  query {\n    posts {\n      id\n      title\n      body\n      createdAt\n    }\n  }\n`;\n\nexport default function MyComponent() {\n  const { loading, error, data: posts } = useQuery(QUERY);\n  if (error) return <div>Error loading posts: {error.message}</div>\n  if (loading) return <div>Loading...</div>;\n  if (!posts.length) return <div>No posts yet!</div>;\n\n  return (<>\n    posts.map(post => (\n      <article>\n        <h2>{post.title}</h2>\n        <div>{post.body}</div>\n      </article>\n    ));\n  </>)\n}\n```\n\nFor styling, we might use anything from Tailwind to Styled-JSX to Styled-Components/Emotion to Linaria/Astroturf to CSS Modules/PostCSS/SASS/etc and it is a confusing exhausting random eclectic mix of stuff that makes many experts happy and many beginners lost. But we'll talk styling later.\n\n## Redwood Cells\n\nHere's what [Redwood Cells](https://redwoodjs.com/tutorial/cells) look like:\n\n```js\n// Data Example 2\nexport const QUERY = gql`\n  query {\n    posts {\n      id\n      title\n      body\n      createdAt\n    }\n  }\n`;\n\nexport const Loading = () => <div>Loading...</div>;\n\nexport const Empty = () => <div>No posts yet!</div>;\n\nexport const Failure = ({ error }) => <div>Error loading posts: {error.message}</div>;\n\nexport const Success = ({ posts }) => {\n  return posts.map(post => (\n    <article>\n      <h2>{post.title}</h2>\n      <div>{post.body}</div>\n    </article>\n  ));\n};\n```\n\nThis does the same thing as the \"SFC\" example above, except instead of writing a bunch of `if` statements, we are baking in some conventions to make things more declarative. Notice in both examples are already breaking out the GraphQL queries, so that they can be statically consumed by the [Relay Compiler](https://github.com/facebook/relay/tree/master/packages/relay-compiler), for example, for persisted queries.\n\nThis is a format that is more native to React's paradigms than the Single File Component formats of [Vue](https://vuejs.org/v2/guide/single-file-components.html), [Svelte](https://dev.to/vannsl/all-you-need-to-know-to-start-writing-svelte-single-file-components-cbd), and my own [React SFC proposal](https://github.com/sw-yx/react-sfc-proposal) a year ago, and I like it a lot. \n\nNotice that, unlike the HTML-inspired formats of the above options, we don't actually lose the ability to declare smaller utility components within the same file, since we can just use them inline without exporting them. This has been a major objection of React devs for SFCs in the past.\n\n## Why Formats over Functions are better\n\nAs you can see, the authoring experience between the Example 1 and Example 2 is rather nuanced, and to articulate this better, I have started calling this idea [Formats over Functions](https://www.swyx.io/writing/formats).\n\nThe idea is that you don't actually need to evaluate the entire component's code and mock the data in order to access one little thing. In this way [you bet on JS](https://twitter.com/alwaysbetonjs?lang=en) more than you bet on React itself.\n\nThis makes your components more consumable and statically analyzable by different toolchains, for example, by Storybook. \n\n## Component Story Format\n\n[Team Storybook](https://storybook.js.org) was actually first to this idea and a major inspiration for me, with it's [Component Story Format](https://storybook.js.org/docs/formats/component-story-format/). Here's how stories used to be written:\n\n```js\n// Storybook Example 1\nimport React from 'react';\nimport { storiesOf } from '@storybook/react';\nimport { Button } from '@storybook/react/demo';\n\nstoriesOf('Button', module)\n .addWithJSX(\n   'with Text',\n   () => <Button>Hello Button</Button>\n )\n .add('with Emoji', () => (\n  <Button>\n    <span role=\"img\" aria-label=\"so cool\">\n      😀 😎 👍 💯\n    </span>\n  </Button>\n ));\n```\n\nYou needed to have Storybook installed to make use of this, and if you ever needed to migrate off Storybook to a competitor, or to reuse these components for tests (I have been in this exact scenario), you were kind of screwed.\n\nComponent Story Format (CSF) lets you write your components like this:\n\n```js\n// Storybook Example 2\nimport React from 'react';\nimport { Button } from '@storybook/react/demo';\n\nexport default { title: 'Button' };\n\nexport const withText = () => <Button>Hello Button</Button>;\n\nexport const withEmoji = () => (\n  <Button>\n    <span role=\"img\" aria-label=\"so cool\">\n      😀 😎 👍 💯\n    </span>\n  </Button>\n);\n```\n\nand all of a sudden you can consume these files in a lot more different ways by different toolchains (including by design tools!) and none of them have to use Storybook's code, because all they need to know is the spec of the format and how to parse JavaScript. *(yes, JSX compiles to React.createElement, but that is easily mockable)*.\n\n## Merging CSF and SFCs\n\nYou're probably already seeing the similarities - why are we authoring stories and components separately? Let's just stick them together?\n\nYou already can:\n\n```js\n// CSF + SFC Example\nexport default { \n  title: 'PostList',\n  excludeStories: ['QUERY']\n};\n\nexport const QUERY = gql`\n  query {\n    posts {\n      id\n      title\n      body\n      createdAt\n    }\n  }\n`;\n\nexport const Loading = () => <div>Loading...</div>;\n\nexport const Empty = () => <div>No posts yet!</div>;\n\nexport const Failure = ({ error }) => <div>Error loading posts: {error.message}</div>;\n\nexport const Success = ({ posts }) => {\n  return posts.map(post => (\n    <article>\n      <h2>{post.title}</h2>\n      <div>{post.body}</div>\n    </article>\n  ));\n};\n```\n\nAdding TypeScript would take no change in tooling at all.\n\nAnd therein lies the beauty of the format, and the impending necessity of standardizing exports for fear of stepping on each others' toes as we push forward React developer experience.\n\n## The Full Potential of Single File Components\n\nI think Styling is the last major frontier we need to integrate. Utility CSS approaches aside, here's how we can include static scoped styles for our components:\n\n```js\n// Styled SFC - Static Example\nexport const STYLE = `\n  /* only applies to this component */\n  h2 {\n    color: red\n  }\n`\nexport const Success = ({ posts }) => {\n  return posts.map(post => (\n    <article>\n      <h2>{post.title}</h2>\n      <div>{post.body}</div>\n    </article>\n  ));\n};\n\n// etc...\n```\n\nand, if we needed dynamic styles, the upgrade path would be fairly simple:\n\n\n```js\n// Styled SFC - Dynamic Example\nexport const STYLE = props => `\n  h2 {\n    color: ${props.color} // dynamic!\n  }\n`\n// etc...\n```\n\nAnd that would upgrade to a CSS-in-JS equivalent implementation.\n\n## Interacting with Hooks\n\nWhat if styles or other future Single File Component segments need to interact with component state? We could lift hooks up to the module level:\n\n```js\n// Hooks SFC Example\nconst [toggle, setToggle] = useState(false)\n\nexport const STYLE = `\n  h2 {\n    color: ${toggle ? \"red\" : \"blue\"}\n  }\n`\nexport const Success = ({ posts }) => {\n  return posts.map(post => (\n    <article>\n      <h2 onClick={() => setToggle(!toggle)}>{post.title}</h2>\n      {toggle && <div>{post.body}</div>}\n    </article>\n  ));\n};\n\n```\n\nThis of course changes the degree of reliance on the React runtime that we assume in SFCs, so I am less confident about this idea, but I do still think it would be useful. I have other, more extreme ideas on this front.\n\n## Other Opportunities\n\n[Dan Abramov replied](https://twitter.com/dan_abramov/status/1238810260082745344) with something I missed - the **server/client** split. There is ongoing work with React Flight (to do with streaming SSR) and Blocks (to do with  [blocking rendering without being tied to Relay/GraphQL](https://twitter.com/addyosmani/status/1238877892618018816)) that I'm basically completely ignorant about.\n\nWhile Redwood uses exports to declare loading and error states, Suspense uses `<Suspense>` and [error boundaries](https://reactjs.org/docs/error-boundaries.html). It's possible to compile from the former to the latter but not the other way, which is a key point of the \"formats over functions\" idea - things are more consumable that way. However it is a valid question *whether* you should be able to access those internal states - after all, if you're just reading them for testing purposes, should you be testing how React works? Counterpoint: what if you wanted to see your loading and error states separately in a Storybook or design tool?\n\nIt also brings to mind the work that Next.js has done with `getStaticProps`, `getStaticPaths` and `getServerSideProps` - as the first hybrid framework, it is nice to use static exports to let the framework pick from data requirements, as well as to not tie yourself so tightly to GraphQL. `getStaticPaths` in particular [is very elegant](https://twitter.com/swyx/status/1199150462622003206) - moving page creation inside components themselves.\n\n## Conclusion - Ending with Why\n\nIt's reasonable to question why we want everything-in-one file rather than everything-in-a-folder. But in a sense, SFCs simply centralize what we already do with loaders. \n\nThink about it: we often operate like this:\n\n```\n/components/myComponent/Component.tsx\n/components/myComponent/Component.scss\n/components/myComponent/Component.graphql\n/components/myComponent/Component.stories.js\n/components/myComponent/Component.test.js\n```\n\nAnd some people may think that is better than this:\n\n```\n/components/myComponent/Component.tsx\n/styles/Component.scss\n/graphql/Component.graphql\n/stories/myComponent/Component.stories.js\n/tests/myComponent/Component.test.js\n```\n\nBut we're exchanging that for:\n\n```js\nexport default = // ... metadata\nexport const STYLE = // ...\nexport const QUERY = // ...\nexport const Success = // ...\nexport const Stories = // ...\nexport const Test = // ...\n```\n\nI find that the file length is mitigated by having keyboard shortcuts for folding/expanding code in IDE's. In VSCode, you can fold/unfold code with [keyboard bindings](https://code.visualstudio.com/docs/getstarted/keybindings#_basic-editing):\n\n**Fold** folds the innermost uncollapsed region at the cursor:\n\n- `Ctrl + Shift + [` on Windows and Linux\n- `⌥ + ⌘ + [` on macOS\n\n**Unfold** unfolds the collapsed region at the cursor:\n\n- `Ctrl + Shift + ]` on Windows and Linux\n- `⌥ + ⌘ + ]` on macOS\n\n**Fold All** folds all regions in the editor:\n\n- `Ctrl + (K => 0)` (zero) on Windows and Linux\n- `⌘ + (K => 0)` (zero) on macOS\n\n**Unfold All** unfolds all regions in the editor:\n\n- `Ctrl + (K => J)` on Windows and Linux\n- `⌘ + (K => J)` on macOS\n\nUltimately, Colocating concerns rather than artificially separating them helps us delete and move them around easier, and that [optimizes for change](https://overreacted.io/optimized-for-change/).\n\nI have more thoughts on how we can apply twists of this ideahere [on my old proposal](https://github.com/sw-yx/react-sfc-proposal).\n\nThis movement has been a long time coming, and I can see the momentum accelerating now.\n"
  },
  {
    "slug": "no-controlled-forms",
    "data": {
      "title": "You May Not Need Controlled Form Components",
      "description": "A common design pattern for forms in React is using Controlled Components - but involves a lot of boilerplate code. Here's another way.",
      "tag_list": [
        "react"
      ]
    },
    "content": "\n2 common design patterns for forms in React are:\n\n- using [Controlled Components](https://reactjs.org/docs/forms.html#controlled-components) but it involves a lot of boilerplate code with a bunch of **React states**, often necessitating a [Form library like Formik](https://jaredpalmer.com/formik/).\n- using [Uncontrolled Components](https://reactjs.org/docs/uncontrolled-components.html) with a bunch of **React refs**, trading off a lot of declarativity for not much fewer lines of code.\n\nBut a lower friction way to handle form inputs is to use [HTML name attributes](https://www.w3resource.com/html/attributes/html-name-attribute.php). As a bonus, your code often turns out less React specific!\n\n*[Twitter discussion here](https://twitter.com/swyx/status/1237171075081003008)*.\n\n## Bottom Line Up Front\n\nYou can access [HTML name attributes](https://www.w3resource.com/html/attributes/html-name-attribute.php) in event handlers:\n\n\n```js\n// 31 lines of code\nfunction NameForm() {\n  const handleSubmit = (event) => {\n    event.preventDefault();\n    if (event.currentTarget.nameField.value === 'secretPassword') {\n      alert('congrats you guessed the secret password!')\n    } else if (event.currentTarget.nameField.value) {\n      alert('this is a valid submission')\n    }\n  }\n  const handleChange = event => {\n    let isDisabled = false\n    if (!event.currentTarget.nameField.value) isDisabled = true\n    if (event.currentTarget.ageField.value <= 13) isDisabled = true\n    event.currentTarget.submit.disabled = isDisabled\n  }\n  return (\n    <form onSubmit={handleSubmit} onChange={handleChange}>\n      <label>\n        Name:\n        <input type=\"text\" name=\"nameField\" placeholder=\"Must input a value\"/>\n      </label>\n      <label>\n        Age:\n        <input type=\"number\" name=\"ageField\" placeholder=\"Must be >13\" />\n      </label>\n      <div>\n        <input type=\"submit\" value=\"Submit\" name=\"submit\" disabled />\n      </div>\n    </form>\n  );\n}\n```\n\n*Codepen Example here: https://codepen.io/swyx/pen/rNVpYjg*\n\nAnd you can do everything you'd do in vanilla HTML/JS, inside your React components.\n\nBenefits:\n\n- This is fewer lines of code\n- a lot less duplicative naming of things\n- Event handler code works in vanilla JS, a lot more portable\n- Fewer rerenders\n- If SSR'ed, works without JS, [with action attributes](https://www.w3schools.com/tags/att_form_action.asp), (thanks [Brian](https://twitter.com/brianleroux/status/1237173597623336960)!)\n- you can supply a default value with `value`, as per native HTML, instead of having to use the React-specific `defaultValue` (thanks [Li Hau](https://twitter.com/lihautan/status/1237241647047553025)!)\n\n## Controlled vs Uncontrolled Components\n\nIn the choice between Controlled and Uncontrolled Components, you basically swap a bunch of states for a bunch of refs. Uncontrolled Components are typically regarded to have less capabilities -  If you click through the React docs on Uncontrolled Components you get [this table](https://goshakkk.name/controlled-vs-uncontrolled-inputs-react/):\n\n| feature  | uncontrolled  | controlled  |\n|---|---|---|\n| one-time value retrieval (e.g. on submit)  | ✅  |  ✅ |\n|  validating on submit  | ✅  |  ✅ |\n|  field-level validation |  ❌ | ✅  |\n|  conditionally disabling submit button |  ❌ | ✅  |\n|  enforcing input format |  ❌ | ✅  |\n|  several inputs for one piece of data  |  ❌ | ✅  |\n|  dynamic inputs |  ❌ | ✅  |\n\n\nBut this misses another option - which gives Uncontrolled Components pretty great capabilities almost matching up to the capabilities of Controlled Components, minus a ton of boilerplate.\n\n## Uncontrolled Components with Name attributes\n\nYou can do field-level validation, conditionally disabling submit button, enforcing input format, etc. in React components, without writing controlled components, and without using refs.\n\nThis is due to how Form events let you access name attributes by, well, name! All you do is set a name in one of those elements that go in a form:\n\n```jsx\n<form onSubmit={handleSubmit}>\n  <input type=\"text\" name=\"nameField\" />\n</form>\n```\n\nand then when you have a form event, you can access it in your event handler:\n\n```jsx\nconst handleSubmit = event => {\n  alert(event.currentTarget.nameField.value) // you can access nameField here!\n}\n```\n\nThat field is a proper reference to a DOM node, so you can do everything you'd normally do in vanilla JS with that, including setting its value!\n\n```jsx\nconst handleSubmit = event => {\n  if (event.currentTarget.ageField.value < 13) {\n     // age must be >= 13\n     event.currentTarget.ageField.value = 13\n  }\n  // etc\n}\n```\n\nAnd by the way, you aren't only restricted to using this at the form level. You can take advantage of [event bubbling](https://lavalite.org/blog/bubbling-and-capturing-in-react) and throw an `onChange` onto the `<form>` as well, running that `onChange` **ANY TIME AN INPUT FIRES AN ONCHANGE EVENT**! Here's a full working form example with [Codepen](https://codepen.io/swyx/pen/rNVpYjg):\n\n```js\n// 31 lines of code\nfunction NameForm() {\n  const handleSubmit = (event) => {\n    event.preventDefault();\n    if (event.currentTarget.nameField.value === 'secretPassword') {\n      alert('congrats you guessed the secret password!')\n    } else if (event.currentTarget.nameField.value) {\n      alert('this is a valid submission')\n    }\n  }\n  const handleChange = event => {\n    let isDisabled = false\n    if (!event.currentTarget.nameField.value) isDisabled = true\n    if (event.currentTarget.ageField.value <= 13) isDisabled = true\n    event.currentTarget.submit.disabled = isDisabled\n  }\n  return (\n    <form onSubmit={handleSubmit} onChange={handleChange}>\n      <label>\n        Name:\n        <input type=\"text\" name=\"nameField\" placeholder=\"Must input a value\"/>\n      </label>\n      <label>\n        Age:\n        <input type=\"number\" name=\"ageField\" placeholder=\"Must be >13\" />\n      </label>\n      <div>\n        <input type=\"submit\" value=\"Submit\" name=\"submit\" disabled />\n      </div>\n    </form>\n  );\n}\n```\n\n*Codepen Example here: https://codepen.io/swyx/pen/rNVpYjg*\n\nNames only work on `button, textarea, select, form, frame, iframe, img, a, input, object, map, param and meta` elements, but that's pretty much everything you use inside a form. [Here's the relevant HTML spec](https://www.w3.org/TR/html52/sec-forms.html#indexed-for-named-property-retrieval) - ([Thanks Thai!](https://twitter.com/dtinth/status/1237218169078464512)) so it seems to work for ID's as well, although I personally don't use ID's for this trick.\n\n\nSo we can update the table accordingly:\n\n| feature  | uncontrolled  | controlled  | uncontrolled with name attrs | \n|---|---|---|---|\n| one-time value retrieval (e.g. on submit)  | ✅  |  ✅ | ✅  | \n|  validating on submit  | ✅  |  ✅ | ✅  |\n|  field-level validation |  ❌ | ✅  |❌ |\n|  conditionally disabling submit button |  ❌ | ✅  | ✅  | \n|  enforcing input format |  ❌ | ✅  | ✅  | \n|  several inputs for one piece of data  |  ❌ | ✅  | ✅  | \n|  dynamic inputs |  ❌ | ✅  | 🤔  |\n\nAlmost there! but isn't field-level validation important?\n\n## setCustomValidity\n\nTurns out the platform has a solution for that! You can use the [Constraint Validation API](https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/HTML5/Constraint_validation#Complex_constraints_using_the_Constraint_Validation_API) aka `field.setCustomValidity` and `form.checkValidity`! woot! \n\n[Here's the answer courtesy of Manu](https://twitter.com/swyx/status/1237171075081003008)!\n\n```js\nconst validateField = field => {\n  if (field.name === \"nameField\") {\n    field.setCustomValidity(!field.value ? \"Name value is required\" : \"\");\n  } else if (field.name === \"ageField\") {\n    field.setCustomValidity(+field.value <= 13 ? \"Must be at least 13\" : \"\");\n  }\n};\n\nfunction NameForm() {\n  const handleSubmit = event => {\n    const form = event.currentTarget;\n    event.preventDefault();\n\n    for (const field of form.elements) {\n      validateField(field);\n    }\n\n    if (!form.checkValidity()) {\n      alert(\"form is not valid\");\n      return;\n    }\n\n    if (form.nameField.value === \"secretPassword\") {\n      alert(\"congrats you guessed the secret password!\");\n    } else if (form.nameField.value) {\n      alert(\"this is a valid submission\");\n    }\n  };\n  const handleChange = event => {\n    const form = event.currentTarget;\n    const field = event.target;\n\n    validateField(field);\n\n    // bug alert:\n    // this is really hard to do properly when using form#onChange\n    // right now, only the validity of the current field gets set.\n    // enter a valid name and don't touch the age field => the button gets enabled\n    // however I think disabling the submit button is not great ux anyways,\n    // so maybe this problem is negligible?\n    form.submit.disabled = !form.checkValidity();\n  };\n  return (\n    <form onSubmit={handleSubmit} onChange={handleChange}>\n      <label>\n        Name:\n        <input type=\"text\" name=\"nameField\" placeholder=\"Must input a value\" />\n        <span className=\"check\" role=\"img\" aria-label=\"valid\">\n          ✌🏻\n        </span>\n        <span className=\"cross\" role=\"img\" aria-label=\"invalid\">\n          👎🏻\n        </span>\n      </label>\n      <label>\n        Age:\n        <input type=\"number\" name=\"ageField\" placeholder=\"Must be >13\" />\n        <span className=\"check\" role=\"img\" aria-label=\"valid\">\n          ✌🏻\n        </span>\n        <span className=\"cross\" role=\"img\" aria-label=\"invalid\">\n          👎🏻\n        </span>\n      </label>\n      <div>\n        <input type=\"submit\" value=\"Submit\" name=\"submit\" disabled />\n      </div>\n    </form>\n  );\n}\n```\n\n*Codesandbox Example here: https://codesandbox.io/s/eloquent-newton-8d1ke*\n\n*More complex example with cross-dependencies: https://codesandbox.io/s/priceless-cdn-fsnk9*\n\n\nSo lets update that table:\n\n| feature  | uncontrolled  | controlled  | uncontrolled with name attrs |\n|---|---|---|---|---|\n| one-time value retrieval (e.g. on submit)  | ✅  |  ✅ | ✅  |\n|  validating on submit  | ✅  |  ✅ | ✅  |\n|  field-level validation |  ❌ | ✅  |✅ |\n|  conditionally disabling submit button |  ❌ | ✅  | ✅  |\n|  enforcing input format |  ❌ | ✅  | ✅  |\n|  several inputs for one piece of data  |  ❌ | ✅  | ✅  | \n|  dynamic inputs |  ❌ | ✅  | 🤔  |\n\nI am leaving dynamic inputs as an exercise for the reader :)\n\n## React Hook Form\n\nIf you'd like a library approach to this, [BlueBill](https://twitter.com/bluebill1049/)'s [React Hook Form](https://react-hook-form.com/) seems similar, although my whole point is you don't NEED a library, you have all you need in vanilla HTML/JS!\n\n## So When To Use Controlled Form Components?\n\nIf you need a lot of field-level validation, I wouldn't be mad if you used Controlled Components :)\n\nHonestly, when you need to do something higher powered than what I've shown, eg when you need to pass form data down to a child, or you need to guarantee a complete rerender when some data is changed (i.e. your form component is really, really big). We're basically cheating here by directly mutating DOM nodes in small amounts, and the whole reason we adopt React is to not do this at large scale!\n\nIn other words: Simple Forms probably don't need controlled form components, but Complex Forms (with a lot of cross dependencies and field level validation requirements) probably do. Do you have a Complex Form?\n\nPassing data up to a parent or sibling would pretty much not need Controlled Components as you'd just be calling callbacks passed down to you as props.\n\n[Here's Bill's take](https://twitter.com/bluebill1049/status/1237246244101079040):\n\n> I love this topic. Let's take a step back — everything is achievable with vanilla Javascript. For me the thing that React and other libraries offer is a smoother development experience, and most importantly making projects more maintainable and easy to reason with.\n>\n> Here are some of my thoughts, hopefully people give equal opportunity to controlled and uncontrolled. They both have trade-offs. Pick the right tool to make your life easier.\n>\n> Let React handle the re-render when it's necessary. I wouldn't say it's cheating on React instead letting React be involved when it needs to be.\n>\n> Uncontrolled inputs are still a valid option for large and complex forms (and that's what I have been working with over the years professionally). It doesn't matter how big your form is it can always be breake apart and have validation applied accordingly.\n\n## References\n\n- https://formvalidation.io/"
  },
  {
    "slug": "guo-lai-ren",
    "data": {
      "title": "Guo Lai Ren (过来人)",
      "description": "One of the most powerful forms of persuasion is the argument from crossover people",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nHave you noticed that people who have changed their minds are more convincing?\n\nLet's say there's two sides of a debate, A and B. There are staunch advocates of A and loyal supporters of B. But they've always been advocates of A and supporters of B. A people have their talking points, B people have theirs, and when they argue they yell these as loudly as they can to each other and to the neutrals in the middle. Neither really budge.\n\nThen, every now and then, you get someone who switches sides, for whatever reason. Someone who was a lifelong A starts advocating for B. That person is more effective at changing the minds of other A's, and in speaking to neutral people as well.\n\n## What To Call It?\n\nIn English we might call this person a **flip-flopper**, a derogatory term. We sneer at them, call them out on presidential debate changes, use their former statements against them to deny their credibility. We admire the person who has just thought one thing all their life and stuck to their guns under adversity, even when - especially when - everyone else doesn't agree. We don't have a neutral shorthand for someone who changes their mind.\n\nIn Chinese we have a different phrase for these people - 过来人 (Guo Lai Ren). The [Chinese English Pinyin dictionary](https://chinese.yabla.com/chinese-english-pinyin-dictionary.php?define=guo+lai+ren) defines 过来人 as \"an experienced person\", \"somebody who has 'been around (the block)'\". This is accurate, but when applied to switching sides, it takes on an additional meaning. \n\n过来 (Guo Lai) means \"[to come over](https://chinese.yabla.com/chinese-english-pinyin-dictionary.php?define=guo+lai)\". A 过来人 (Guo Lai Ren) is, literally \"a person (人) who comes over\". Kind of a neutral statement, not as derogatory as the English equivalent.\n\nYou don't even really have to formally \"change your mind\" to be a 过来人 - you could just have spent your life doing, saying, believing one thing without really questioning it, and then suddenly have a [\"Come to Jesus\" moment](https://grammarist.com/idiom/come-to-jesus-moment-and-come-to-jesus-meeting/) and switch to the alternative side.\n\n## Guo Lai Ren in Action\n\nHere are some 过来人 in action:\n\n- [Ryan Dahl](https://www.youtube.com/watch?v=M3BM9TB-8yA) (the Creator of Node.js) - \"10 Things I Regret About Node.js\"\n- [Solomon Hykes](https://twitter.com/solomonstre/status/1111004913222324225?lang=en) (original Docker team) - \"If WASM+WASI existed in 2008, we wouldn't have needed to create Docker.\"\n- [Megan Phelps-Roper](https://www.ted.com/talks/megan_phelps_roper_i_grew_up_in_the_westboro_baptist_church_here_s_why_i_left/up-next?language=en) - \"I grew up in the Westboro Baptist Church. Here's why I left\"\n- [Eric Falkenstein - \"An Economist's Rational Road to Christianity\"](http://falkenblog.blogspot.com/2016/02/an-economists-rational-road-to.html) - a well known econ/quant finance authority, on becoming a born-again Christian late in life\n- [Michael Shellenberger](https://www.youtube.com/watch?v=ciStnd9Y2ak) - \"Why I changed my mind about nuclear power\"\n- [Mikhail Gorbachev](https://en.wikipedia.org/wiki/Mikhail_Gorbachev) ending the Cold War by reversing several key tenets of Soviet Communism\n- [Robert Oppenheimer](https://www.history.com/news/father-of-the-atomic-bomb-was-blacklisted-for-opposing-h-bomb), leader of the Manhattan Project, failed to convince Truman to not use the Atomic Bomb but ultimately prevailed at the UN - no country has used atomic weapons since 1945.\n- Every example of [character development in great films](https://www.studiobinder.com/blog/character-development/)\n- *more? I will keep adding if I think of more*\n\n## How It Works\n\nI think the 过来人 effect works because it engages a few core persuasion principles, which are really the same thing, stated a few different ways:\n\n- **Seek First to Understand, Then to Be Understood**: One of [the 7 Habits of Highly Effective People](https://www.franklincovey.com/the-7-habits/habit-5.html), you absolutely understand the opposite side's point of view because you *were* the opposite side. So you are intimately familiar with the flaws in their talking points, and hyper aware of important leverage points that they may be underappreciating.\n- **Pacing and Leading**: Persuasion students know that [pacing and leading](https://www.mindwhirl.com/marketing/marketing-psychology/how-to-persuade-anyone-using-pacing-and-leading/) is extremely powerful, and there can be no more authentic form of pacing than self identifying to be formerly part of your opponent's tribe.\n- **Inception**: Much like [the Christopher Nolan movie](https://en.wikipedia.org/wiki/Inception), the effect here is to soundly convince your audience that you *are* them, get them to treat you as their avatar, and that what you're talking about is really an idea that *they* came up with. Then it *really* takes root.\n- **Skin in the Game**: It is hard to publicly admit you were wrong, or to change the course of your career or personal beliefs. People respect that and listen to you when you put yourself on the line like that.\n- **Appeal to Reality**: Every idealist's enthusiasm eventually carries them beyond the bounds of reality due to their optimism and rose-tinted glasses. This is a very human thing to do, but it is also a very human thing to respect \"reality checks\". Nobody looks good denying reality. A 过来人, having made the arduous personal journey of changing sides, probably has new, relevant facts on their side (at least, relevant for changing minds). John Maynard Keynes put it best: [“When the facts change, I change my mind. What do you do, Sir?”](https://www.mitonoptimal.com/news/2018/when-the-facts-change-i-change-my-mind-what-do-you-do-sir/)\n\nI don't have a name for this but there's also the very simple fact that people who have changed their minds are probably better at convincing people to change their minds, than people who have never changed their minds.\n\n## Conclusion\n\nI have noticed in my life a lot of small instances of this principle. Some people note that the best Programming Teachers and Developer Advocates often are career changers, because they've had to cross over from a later stage and didn't grow up with a Commodore 64. I take notice when someone whose entire working career was spent doing one thing suddenly starts talking about a different way to do something.\n\nWhat are the takeaways? I don't know, I don't have a Call To Action or anything. If you notice a 过来人, notice this effect at play. If you are a 过来人 yourself, take note of how you can be more effective by listening to yourself over time (maybe if you have previous tweets and blogs, you can quote yourself and talk about what you got wrong).\n\nMostly I just wanted to de-stigmatize changing your mind. It shouldn't be done lightly, but it shouldn't be a negative either if genuine. And you gain persuasive powers when it happens. [Robert Cialdini called these \"crossover communicators\"](https://theartofcharm.com/podcast-episodes/robert-cialdini-pre-suasion-episode-543/) once, so if you'd like an English term for this, you could use that."
  },
  {
    "slug": "feedback-ladders",
    "data": {
      "title": "Feedback Ladders",
      "description": "A post for the Netlify blog on how we did Code Reviews during my recent product rotation.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\n*Canonical URL: [https://www.netlify.com/blog/2020/03/05/feedback-ladders-how-we-encode-code-reviews-at-netlify/](https://www.netlify.com/blog/2020/03/05/feedback-ladders-how-we-encode-code-reviews-at-netlify/?utm_source=twitter&utm_medium=laddersblog-swyx&utm_campaign=devex)*\n\nThe code review is a critical part of life as a professional developer: in most engineering organizations, no code gets checked in without at least a second look from another engineer. This has many benefits for enforcing code standards; however, we can run into communication issues because we don’t establish standards for the code reviews themselves.\n\nTo solve this, Netlify’s UX team developed shared terminology for code reviews that we call the Feedback Ladder! It has worked very well for Netlify and we share it in the hope that it helps your team.\n\n![https://www.netlify.com/img/blog/frame-4-2x.png](https://www.netlify.com/img/blog/frame-4-2x.png)\n\n## The Problem\n\nCode reviews catch everything systems cannot. We can run Prettier and lint rules and automated unit/integration/end-to-end tests all we like. But people still have to check that tests test the right things, that code conforms to agreed-upon standards, that comments and naming and code organization and architectural choices can be maintained by others.\n\nHowever, there isn’t a binary divide between good code that can be merged and bad code that must be changed. There are tradeoffs between ideal solutions and speed of shipping: Perfect is the enemy of good. Code review feedback isn’t all created equal either - sometimes you could just be pointing out a slight optimization, sometimes you could be ringing the alarm bells on a blocker that deserves immediate attention!\n\nLast but not least, code review is still a form of asynchronous communication, so a lot of context (like body language and the opportunity for back-and-forth) is lost. This is especially important to us as a [majority remote company](https://www.netlify.com/about/), so we wanted a way to add some nuance to our feedback.\n\nMany developers indicate the priority level of their feedback by noting if it is a “nit”. A “nit” is like a “nitpick” - you’re voluntarily indicating the severity of the issue discussed. However there is still a lot of room for interpretation - some “nits” are still code standards issues, and not every piece of feedback without a “nit” is a high priority blocker.\n\n## The Solution\n\nThe system Leslie eventually landed on was the idea of the Feedback Ladder. When giving feedback, it helps for the team to use shared terminology to better understand where the feedback fits into the larger picture. This is especially necessary in the context of our work, where we aim to ship fast, iterate often, and squash blockers as quickly as possible.\n\nIn an effort to make the levels of the feedback ladder easier to remember and use, Kristen came up with metaphorical names to describe each step. The idea is that you're living in a house that's still being built. Each of the \"inconveniences\" we've listed (mountain, boulder, pebble, sand, dust) have a different level of impact on your day-to-day life in the house. For example, you may notice dust on the floor, but it doesn't impede your ability to live your life; on the other hand, a boulder blocking the door would quite literally be a blocker.\n\nWe prefix each piece of feedback with the name in square brackets, like so:\n\n![Code review suggestions and feedback on GitHub](https://lh6.googleusercontent.com/dmKl2BhMBBogPdCbFxyySXo4YjFZ0u9EOH8dS6UKG5cQlyeVCKiXnWpr6tVVdlvDt7-bUMOC1GPsLnhb5JGl_ynZeijF7yQJ7ryd0tUjnqDQq7U8yL1XpCwBrJvP_cURPuMTCM7D)\n\nThe way we give and receive feedback for each step on the ladder should scale appropriately according to its severity.\n\n## ⛰ Mountain / Blocking and requires immediate action\n\n> This mountain has torn the roof off of your house. There's no room for you inside, you gotta take care of that right away before doing anything else!\nThis feedback blocks all related work, and requires immediate action. This type of feedback should never be given in direct message or private conversation.\n\n**Example**: A developer is working on an issue and discovers more about the problem domain that was not previously known, realizing that the issue itself needs to be taken back to the drawing board because it is not feasible, needs to be broken up, or the solution doesn’t actually address the intended user stories.\n\n## 🧗‍♀️ Boulder / Blocking\n\n> This boulder is blocking the door, people can't get in and you can't get out. You gotta get it out of there. But you can take care of some other stuff first, like finishing installing the lights in the kitchen.\nThis feedback should block the work from moving forward or being approved, but it does not necessarily require immediate action from the team.\n\n**Example**: A feature has been implemented and tagged for review. When testing the [Deploy Preview](https://www.netlify.com/blog/2016/07/20/introducing-deploy-previews-in-netlify/), a reviewer notices that the wrong API endpoint is being used and the data displayed in the frontend is incorrect.\n\n## ⚪️ Pebble / Non-blocking but requires future action\n\n> This pebble is super frustrating when you encounter it on the floor, but it doesn't get in your way that often so you can deal with it later when you have time.\nThis feedback should not block the work from moving forward or being approved, but it does require future action from the team. This generally means the feedback is not required for the MVP or initial draft of this work, but should be addressed at some point.\n\n**Example**: During a Design review, a designer says they would like to to change the background color of a status badge to help make the status more clear.\n\n## ⏳ Sand / Non-blocking but requires future consideration\n\n> It's a little annoying to have sand in your house; you should consider later if it's worth cleaning up. But maybe you just end up pointing out to folks that your house is at the beach so it's always gonna be a little sandy no matter what and you're willing to live with that since you get to be at the beach!\nThis feedback should not block the work from moving forward or being approved. The project lead or owner should consider the feedback and implement it if there is at least one other team member who concurs.\n\n**Example**: During code review, a team member leaves feedback about refactoring for readability. A very common example of \\[sand] feedback on the frontend team is code style or approach (not user-facing).\n\n## 🌫 Dust / Non-blocking, \"take it or leave it\"\n\n> Some people may notice the dust and think it's worth cleaning up, others won't mind it at all.\nThis feedback should not block the work from moving forward or being approved. The project lead or owner should consider the feedback and implement it if they deem it useful.\n\n**Example**: During code review, a team member suggests a different name for a new variable.\n\n## One more tip: The Ground Floor\n\nWe have found our Feedback Ladder system very helpful in concisely encoding severity of feedback, and think it will be helpful in other engineering teams too. You may wish to add or remove rungs when you implement this in your teams, and if you have any interesting variations on this idea, [we’d love to hear them too](https://twitter.com/netlify)!\n\nOne final tip for asynchronous code reviews: Roundtrips (from teammate to reviewer back to teammate) are expensive, costing up to two working days of a sprint each time. One great way to minimize the number of roundtrips is to preemptively review your own code, adding comments that are only meant to be read at review-time. (Of course, if the comment makes sense to future readers, consider adding the comment in the actual committed code itself!) \n\nThis lets you leverage your knowledge of your teammates and indicates that you are considerate of the concerns they would typically raise. This increases team trust and shipping velocity. As a pre-emptive measure before you even step on to the Feedback Ladder, you could consider this the “Ground Floor” of code reviews!"
  },
  {
    "slug": "coronavirus-recession",
    "data": {
      "title": "The Coronavirus Recession and What it Means for Developers",
      "description": "The US is probably going into recession - here's why I'm talking about it now, what it could look like, what Devs can do to prepare, and why it's not the End of the World.",
      "tag_list": [
        "advice"
      ]
    },
    "content": "\n*written March 7*\n\n**[PSA: the US is probably going into recession](https://twitter.com/swyx/status/1236126821713879040).** \n\nHere are some thoughts for what developers can do to prepare. The TL;DR of it is that you should know your economic dependencies like you do your open source dependencies, and there are serious vulnerabilities being raised right now in your economic dependency chain. But it is nothing that won't be fixed with time.\n\n> Edit: Discussions on\n> - [Hacker News](https://news.ycombinator.com/item?id=22525238)\n> - [Twitter](https://twitter.com/swyx/status/1236164959236653057?s=20)\n> - [More HN](https://news.ycombinator.com/item?id=22554886) \n\n## Not the End of the World\n\nI hate alarmism so I will say up front that of course it is not the end of the world if a recession does result. \n\nThis isn't a leverage induced generational shock like 2008 was, with multiple financial institution failures. This is better compared to 2001, where the tech sector deflated a lot and travel shut down, but the real economy quickly rebounded. It will be fine long term but we may see a couple rough years.\n\nI write this because developers may be heads down in their tech bubble, only to have them or their family surprised out of left field with some economic impact because they haven't been paying attention.\n\n## What It Could Look Like\n\nSome states (Texas, North Dakota, Alaska, Oklahoma, etc) will be disproportionately affected by the oil slump - and others (Seattle, NYC, etc) by the travel slump - so even if you don't directly have anything to do with the travel and energy industry, you may work for someone that does. If they lose business, so do you. These things ripple out.\n\nStartup fundraising will evaporate. VC's are already highly attuned to this and [Sequoia has already put out its warning](https://medium.com/sequoia-capital/coronavirus-the-black-swan-of-2020-7c72bdeb9753) - reminiscent of [2008's RIP Good Times memo](https://www.sequoiacap.com/article/rip-good-times). Every investment is being made (or revoked) with this lens - VC's react MUCH faster than normal people do, that's kind of their whole job. They are also inclined to recognize asymmetric payoffs and exponential moves. Of course, [funding is still there for exceptional founders](http://investorfieldguide.com/benchmark/).\n\nIf you are a freelancer or agency: Clients will take longer - their own projects become more uncertain, they'll magically receive more competitive proposals, they will book shorter contracts, they will renegotiate more things and change the deal abruptly. *This exact thing is happening to the entire travel industry right now and will happen to you.*\n\nIf you are an employee: Hiring will take longer. Same deal but now companies have to weigh your healthcare and WFH benefits against their downward revised cashflow projections. Regrettable layoffs can and will happen - nothing at first (because employers will try to hold the line), then we'll see a flood of them (because employers run out of options).\n\nIf you work at Airbnb: [you're not getting an IPO](https://www.bloomberg.com/news/articles/2020-03-02/airbnb-s-path-to-2020-stock-listing-imperiled-by-coronavirus). Sorry. The window was last year.\n\nCompanies can be less willing to try new tech, preferring the tried and tested, though proven cost saving projects may get a lot of interest (especially ones that replace humans, like [Process Automation](https://www.swyx.io/writing/no-code-rpa)). \n\nRemote-first tools like Zoom of course get a bump, but also check out [Tuple](https://tuple.app/) for remote pair programming. Meetups and Conferences will also have to figure out how to go online, given travel bans from [Intel](https://www.koin.com/news/health/coronavirus/intel-bans-employee-travel-to-8-countries-over-coronavirus/), [Google](https://www.businessinsider.com/coronavirus-covid-19-google-bans-all-employee-international-business-travel-2020-3), [Facebook](https://www.cnn.com/2020/01/28/business/facebook-china-travel-ban-coronavirus/index.html), [Nike, Microsoft, Twitter, Amazon](https://finance.yahoo.com/news/coronavirus-covid-19-twitter-amazon-nike-google-offices-deep-clean-travel-ban-092211063.html) and more. [YC is now online only](https://news.ycombinator.com/item?id=22506013), as is [Stanford](https://news.ycombinator.com/item?id=22509939). Some of these online shifts may become permanent even after the virus blows over, as we were all aware this had inherent benefits anyway.\n\nSome expenses are more discretionary than others; those get cut first. Developer Relations as a field has already had an uncomfortable conversation around metrics; it will now accelerate to a discussion of fullblown online-only affiliate/content marketing programs comparable to [an identity crisis](https://twitter.com/editingemily/status/1235364507553116160). But consider also Product Marketing, or Inhouse Recruiting, or User Research, or even Support. Training budgets, offsites, corporate sponsorships are all on the table for discreet cuts. [Corporate venture arms](https://www.cognizant.com/accelerator)? Sorry, no time for fair weather toys.\n\nAs an industry though, we are relatively safer than others because of our ability to work remotely and transferability of our core skillset. But our friends and family may not be so lucky.\n\n## What Developers Can Do\n\n- **If you are about to quit your job to learn to code, don't.** This is a terrible environment to take this particular risk. [Learn for free from FreeCodeCamp, in nights and weekends](https://hackernoon.com/no-zero-days-my-path-from-code-newbie-to-full-stack-developer-in-12-months-214122a8948f), and [Learn in Public](https://www.swyx.io/writing/learn-in-public).\n- If you are about to quit your job to go job searching, make sure you have ample networks or backup plans. Again, if you are a developer who can work remote, you will be relatively insulated.\n- If you make buying decisions (e.g. running a company or a department): think about how you can **turn fixed cost into variable cost**. If you were kinda half remote anyway, maybe go full remote or swap the office for a coworking space subscription. Hire contractors instead of full-time. **Buy SaaS services instead of build internally.** Focus on conversion and retention rather than inorganic user acquisition.\n- If you are a freelancer or agency: Think ahead to how your clients could be affected - you may need to aggressively grow your funnel while also **planning for more downtime** between work.\n- If you are an employee: You can't do much about your options or 401k. But recognize that you are doubly long the economy in terms of both your stock compensation and your job (and real estate in affected states). \n- If you work at a startup that has a cash burn necessitating future fundraising, be aware that this can become very very relevant to you **if the funds don't materialize**, through no fault of the founders. Think through second order effects - the funds may not materialize not just because VCs are less willing to part with cash, but also because the startup itself may fail to meet targets because of the broader economic environment.\n- If you are a founder - don't worry, just struggle through. Hear [Paul Graham](http://paulgraham.com/badeconomy.html) from the last recession, and [Bill Gurley and Chetan Puttagunta](http://investorfieldguide.com/benchmark/) more recently.\n\nI'm an ex Finance guy turned Developer, and Finance people are very used to thinking about ripple and second order effects. The Developer-speak equivalent is thinking about this as your economic dependency chain. \n\nIt's going to turn out differently for everyone. FAANG types aren't going to be as affected as early stage startup types or freelancer types. So I encourage you think about your own dependencies. I hope I've got your gears turning on how your world may be impacted by the Coronavirus Recession.\n\n## Why Now\n\nIt's been known that the quarantines in China has slowed global supply chains (e.g. [Apple](https://apnews.com/2ed97816c426883a735db996cb1185af)). [Store closures have already hit the US]( https://www.vox.com/2020/2/28/21153492/coronavirus-recession-china-stock-market-economy).  And of course worldwide travel has been impacted.\n\nBut today's [breaking of OPEC+](https://twitter.com/business/status/1235950989477400576) is something I've been waiting to see - some compounding reaction not directly related to Coronavirus that nevertheless just comes at the most inopportune time because that's Murphy's law.\n\n![https://pbs.twimg.com/media/ESeY-P1X0AAYQhw?format=jpg&name=large](https://pbs.twimg.com/media/ESeY-P1X0AAYQhw?format=jpg&name=large)\n\nThe US is pretty much neutral in terms of imports vs exports, but if we see [$30 Crude](https://www.marketwatch.com/story/opec-oil-deal-failure-may-lead-to-30-oil-prices-2020-03-06) a broad swath of US shale oil and natural gas drillers will be insolvent and we will revisit the energy market panic of 2016. A decent number of these companies are leveraged, so their bankruptcies have a plausible transmission mechanism to non-energy companies via credit markets. This nearly happened in 2016 and now could happen again, this time with the added impact from Coronavirus.\n\nI think this is enough to start talking about in advance, with the caveat that [economists have predicted 9 of the last 5 recessions](https://www.bloomberg.com/news/articles/2019-03-28/economists-are-actually-terrible-at-forecasting-recessions) and I am no exception so this is not a certainty by any stretch.\n\n## Further Reading\n\n- https://designobserver.com/feature/designing-through-the-recession/7177\n- https://medium.com/sequoia-capital/coronavirus-the-black-swan-of-2020-7c72bdeb9753\n- https://swizec.com/blog/how-to-recession-proof-your-career/swizec/9226\n- https://nesslabs.com/working-from-home\n- Paul Graham on founding during a bad economy: http://paulgraham.com/badeconomy.html\n- Suhail as a founder during 2009: https://twitter.com/Suhail/status/1237035812916621312?s=20\n- Cyan Banister on layoffs: https://twitter.com/cyantist/status/1238299604287770624\n- [Ryan Chenkie's Qtn on impact on Developers](https://twitter.com/ryanchenkie/status/1240652806983483394?s=20)\n- [Marco Arment on Under the Radar](https://www.relay.fm/radar/187) (iOS developers and Ad supported businesses)\n- [Ray Dalio on the Coronavirus Depression and a 500 year perspective](https://www.ted.com/talks/ray_dalio_what_coronavirus_means_for_the_global_economy)\n- For Founders\n  - [May Habib on laying off 1/3 of your team](http://traffic.libsyn.com/saastr/319_SaaStr_319-_May_Habib_Founder__CEO__Qordoba.mp3?dest-id=349407) \n  - [Suhail](https://twitter.com/Suhail/status/1237035812916621312?s=20) on how Startups can survive\n  - [Eric Ries](https://thisweekinstartups.com/e1041-the-lean-startups-eric-ries-gives-tactical-advice-for-founders-during-an-economic-downturn-obligations-of-leadership-extending-runway-handling-layoffs-with-grace-finding-new-revenue-oppor/) on holding on to values and taking care of your people first\n  - [Nathan Barry's letter to team](https://nathanbarry.com/uncertainty/)\n  - [Stewart Butterfield's recap](https://twitter.com/stewart/status/1243000487365861376?s=20) and [Recode Decode interview](https://www.stitcher.com/podcast/vox/recode-decode/e/68485468)\n- Impact on \n  - [Podcasts](https://twitter.com/Techmeme/status/1243726092583817216?s=20)\n  - [Subway usage](https://twitter.com/todd_schneider/status/1243893131680120838?s=20)\n  - [Nature](https://twitter.com/ikaveri/status/1239660248207589383?s=20)\n- [Marketing/Customer spend patterns](https://cxl.com/blog/marketing-growth-uncertain-times/)\n\n![https://lh3.googleusercontent.com/spBgrypQjIcngcVf4bzXxmCq6yvYqS19qmU-DmWnUZCtHEafRzqidob4FYqKj09FimQBp_DjJfuUw74xawInBXTPnC22Z7_aC9_P6Rwq53EgXqywpmBk51EXthvNkPTWcjYSHdQ5](https://lh3.googleusercontent.com/spBgrypQjIcngcVf4bzXxmCq6yvYqS19qmU-DmWnUZCtHEafRzqidob4FYqKj09FimQBp_DjJfuUw74xawInBXTPnC22Z7_aC9_P6Rwq53EgXqywpmBk51EXthvNkPTWcjYSHdQ5)\n\n![https://lh3.googleusercontent.com/giQ3a2KDysDUpcsYrGOOg5s0vj0RJyPB6xuCrtmOvRl5ZqkaVLrHvkfdMz7pc_N7HK70Wq2UCw6NznmaEUEO-O_u8Iwh-jklZk70Mvu-UG0hdgjvuZrhdUKu14K5TkAZ82J2kLW_](https://lh3.googleusercontent.com/giQ3a2KDysDUpcsYrGOOg5s0vj0RJyPB6xuCrtmOvRl5ZqkaVLrHvkfdMz7pc_N7HK70Wq2UCw6NznmaEUEO-O_u8Iwh-jklZk70Mvu-UG0hdgjvuZrhdUKu14K5TkAZ82J2kLW_)\n\n## Epistemic Disclosures\n\n- **Epistemic status**: Obviously I'm not predicting recession with  certainty, and my opinion will change as facts change, but risks are real.\n- **Epistemic effort**: I am of course not an expert on the economy, but my prior career was in Finance (buy- and sell-side) and have kept in touch with friends on Wall Street and talked to them for this post.\n\nI'm also OF COURSE not commenting on Coronavirus itself - [refer to the CDC for that](https://www.cdc.gov/coronavirus/2019-ncov/index.html) - but economic impact is something I have some opinions on. "
  },
  {
    "slug": "why-svelte-short",
    "data": {
      "title": "Why Svelte? (The Short Version)",
      "description": "The short version of Why Svelte",
      "tag_list": [
        "svelte"
      ]
    },
    "content": "\nI'm preparing an introductory talk on Why Svelte for the [March Svelte Society NYC meetup](https://www.downtomeet.com/Svelte-Society-NYC/Svelte-Society-NYC-5-the-IDE-s-of-March-2074654) so here are my talk notes.\n\n[My slides are here.](https://docs.google.com/presentation/d/1TN7vjWh_7Q0S9P0CBUzLymA6mHaNr4ZA9qr6R1QcNTE/edit?usp=sharing)\n\n[Ryan Atkn wrote a longer version here](https://github.com/feltcoop/why-svelte).\n\n## Why is it called Svelte?\n\nAttractively thin, graceful and stylish.\n\n## What is Svelte?\n\nSvelte is a compiler, not a library - it takes your Svelte input and turns it into server rendered HTML, static CSS, and clientside JS, and it can build to WebGL, iOS, Android, and Web Components.\n\nIn a library, the code that you write is the code that runs. You have to trade off features vs code size.\n\nWith a compiler, the code that you write can compile to all sorts of formats, so your solution space gets much larger, while also being able to turn code into efficient low level equivalents. You can include features like styles, [transitions](https://svelte.dev/examples#custom-css-transitions), and [FLIP animations](https://svelte.dev/tutorial/animate) without adding code size if it isn't used, or building in compile time accessibility and [unused CSS checking](https://css-tricks.com/what-i-like-about-writing-styles-with-svelte/).\n\n## Svelte in Rich's words\n\nIn 2019 Rich did a series of 3 talks that explored different viewpoints of Svelte:\n\n- [Computer, build me an app](https://www.youtube.com/watch?v=qqt6YxAZoOc) - Svelte as a compiler that takes your component code and turns it into vanilla JavaScript, that manipulates the DOM directly. This means bundles are smaller, which means for faster download and startup times.\n- [Rethinking Reactivity](https://www.youtube.com/watch?v=AdNJ3fydeao) - React reruns too much, memoization and amortization APIs are abstraction leaks. Svelte 3 moves reactivity out of the component API (`this.set()`), and into the language (`=` and `$:`). Spreadsheets and Reactive Programming is approachable to more people, like Svelte is with the HTML approach.\n- [The Return of Write Less, Do More](https://www.youtube.com/watch?v=BzX4aTRPzno) - Svelte lets you write less code with better tooling support because HTML is the language of the web. Writing less lets you do more. Taken to an extreme it may let a lot more people create software and save the planet.\n\n\n## Benchmark Hijacks:\n\n- https://twitter.com/Rich_Harris/status/838115880680710144\n- https://twitter.com/Rich_Harris/status/1065992585095929857/photo/1\n- https://twitter.com/rich_harris/status/1200805237948325888?lang=en\n- https://twitter.com/jpriceio/status/965964676273188869\n\nBasically, Svelte is fast enough. [I don't really care about who is fastest.](https://www.swyx.io/writing/good-enough)\n\n## Why I enjoy Svelte \n\nThis is a [rehash of a blogpost I did](https://www.swyx.io/writing/svelte-why/)\n\n- Batteries Included\n- The Joy of Mutability\n- $ugar $yntax\n  - Two Way Binding\n  - Stores\n- Good Docs\n- Simple Internals\n  - It is easy to contribute! [Here's how](https://www.swyx.io/writing/svelte-contributing/)\n\n## It is still Early Days\n\nMost of all, it is fun and small. Ever felt like you were too late on a framework or community? Now is your chance, there's very little baggage and a lot of blank space for you to color in:\n\n- Projects\n  - [Sapper](https://sapper.svelte.dev/)\n  - [Svelte Native](https://svelte-native.technology/)\n  - [SvelteGL](https://twitter.com/rich_harris/status/1127930615033102337)\n  - [Svelte Routify](https://github.com/sveltech/routify)\n  - [TypeScript Support](https://gist.github.com/orta/ce05553de3fa9ba847ab410f29f874a4)\n  - [MDSvex](https://github.com/pngwn/MDsveX)\n  - [SSG](https://github.com/sw-yx/ssg/blob/master/packages/ssg/README.md)\n- Community \n  - [on Discord](https://svelte.dev/chat)\n  - [13 Meetups worldwide](https://twitter.com/SvelteSociety/status/1235264100600631296)\n- Jobs\n  - https://twitter.com/sveltejobs\n  - [on Discord](https://svelte.dev/chat)\n\n"
  },
  {
    "slug": "working-from-series-b-to-c",
    "data": {
      "title": "Working at a Startup From Series B to C",
      "description": "Some careful thoughts from joining a startup from Series B to C.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nThe startup I work at announced its Series C fundraising today. I thought it worth writing down some thoughts in the moment as this is a special one I don't expect to relive very often, and I'm sure a lot of people out there must wonder what it is like to join a startup through one significant stage.\n\nFirst a couple of disclaimers:\n\n- I don't feel safe to be fully candid about everything I experienced and feel during this, so what you're getting is the extremely filtered story, more due to self preservation and abundance of caution in consideration of others.\n- I'm not even naming the startup so as to make it a generic observation. Obviously you could figure it out. But I don't want to make this about any particular startup - I am just trying to make generally useful observations about how it feels to follow a company through these important stages in its life.\n\nOk now on to my takeaways:\n\n1. It is well known that the actual fundraising usually takes place before the announcement. \n  - A cynical perspective views this as a publicity device, for example to sell a message or to signal to customers or prospective employees. However I do see a strong case for delaying announcements so as to have a coordinated media push through press releases with preapproved quotes from key customers and investors, the sort of thing that you don't really think about when reading announcement blogposts but actually take a lot of wrangling behind the scenes.\n  - It's also common that hiring tends to go up after a fundraising - not simply because companies have more money, but also higher goals are set and therefore more people are needed to get to those goals. Since careers pages are public, you can often see these pages grow before something is announced.\n  - You also see careers pages and other pages get a facelift as the company prepares for the big announcement. This is a flaky indicator though, because of course it is trivial to queue up this facelift to be published alongside the announcement.\n2. Career Capital\n  - I've been told that being part of a well regarded company through this stage means something, you don't have to see it through to IPO or acquisition. I don't necessarily buy that since it's not evident what any individual person's contribution was, and anyway seeing things through is massively value creating. \n  - But still it's not nothing, and people probably ascribe you more competence from your association due to the trackable rise in the company's metrics as inferred from public statements and the continued investment from VCs.\n3. Options are not worth 0\n  - Most people advised me to value options at near-0 when I joined, since startups fail all the time and even if they succeed there is a illiquidity discount that should be applied. However seeing a company through a stage can net you 5-6 figures in value, which is very much not zero\n  - I think a Series B company with well known investors is quite derisked and you are essentially coinvesting with them as an employee (except you invest time instead of initial capital). Of course not a sure-win proposition, but much, much lower risk than the Seed-Series A stage. (duh, I know)\n  - I hear that active secondary markets for startup stock exist, so it's not true that you have to hold to IPO in order to make money on your options.\n  - I don't know anything about tax implications of revalued options, and now I have to find out\n4. Becoming a \"real company\"\n  - [Investopedia has a good breakdown](https://www.investopedia.com/articles/personal-finance/102015/series-b-c-funding-what-it-all-means-and-how-it-works.asp#b-is-for-build) of what each stage roughly maps to in terms of a company's growth intentions - B is for Build, C is for Scale.\n  - I've heard that there are revenue expectations for what each stage is supposed to map to, but I think the variance is so wide as to not be useful. You can't say \"Company X just raised a Series A so they must be making at least $x M in ARR\", [it doesn't work that way](https://tomtunguz.com/benchmarking-exceptional-series-a-companies/). Too many factors, especially in the age of [the Pegasus](https://calacanis.com/2019/09/09/the-pegasus-startup-flying-over-vcs-on-the-wings-of-profits/).\n  - I've heard that around 50-100 people is where you start getting inhouse recruiters and HR folks. Checked out.\n  - Company or Engineering \"all hands\" gets tricky. [Dunbar's number](https://en.wikipedia.org/wiki/Dunbar%27s_number) is supposed to be 150 but I definitely stopped really knowing everybody's names, faces, and brief bios before that. Arguably my fault, and also the fact that we are remote. We did innovate around the format, and the 2-3 new formats I've seen are REALLY great and ones I quite enjoy.\n  - Metrics are key to scaling. Picking a good mix of leading and trailing metrics are the key to timely surfacing issues. Having a healthy arms length attitude around metrics are the key to keep your people being [long term greedy](https://chiefexecutive.net/ceos-pay-long-term-greedy/) instead of doing things for short term wins.\n  - Eng. Management, Career Ladders, Documented Policies for everything becomes a lot more serious and earnest. \n    - I think it is a challenge balancing internal promotion vs bringing in senior talent. I am also at once aware that [the ideal engineer to manager ratio is roughly 6-8:1](https://firstround.com/review/how-to-size-and-assess-teams-from-an-eng-lead-at-stripe-uber-and-digg/), and wary that adding extra management layers can add rather than solve issues. \n    - I find Career Ladders very helpful, and wish more companies simply publicly shared them so that we all as an industry know what each level is broadly expected to achieve. I think it can help our industry professionalize a lot quicker, and also help give more information on how the company is run to prospective applicants, while not really giving any secret sauce away to competitors.\n    - I also find Documented Policies great, but of course Knowledge Management becomes key, so as to avoid uncontained sprawl of policy after policy and pretty soon you don't dare to lift a finger before checking 8 different memos on a thing. (No, not an actual issue at my company, I'm just projecting for illustrative purposes)\n\nIt's totally fine to feel some level of chaos still, at Series C. In fact I think it's a common trope that from the outside the progress can look so smooth, but employees can feel like things are a mess and yet stuff still somehow gets done.\n\nCompared to Series B, this stage feels like the company is a lot less \"fringe\" - you get competitors directly copying your marketing copy and product design, and partners allying themselves with you because they think it will benefit *them*, not the other way around. You even get your first few haters. All good things.\n\nWhile A->B->C seems a relatively well trodden path, I think D rounds and beyond are very dependent on the startup's cashflow needs. I have a less clear idea of what to expect after this round, than I did at the last. The best perspectives I've seen on how funding markets are evolving are from Tom Tunguz:\n\n- https://tomtunguz.com/fundraising-environment-2019/\n- https://tomtunguz.com/saas-round-sizes-2017/"
  },
  {
    "slug": "compile-svelte-lihau",
    "data": {
      "title": "Compile Svelte in Your Head by Tan Li Hau",
      "description": "Linking to a blogpost I was too lazy to do",
      "tag_list": [
        "svelte"
      ]
    },
    "content": "\nLast year, when [I mobilized my network to put together the first NYC Svelte meetup in one week](https://www.swyx.io/writing/starting-dev-community-meetup), I was not even an active Svelte user at the time. I was just a Rich Harris fan and friend.\n\nSo of course organizing a meetup involves finding speakers, and when you give yourself one week to find speakers, you can pretty much expect to be one of the speakers 😂\n\nWhat do I do when I don't know a thing and have to talk about it? I go look at the source code. I had had a lot of success [doing this for React](https://www.swyx.io/speaking/react-hooks/), so I figured I would do this for Svelte. \n\nHaving worked on a [React SFC proposal](https://github.com/sw-yx/react-sfc-proposal), I was super curious exactly how Svelte achieved scoped styles for its components. If you style a `<p>` tag inside a `Parent` component that contains a `Child`, and if there is a `<p>` within that `Child`, somehow the `p` styling from the `Parent` doesn't \"bleed\" to the `Child`!! It turns out that scoped styling means scoping to the component by attaching a unique classname.\n\nI discovered all this and more by looking at the before-and-after output of the [Svelte REPL](https://svelte.dev/repl/). After all, [Svelte is a language](https://gist.github.com/Rich-Harris/0f910048478c2a6505d1c32185b61934) - and any language has a REPL! As I explored more and more I found that I was able to completely demystify the Svelte template language by being able to predict what the compiled output was going to be.\n\nSo I figured I would write a blogpost to organize my thoughts, [like I did for the React one](https://www.netlify.com/blog/2019/03/11/deep-dive-how-do-react-hooks-really-work/) - however the 1 week ran out in a blink and I ended up giving my talk with my slides not even half done.\n\nI wasn't very happy with this and so never posted up my talk at the time. A month later I was visiting Singapore and [got the chance to rerecord the talk](https://www.swyx.io/speaking/svelte-compile-lightning/), but unfortunately the audio was not great.\n\nI never did get around to writing that blogpost, since my curiosity was satiated.\n\nBut now, I don't have to :) Li Hau, one of the Svelte maintainers, and a fellow Singaporean who is just better than me in every way, wrote it up!\n\n[You can read Part 1 of his blogpost, Compile Svelte in your Head, on his blog now.](https://lihautan.com/compile-svelte-in-your-head-part-1/)\n\nWhen you [Learn in Public](https://www.swyx.io/writing/learn-in-public), sometimes you don't even have to write the blogpost, you can just fail a few times and eventually someone will give up waiting for you and just write it instead 😎"
  },
  {
    "slug": "containers-3-jobs",
    "data": {
      "title": "Three Jobs of Containers",
      "description": "A great nugget I learned from watching Brian Holt's Intro to Containers workshop",
      "tag_list": [
        "tech"
      ]
    },
    "content": "\nI've been watching Brian Holt's Intro to Containers on Frontend Masters (this is a [paid, recorded workshop](https://frontendmasters.com/courses/complete-intro-containers) that is every bit worth the subscription, but also the [course notes are available free online](https://btholt.github.io/complete-intro-to-containers/)) and learned something about containers I thought I should share. \n\nContainers do 3 important jobs for multitenant cloud environments: \n\n- No process should be able to access the filesystem of its neighbor\n- No process should be able to see or control the processes its neighbor is running\n- No process should be able to deprive system resources (CPU, memory, etc) from its neighbor\n\nIf you're a frontendy dev like me, you probably have a vague idea that containers are superior to VMs, and you are maybe aware that [serverless functions run inside containers](https://www.swyx.io/writing/stateful-serverless/). You've probably even seen an image like this showing how containers are lighter weight than VMs because VMs have OSes in them:\n\n![https://sites.google.com/site/mytechnicalcollection/_/rsrc/1461078190451/cloud-computing/docker/container-vs-process/docker-containers-vms.png](https://sites.google.com/site/mytechnicalcollection/_/rsrc/1461078190451/cloud-computing/docker/container-vs-process/docker-containers-vms.png)\n\nBut you've maybe never really thought about *how* containers are made! Brian starts off his workshop by showing how to make a container from scratch, which at once blows away the magic and also articulates clearly why you would want something like Docker to take care of all the little details for you.\n\n## Why Containers\n\nBrian first starts off establishing [Why Containers](https://btholt.github.io/complete-intro-to-containers/what-are-containers) - and implicitly - why containers are the future of deployment and the future of cloud (for many of you, they are already present and boring, but remember the future is unevenly distributed!).\n\nThe main thing to understand here, which necessitates the 3 jobs we are about to describe, is that we (and more importantly public cloud vendors) want multiple applications (tenants) to share the same machine, as cost efficiently as possible. \n\nManaging VMs is expensive (in the money sense of the word as well as in the computing speed/efficiency sense) because each VM runs an OS inside a host OS. So we want these apps to run in the same OS, but yet have security and resource-isolation. This isn't just to defend a malicious attacker from snooping its neighbor, but also to be resistant to someone else's badly coded infinite loop to hog or crash the machine and wreck neighbor applications.\n\n## Desired Outcomes\n\nSo here are three properties we want:\n\n- No process should be able to access the filesystem of its neighbor\n- No process should be able to see or control the processes its neighbor is running\n- No process should be able to deprive system resources (CPU, memory, etc) from its neighbor\n\nThere are more, but these are the big 3 we will discuss.\n\n## Job 1: Isolate the Filesystem\n\n> No process should be able to access the filesystem of its neighbor\n\n![https://miro.medium.com/max/4448/1*QuHou1T2IBoPgi8-dB7kBQ.png](https://miro.medium.com/max/4448/1*QuHou1T2IBoPgi8-dB7kBQ.png)\n\nThe \"hack\" here is to use [the Linux `chroot` command](https://en.wikipedia.org/wiki/Chroot). It is a magic command that restricts the root directory of the currently running process, so it can't reach up and out beyond that.\n\n```bash\nmkdir my-new-root\nsudo chroot my-new-root pwd\n# chroot: pwd: No such file or directory\n```\n\nAs you can see it isn't as simple as running the command - you have to also copy over all the standard libraries that would normally be on your root path like `ls` and `bash` or you can't use them! (see [notes for how](https://btholt.github.io/complete-intro-to-containers/chroot))\n\nTo install these libraries you can use [debootstrap](https://wiki.debian.org/Debootstrap), a tool which will install a Debian base system into a subdirectory of another, already installed system:\n\n```bash\napt-get update -y\napt-get install debootstrap -y\ndebootstrap --variant=minbase bionic my-new-root\n\n\nmkdir my-new-root\nsudo chroot my-new-root pwd \n/           # now pwd works!\n```\n\n## Job 2: Isolate Running Processes\n\n> No process should be able to see or control the processes its neighbor is running\n\nProcesses have a surprising amount of control over other unrelated processes. You can `ps` to see the other processes running on your system, and arbitrarily `kill` them:\n\n```bash\n$ ps\n\n  PID TTY           TIME CMD\n16448 ttys000    0:02.56 zsh -l\n57031 ttys000    0:00.19 MY_SUPER_SECRET_RUNTIME\n47986 ttys003    1:38.59 /bin/zsh --login\n\n$ kill 57031\n\n# MY_SUPER_SECRET_RUNTIME killed\n```\n\nWe obviously can't have that. Enter [Namespaces](https://en.wikipedia.org/wiki/Linux_namespaces), introduced in 2002 to help with this. You create a namespace with `unshare`, and give it flags to control what can be locked down for each `chroot`ed environment:\n\n```bash\nunshare --mount --uts --ipc --net --pid --fork --user --map-root-user chroot /better-root bash # this also chroot's for us\n```\n\nThere are [7 kinds of namespaces](https://en.wikipedia.org/wiki/Linux_namespaces#Namespace_kinds):\n\n- Mount points (--mount)\n- Process IDs (--pid)\n- Network (--net): Each namespace will have a private set of IP addresses, its own routing table, socket listing, connection tracking table, firewall, and other network-related resources.\n- Interprocess Communication (--ipc): This prevents processes in different IPC namespaces from using, for example, the SHM family of functions to establish a range of shared memory between the two processes. \n- UTS (--uts): UTS namespaces allow a single system to appear to have different host and domain names to different processes.\n- User ID (--user):  build a container with seeming administrative rights without actually giving elevated privileges to user processes\n- [Control Group](https://en.wikipedia.org/wiki/Linux_namespaces#Control_group_(cgroup)_Namespace): this one seems to be newer.\n\nThis is as much to defend from attackers as it is to protect ourselves from ourselves.\n\n## Job 3: Isolate System Resources\n\n> No process should be able to deprive system resources (CPU, memory, etc) from its neighbor\n\nEvery isolated environment has access to all physical resources of the server - primarily CPU and memory, but also other things like storage and network access. Google engineers came up with [control groups (cgroups)](https://en.wikipedia.org/wiki/Cgroups) in 2006 to isolate resources to their respective processes.\n\n![https://leezhenghui.github.io/assets/materials/build-scalable-system/architecture-nomad-isolation-java-driver-tech.png](https://leezhenghui.github.io/assets/materials/build-scalable-system/architecture-nomad-isolation-java-driver-tech.png)\n\n```bash\n# outside of unshare'd environment get the tools we'll need here\napt-get install -y cgroup-tools htop\n\n# create new cgroups\ncgcreate -g cpu,memory,blkio,devices,freezer:/sandbox\n\n# add our unshare'd env to our cgroup\nps aux # grab the bash PID that's right after the unshare one\ncgclassify -g cpu,memory,blkio,devices,freezer:sandbox <PID>\n\n# Limit usage at 5% for a multi core system\ncgset -r cpu.cfs_period_us=100000 -r cpu.cfs_quota_us=$[ 5000 * $(getconf _NPROCESSORS_ONLN) ] sandbox\n\n# Set a limit of 80M\ncgset -r memory.limit_in_bytes=80M sandbox\n```\n\nAgain, you can read more in the [notes on cgroups](https://btholt.github.io/complete-intro-to-containers/cgroups).\n\n## Conclusion\n\nThese are probably not the only jobs of containers, they are just the 3 that Brian highlighted and I thought it was insightful enough to share. \n\nI was going to try to draw some of these systems but didn't think I would do a good job of it so here are some diagrams I found that now make sense:\n\n![https://www.insecure.ws/_images/ns3.png](https://www.insecure.ws/_images/ns3.png)\n\n![https://duffy.fedorapeople.org/blog/designs/cgroups/diagram2.png](https://duffy.fedorapeople.org/blog/designs/cgroups/diagram2.png)\n\nAnd of course if you want to learn more like this check out [Brian Holt's Intro to Containers on Frontend Masters](https://frontendmasters.com/courses/complete-intro-containers) !\n\nOnce you [approach technology from First Principles](https://www.swyx.io/writing/first-principles-approach), you learn lasting knowledge that you can reapply in other scenarios, and gain the ability to critical analyze the tradeoffs of the technology for yourself instead of taking other people's thoughts at face value.\n\n## Addendum: Firecracker and microVMs\n\nRecent advancements have blurred the line between VM and container. [Firecracker from AWS is regarded](https://news.ycombinator.com/item?id=22512196) as a microVM solution:\n\n> Until now, you needed to choose between containers with fast startup times and high density, or VMs with strong hardware-virtualization-based security and workload isolation. With Firecracker, you no longer have to choose. Firecracker enables you to deploy workloads in lightweight virtual machines, called microVMs, which provide enhanced security and workload isolation over traditional VMs, while enabling the speed and resource efficiency of containers. \n\nFirecracker comes with [a paper writeup](https://www.usenix.org/conference/nsdi20/presentation/agache), here's [a more digestible breakdown](https://blog.acolyer.org/2020/03/02/firecracker/):\n\n> At the core of Firecracker therefore is a new VMM that uses the Linux Kernel’s KVM infrastructure to provide minimal virtual machines (MicroVMs), that support modern Linux hosts, and Linux and OSv guests. It’s about 50kloc of Rust – i.e. a significantly smaller code footprint, and in a safe language. Wherever possible Firecracker makes use of components already built into Linux (e.g. for block IO, process scheduling and memory management, and the TUN/TAP virtual network interfaces). By targeting container and serverless workloads, Firecracker needs to support only a limited number of emulated devices, many less than QEMU (e.g. , no support for USB, video, and audio devices). virtio is used for network and block devices. Firecracker devices offer built-in rate limiters sufficient for AWS’ needs, although still considerably less flexible than Linux cgroups.\n\n## Further reading\n\n- [Bryan Cantrill on a more detailed history of Containers](https://www.youtube.com/watch?v=xXWaECk9XqM&list=WL&index=3&t=307s)\n- https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/"
  },
  {
    "slug": "starting-dev-community-meetup",
    "data": {
      "title": "Starting a Developer Meetup Community",
      "description": "Thoughts on how I am doing with 4 months of Svelte Society in the bag.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\nI have just posted [the full video stream of Svelte Society 4](https://www.youtube.com/watch?v=fa8GRqDkAWQ). This marks 4 months of running my first meetup with the help of community members and I thought it would be a good opportunity for reflection.\n\n## Why Community is Important\n\nI've written before about the importance of [scaling coding communities](https://www.swyx.io/writing/scaling-coding-communities/) - and recently chatted on [JAMstack Radio](https://www.heavybit.com/library/podcasts/jamstack-radio/ep-51-community-over-code-with-shawn-wang-of-netlify/) about how this applies to Svelte and why you have to scale Community to scale Code. It's also a nice way to build an IRL friend base - there are always things people can talk about in person, that they don't online.\n\n## Impetus\n\n**It takes a very special kind of masochist to be an event organizer.** I am NOT that person. I have never hosted a dinner party at home. I don't do housewarmings. I don't even have birthday celebrations! The social anxiety would be way more stress than would be worth it.\n\nHowever in Sept 2019 I found out that there would be a Svelte London meetup, the first Svelte meetup in the world. This felt wrong to me - Rich Harris lives in NYC and there was no meetup here!\n\nI am a sucker for things that should happen that wouldn't happen without my help.\n\nSo I asked Rich if he would join in and [put up a tweet](https://twitter.com/swyx/status/1176652330680426496). We had no speakers, no venue, no mailing list. Within a week [Tierney had helped us find a place at Microsoft Reactor](https://twitter.com/swyx/status/1178720682017988610) (a GREAT place for dev community meetups!) and we had put up a Meetup page and organized the full session with ~50 devs in the room! It was awesome and we showed London that NYC can get its act together too :) [Achievement unlocked](https://twitter.com/swyx/status/1179261731304022017?s=20). \n\nWe even managed to [record and upload Rich's talk](https://www.youtube.com/watch?v=uqR4I8yQaHw&t=1286s)!\n\n![sveltesociety1](https://user-images.githubusercontent.com/6764957/75637833-bf6e9800-5bf6-11ea-8fcc-520cf348e0de.png)\n\n## Components of a Great Meetup\n\nWe didn't have a lot of the niceties a well organized group might have. Here are the sorts of things that people might expect out of a well organized meetup:\n\n- Code of Conduct\n- Nice Venue\n- Food\n- Logo\n- Twitter account\n- Youtube channel\n- Twitch stream\n- Meetup Page\n- Steady stream of Spaekers\n- Active and Diverse Organizer Group\n- Returning and Diverse Attendee Base\n- Sponsors?\n- Afterparty?\n\nDecent planning would involve having worked out the schedule and responsibilities for the next 1-2 months by the time you're hosting the current meetup.\n\nIn the lead-up to the event and on the day itself you have all these roles:\n\n- Marketing the event\n- Venue liason\n- Sponsor liason\n- Reception/Signup people\n- Speaker wrangler\n- Backup speakers\n- Speakers\n- Emcee\n- AV person\n- Social chair?\n\nAll this, and you haven't really even nailed the value proposition of why people should come to, speak at, sponsor, watch online and return to your meetup. These could be:\n\n- To meet fellow devs\n- To learn new things\n- To get a job\n- To hire developers\n- To practice public speaking\n- To record material for future reference\n- ??? i'm sure there are more\n\n## Choices\n\nI've spent about 3 years going to pretty much every webdev meetup in New York so I have some sense of how these things go. For the Svelte meetup I chose to take inspiration from the [Elm NYC](https://www.meetup.com/Elm-NYC/) meetup which does lightning talks and then a hack/chat hour. This is because most attendees do not use Svelte at work and are coming to learn, and so I figured this format would be appropriate. I also adopted [Orta's TypeScript NYC format](https://www.meetup.com/TypeScriptNYC/) of having beginner, intermediate, and advanced content so that it is inclusive of people at all levels.\n\nI also [dislike the bland pizza served at every meetup ever](https://twitter.com/swyx/status/1214272306987589633) so I just got rid of both food and the need to sponsor food. However we have had food sponsorship from [Myles Borins of Google](https://twitter.com/MylesBorins) before and we went for Tacos/Burritos. I also observe that people simply dont eat before the meetup and end up starving after so I dont know how much I will stick to this principle.\n\nI made sure to open up to anybody who wanted to co-organize early on and prioritize other emcees than me doing things.\n\nI also chose to open up the idea of franchising the meetup to other countries, which [Kevin from Sweden](https://twitter.com/kevmodrome) has handily picked up the baton to start Svelte Society Stockholm.\n\n## To Pay or Not to Pay\n\nFree meetups have a notorious flake rate. In NYC you have between 30-50% of signups actually showing up. I've seen some meetups charge a nominal $5 and still have a decent showing. I might do that in future, but worry that people won't pay for a meetup that doesn't directly have to do with work.\n\nWe gathered 114 people on Meetup but of course ran into the [Meetup $2 RSVP Plan](https://www.theverge.com/2019/10/15/20893343/meetup-users-furious-new-rsvp-payment-test) and said \"F that\" and migrated to DownToMeet. Hopefully we are done with community churn for now.\n\n![image](https://user-images.githubusercontent.com/6764957/75637865-f644ae00-5bf6-11ea-8037-1d70b6ba66ab.png)\n\n\n## Fun\n\nIt's not all about throwing a mini-conference every month. There should be some social elements and some pure nonsensical entertainment. I've seen some meetups do this really well, but I'm a more serious type of person for whom this doesn't quite come naturally. I do think [kahoot.it](https://kahoot.it) live quizzes can be an easy win.\n\n## It's Hard\n\nA lot of meetups have company backing - [GitNation](https://gitnation.org/) notably has a conscious strategy to grow its conference series by seeding the ground with meetups. But even with company backing, organizers are usually staying out late, missing out on personal and family time. It's really hard.\n\nThe only reason this meetup has even survived or got attention is the gracious participation of Rich and other organizers, as well as folks who have pitched in with venue and even sponsorship offers. Community Meetups need that to survive.\n\nBut it also needs organizational grunt work. For me, I found myself only working on the meetup pretty much 1 week before every meetup, because as the Directly Reponsible Individual the event wouldn't run without me. I haven't figured out how to remove myself from this chain yet - also because nobody else views this as a high priority for them so I am kind of stuck with it. However I've been fortunate to have [Vadim](https://twitter.com/aswellasyouare) and [Sandy](https://twitter.com/sguberting) help with organizing and emceeing duties, and especially Vadim lending his design skills has been extremely clutch for our online branding™️.\n\n![https://pbs.twimg.com/profile_banners/1176969867733479424/1578437608/1500x500](https://pbs.twimg.com/profile_banners/1176969867733479424/1578437608/1500x500)\n\nWe have been lucky so far on the speaker front. I haven't had to beg anyone - people like [Jeff Posnick](https://twitter.com/jeffposnick?lang=en) and [Michael Adam Berry](https://twitter.com/_Adam_Berry) and just get in touch and I put them on. Me and Rich also serve as backup speakers who give talks at the drop of a dime. I suspect we will have dry spells in future and will have to deal with it as it comes.\n\n\n## Franchising\n\nThe Franchising idea - having a global meetup series share social media accounts, branding, and other organizer assets - has not taken off largely due to my inattention to it. I need to either give it more time or get someone else to run it.\n\nIt was largely inspired by [Vue Vixens](https://www.vuevixens.org/) and [Django Girls](https://djangogirls.org/) which serve as great communities and onramps to their respective frameworks. And of course [JSConf](https://jsconf.com/) is the OG of franchising.\n\nI'm just really struggling on this front. I don't know how this works, I don't have good ideas for it, and I don't have people to bounce ideas off of, and frankly I don't have the time or commitment needed to make this fly. yet.\n\n## Online Footprint is Important\n\nWe started off with about 50 people, and now are steady state around 20-30 devs per meetup. I skimp on a lot of things but I don't skimp on the video - because online, talks can get 200-2000 views from people who simply aren't in town or couldn't attend in person. \n\nI expect it will grow more important as virus concerns increase this year.\n\nThe Online Footprint kills so many stones - it spreads content, serves as marketing, and an incentive for speakers to speak.\n\nI can't stress this enough - **if you have a good online presence, you can singlehandedly put your city on the global map as a desirable tech destination.** I have a great desire to visit Wroclaw as a dev destination pretty much because of [Karol Majewski](v) and [WrocTypeScript](https://twitter.com/wroctypescript?lang=en)'s high quality content.\n\n## You Need A Michael Cheng or Roman Ilyushin\n\nI dread editing and putting up the videos. I spent about 6 hours doing that today. It's important but boy is it costly.\n\nBasically it should be somebody's job, and the less editing you must do the better.\n\n[Roman Ilyushin](https://twitter.com/RomaDotDev) performs this role for the NYC JS meetups. He brings all the video and audio equipment, does live editing of them together, and ships them off to YouTube before you've reached home. \n\n[Michael Cheng](https://twitter.com/coderkungfu?lang=en) does this on a grand scale in Singapore. He not only does many events himself, but also trains the [Engineers.sg](https://engineers.sg/) community in Singapore to provide free, fast, professional quality video recording for **every tech meetup in Singapore.**\n\nI don't think it is sustainable for me to keep editing videos. Sooner or later I'm going to drop the ball. I would like to pay somebody to do this.\n\n## Future Resolutions\n\nI would really like to get more organizers and more diverse organizers for Svelte Society.\n\nI would like to sustainably get franchising working for Svelte Society.\n\nI'm thinking of hiring a video editor to help, or otherwise getting a \"AV person\".\n\nThorsten from Stripe Singapore also pointed me to [Get Together](https://gettogetherbook.com/), a book and podcast about growing communities, which I would like to work through more.\n\n## References\n\nSome high quality meetups I've come across that you can use for reference:\n\n- https://js.la/\n- https://usereact.nyc/\n- https://www.realworldreact.com/\n- https://www.vuevixens.org/\n- https://www.meetup.com/ReactNYC/\n- https://www.meetup.com/React-Advanced/\n- https://www.meetup.com/ReactJS-Girls-London/\n- https://www.meetup.com/sfpython/\n- https://reactknowledgeable.org/\n- https://juniordev.sg/\n\nAre there any other \"meetup organizer help\" resources and tips you know? Please shout at me."
  },
  {
    "slug": "first-principles-approach",
    "data": {
      "title": "How I Approach First Principles Thinking",
      "description": "An explanation of First Principles via comparing Inductive vs Deductive Reasoning, and thoughts on Applications",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\n> An expanded version of this essay is [available in my book](https://www.swyx.io/writing/i-m-writing-a-book-45a8/).\n\nIn a recent conversation I had with someone I admire, they commented that they really liked my way of explaining things from first principles. I was quite struck by this in 3 ways:\n\n1. I wasn't actually consciously doing it \n2. It explained things I did well\n3. That they noticed and valued it\n\nI think it is worth introspecting on why First Principles Thinking works and how to do it, so I can consciously try to do it better.\n\n## First Principles Thinking\n\nFirst Principles Thinking (FPT) is starting from unequivocal base facts and building up toward some vision or explanation of reality. It involves reasoning by deduction rather than by analogy or appeal to authority. It's been called the [Dumbest Thing Smart People Do](https://medium.com/@byrnehobart/reasoning-from-first-principles-the-dumbest-thing-smart-people-do-46ff1cbba867).\n\nIronically, plenty of explainers ([1](https://fs.blog/2018/04/first-principles/), [2](https://jamesclear.com/first-principles), [3](https://blog.useproof.com/first-principles/)) on FPT immediately appeal to authority like [Naval](https://twitter.com/naval/status/944303595771412480?s=20) or [Charlie Munger](https://fs.blog/2018/04/first-principles/) or [Elon Musk](https://www.youtube.com/watch?v=NV3sBlRgzTI). Elon probably has done the most to popularize it in recent memory, but if you must be sold on basis of authority then you haven't really got the point of FPT.\n\nTo embrace FPT, you need to understand the philosophy of logic and epistemology.\n\n## Logic\n\n> Logic is the analysis and appraisal of arguments.\n\nI first encountered this via a philosophy lecturer I had in junior college, Lionel Barnard (who was actually supposed to teach Economics, but preferred to turn our class into [a little PPE program](http://www.ox.ac.uk/admissions/undergraduate/courses-listing/philosophy-politics-and-economics) for our intellectual gratification). In particular I have always favored the format of [Syllogism](https://en.wikipedia.org/wiki/Syllogism), which takes a form like this:\n\n- 1. All men are mortal.\n- 2. All Greeks are men.\n- ∴ All Greeks are mortal.\n\nThat `∴` symbol denotes a logical and inevitable-to-the-point-of-truism consequence of the first two propositions. This underlies a lot of how proofs are done in Math, and the other basic sciences - take N facts, put them together, derive a new, more useful fact that is as real as those other facts.\n\nYou can't get very far if you only rely on facts, though, because there are many more unknowns and indeterminate or stochastic processes than there are facts. So you can supplant your syllogisms with [Axioms](https://en.wikipedia.org/wiki/Axiom) - assumptions that you take to be true. You don't HAVE to prove them true, but you can show by deduction that given an acknowledged set of assumptions, you can arrive at a logically sound conclusion. *This is FANTASTIC*, because it lets you enumerate your beliefs, and allows you to change your mind instantly if your assumptions are proven wrong (especially handy because you [can't prove a negative](https://en.wikipedia.org/wiki/Evidence_of_absence)).\n\nSo that's great - who would argue against Logic? \n\nIt turns out that logical deduction has limits, and in fact takes a lot more effort (in corralling facts - sometimes [your facts are actually lies, hence the reproducability crisis](https://en.wikipedia.org/wiki/Replication_crisis) - and validating the integrity of the logical chain of arguments), and by far the more prevalent method we operate on is Induction. We study this in Epistemology.\n\n## Epistemology\n\n> The study of the nature of knowledge, justification, and the rationality of belief.\n\nEpistemology (pardon the jargon) address the question of **how we know what we think we know**? (*Apologies in advance to readers who actually have studied philosophy if I mangle things*)\n\nThere are many approaches to the study of [Certainty](https://en.wikipedia.org/wiki/Certainty) so I will blithely ignore most of them and contrast two: **Induction vs Deduction**.\n\nI have already presented **deductive reasoning** above - given a base set of general facts, we build up as high as we can to useful general conclusions.\n\n**Inductive reasoning** is the opposite - taking specific real world observations and generalizing them to general truths. This is problematic in SO many ways, and we have known this for [hundreds of years](https://en.wikipedia.org/wiki/Problem_of_induction), but yet this is how most of us conduct our businesses, lives, and core belief systems.\n\nThis is how insidiously persuasive induction is: You may laugh at someone taking a single anecdote and generalizing it to everybody. But this is philosophically only slightly worse than taking a population survey and generalizing, which is again only slightly worse than looking *outward* and seeing what else exists and inferring that that is all that *can* exist (the proverbial [frog in the well](https://www.nyrb.com/products/the-frog-in-the-well)). *We do this every day*. We dress it up in statistics and numbers to make it feel more truthy. We call ourselves things like *empirical* or *data-driven*. We look at someone's resume for assessing capability to do a job - unobjectionable at first glance, until you see how much people's opinion changes when \"A went to Harvard\" or \"B was the guy who took X from 10-100m in ARR\", implicitly projecting that that transfers. When hiring for a thing (say, a job in React) we overweight people who have done the thing (a previous job in React) over people who *could* do the thing (anyone with extensive experience in JavaScript/webdev). We keep up on news and trends and competitors and neighbors and peers and celebrities and friends and family and that represents our reality. And rarely [question its nature](https://twitter.com/hbo/status/783075652060663808?lang=en) and limits.\n\nInduction and Deduction are not on equal footing. As a rule, induction datapoints are far more readily available, while deductive facts are far more timeless yet costly to pin down. Humans are GREAT at [backfitting/rationalizing](https://en.wikipedia.org/wiki/Rationalization_(psychology)) from present datapoints, but terrible at examining the basis of their beliefs.\n\n> Sorry... can you tell I feel strongly about this? 😅\n\nAddressing your Epistemology head-on is supremely important for decision-making - because if you don't know how you know what you know then how do you have any faith in the decisions you make based on that knowledge? \n\nBut it is also important tool for explaining mental models - because you can either fill your explanations with a bunch of incidental junk complexity, or you can start from a bunch of irrefutable base facts and build up to something more significant in its presentation.\n\nIf you aren't impressed by this and think this is all faffy mumbo jumbo, let me put it to you this way:\n\n**People who reason by analogy are illogical**.\n\n## Applications\n\nMy talk on [Getting Closure on Hooks](https://www.swyx.io/speaking/react-hooks/) was cited as a \"First Principles\" talk, as is the [followup on Concurrent React](https://www.swyx.io/speaking/react-from-scratch). I think all talks and blogposts in the *From Scratch* genre, like [this on React Router](https://tylermcginnis.com/build-your-own-react-router-v4/) or [this on Redux](https://egghead.io/courses/getting-started-with-redux) or [this on the hardware-software interface](https://www.nand2tetris.org/) is great First Principles fodder. There's even an entire repo on [GitHub for Build Your Own X projects!](https://github.com/danistefanovic/build-your-own-x)\n\nBut of course, First Principles can be applied far beyond code. A good general set of first principles for any system is [now in vogue as Systems Thinking](http://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/), quoting Donella Meadows:\n\n```\nPLACES TO INTERVENE IN A SYSTEM\n(in increasing order of effectiveness)\n\n12. Constants, parameters, numbers (such as subsidies, taxes, standards).\n11. The sizes of buffers and other stabilizing stocks, relative to their flows.\n10. The structure of material stocks and flows (such as transport networks, population age structures).\n9. The lengths of delays, relative to the rate of system change.\n8. The strength of negative feedback loops, relative to the impacts they are trying to correct against.\n7. The gain around driving positive feedback loops.\n6. The structure of information flows (who does and does not have access to information).\n5. The rules of the system (such as incentives, punishments, constraints).\n4. The power to add, change, evolve, or self-organize system structure.\n3. The goals of the system.\n2. The mindset or paradigm out of which the system — its goals, structure, rules, delays, parameters — arises.\n1. The power to transcend paradigms.\n```\n\nI have in my head a set of other questions I also like asking:\n\n- **What do people really want?** Usually it is something simple, but not easy. Maslow's Hierarchy is a good reference, as is the list of Things You Can't Buy:\n  - Freedom\n  - Time\n  - Health\n  - Wealth\n  - Family\n  - Purpose\n- **What are the physical limits?** For example, the speed of light, but also you can do thought experiments on [the limits of Moore's Law vs Wirth's Law](https://www.swyx.io/writing/collapsing-layers/) or use [Big O notation to model scaling factors of Personal Growth](https://www.swyx.io/writing/big-l-notation/)\n- **What are the units of output?** Related to systems thinking, but this is a reductive method that has been very helpful in taking the hype out of tech startups and reducing Uber and AirBnb to driver miles and room nights.\n- **What is your success metric?** How do you know if you succeeded? Starting with the End in Mind is a [good habit for productivity](https://www.franklincovey.com/the-7-habits/habit-2.html) but [Christine Yen likes to ask this to help guide what we should do](https://youtu.be/VA0b6v9vaEM)\n- **Who cares?** [Don Valentine's](https://www.sequoiacap.com/article/remembering-don-valentine/) cryptic [two word](https://www.swyx.io/writing/two-words) test on who the end customer really is and how much they care.\n- **What are the rules of the game, and how are they changing?** When you play any game, you first find out the current rules for winning, but you must also find out how the rules are changing in order to win in future. This is also called [Game vs Metagame](https://www.swyx.io/writing/learn-in-private/).\n\n## Further Reading\n\n- https://github.com/sirupsen/napkin-math\n- http://boringtechnology.club/#23\n- https://waitbutwhy.com/2015/11/the-cook-and-the-chef-musks-secret-sauce.html\n- https://neilkakkar.com/A-framework-for-First-Principles-Thinking.html\n- http://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/\n- http://www.lihaoyi.com/post/FromfirstprinciplesWhyIbetonScalajs.html\n\n"
  },
  {
    "slug": "react-query-miragejs-crud",
    "data": {
      "title": "Mocking and Using CRUD APIs with MirageJS and React-Query",
      "description": "This is how to pair two newcomers on the React scene for mocking and using CRUD APIs, for a great developer experience.",
      "tag_list": [
        "react",
        "testing"
      ]
    },
    "content": "\nTwo recent releases have advanced the state of the art for mocking and using APIs in React apps. I think they pair very well and this post describes what they do and how to use them together. I don't have the space or time to go into the full features of each, which are the actual reasons they deliver great value - those are food for future posts.\n\n## Demo\n\nThis code runs best in development (since the focus is mocking APIs for development): https://github.com/sw-yx/react-query-miragejs-demo\n\n![reactquery2](https://user-images.githubusercontent.com/6764957/75601245-6cc29e00-5a87-11ea-80ab-1602c885f26b.gif)\n\n\n## Mirage JS\n\n[Mirage JS](https://miragejs.com/) describes itself as *an API mocking library that lets you build, test and share a complete working JavaScript application without having to rely on any backend services.*\n\nIt was previously used in the Ember ecosystem, and has recently been split out to be a general purpose framework agnostic API mocking tool. Here is how you install it:\n\n```bash\nyarn add --dev miragejs\n```\n\nIf you care about TypeScript, you can check out https://github.com/zoltan-nz/miragejs/blob/master/types/index.d.ts, however I had some trouble actually using it.\n\nHere's how it [breaks down its concepts in the Intro](https://miragejs.com/docs/getting-started/introduction/):\n\n- routes to handle HTTP requests\n- a database and models for storing data and defining relationships\n- factories and fixtures for stubbing data, and\n- serializers for formatting HTTP responses\n\nThese are all things I've had to write for testing - now there's a proper framework that does this for testing AND for local dev!\n\n### Setting up a basic API\n\nNow let's set up a basic React app that magically responds to an API while in development:\n\n\n```tsx\n// index.js\nimport React from \"react\";\nimport ReactDOM from \"react-dom\";\nimport App from \"./App\";\nimport \"./index.css\";\nimport { Server } from 'miragejs';\n\nnew Server({\n  routes() {\n    this.namespace = 'api';\n\n    this.get('/movies', () => {\n      return [\n          { id: 1, name: 'Inception', year: 2010 },\n          { id: 2, name: 'Interstellar', year: 2014 },\n          { id: 3, name: 'Dunkirk', year: 2017 }\n      ]\n    });\n  }\n});\n\nReactDOM.render(<App />, document.getElementById(\"app\"));\n```\n\nand we can use freely ping it from our frontend:\n\n```ts\nimport React from 'react';\n\nexport default function App() {\n  const [data, setData] = React.useState(null);\n  React.useEffect(() => {\n    fetch('/api/movies')\n      .then((x) => x.json())\n      .then(setData);\n  }, []);\n  return (\n    <div>\n      <div>\n        <div>\n          {data && <pre>{JSON.stringify(data, null, 2)}</pre>}\n        </div>\n      </div>\n    </div>\n  );\n}\n```\n\n![reactquery1](https://user-images.githubusercontent.com/6764957/75600983-70a0f100-5a84-11ea-8430-a6a64ddec3b0.gif)\n\n\nWow. It just works despite not actually having a backend to ping!\n\n## React Query\n\nReact Query describes itself as *\"Hooks for fetching, caching and updating asynchronous data in React\"*. If this makes you think of React-async or Redux-thunk, you're thinking at respectively too low and too high levels of abstraction. I'll expand on this in a future blogpost.\n\n\n```bash\nyarn add react-query\n```\n\nas at time of writing, TypeScript types were only matching the v0.3 API, but there were some APIs that were changed for the v1.0 launch and you can get my tweaks here: https://gist.github.com/sw-yx/1c9428a30f87f678c4fba0a2fd45a47d\n\nHere are a quick [list of it's great features from the docs](https://github.com/tannerlinsley/react-query#quick-features):\n\n- Transport/protocol/backend agnostic data fetching (REST, GraphQL, promises, whatever!)\n- Auto Caching + Refetching (stale-while-revalidate, Window Refocus, Polling/Realtime)\n- Parallel + Dependent Queries\n- Mutations + Reactive Query Refetching\n- Multi-layer Cache + Automatic Garbage Collection\n- Paginated + Cursor-based Queries\n- Load-More + Infinite Scroll Queries w/ Scroll Recovery\n- Request Cancellation\n- React Suspense + Fetch-As-You-Render Query Prefetching\n\nAlright. How does React-Query change how we data fetch?\n\n\n```tsx\nimport React from 'react';\nimport { useQuery } from 'react-query';\n\ntype Data = { id: number; name: string; year: number };\nexport default function App() {\n  const { status, data, error } = useQuery<Data[], any>('movies', () =>\n    fetch('/api/movies').then((x) => x.json())\n  );\n  return (\n    <div>\n      <div>\n        <div>{status}</div>\n        {error && <div>{error}</div>}\n        <div>\n          {status === 'loading' ? (\n            <span>Loading...</span>\n          ) : status === 'error' ? (\n            <span>Error: {error!.message}</span>\n          ) : (\n            <ul>\n              {data!.map((movie) => (\n                <li key={movie.id}>\n                  {movie.name} ({movie.year})\n                </li>\n              ))}\n            </ul>\n          )}\n        </div>\n      </div>\n    </div>\n  );\n}\n```\n\nWow, so everything becomes a good deal more declarative and loading and error states are deal with for us. Great! exactly like [react-async](https://github.com/async-library/react-async).\n\n## Mocking CRUD with Mirage JS\n\nMirage doesn't just spit back static data. You can simulate latency and CRUD to a pretty high fidelity! Let's evolve our mocking to show todo list:\n\n\n```tsx\n// etc..\nimport { Server, Model } from 'miragejs';\nnew Server({\n    models: {\n      todo: Model\n    },\n    seeds(server) {\n      server.create('todo', { text: 'Learn Mirage' } as any);\n      server.create('todo', { text: 'Shake it off', isDone: true } as any);\n      server.create('todo', { text: 'Profit' } as any);\n    },\n    routes() {\n      this.namespace = 'api';\n      this.timing = 750;\n      this.get('/todos', (schema: any) => {\n        return schema.todos.all(); // persistent even after navigating away\n      });\n      this.post('/todos', (schema: any, request) => {\n        const attrs = JSON.parse(request.requestBody);\n        return schema.todos.create(attrs);\n      });\n      this.patch('/todos/:id', (schema, request) => {\n        let todo = JSON.parse(request.requestBody);\n        return schema.db.todos.update(todo.id, todo);\n      });\n    }\n  });\n\nReactDOM.render(\n  <Router><App /></Router>, document.getElementById(\"app\"));\n```\n\nSo it offers some helpers to do Create and Update (patch). I didn't bother to implement Delete but you get the picture.\n\nYou can now build a frontend against your Mirage-mocked API:\n\n\n```tsx\nimport React, { useState } from 'react';\nimport { useQuery } from 'react-query';\n// https://github.com/miragejs/react-demo/blob/master/src/components/Todos.js\ntype TodoType = {\n  text: string,\n  isDone: boolean,\n  id?: string\n}\n\nexport default function Todos() {\n  const { status, data, refetch } = useQuery<TodoType[], any>('todos', () =>\n    fetch('/api/todos')\n      .then((res) => res.json())\n      .then((json) => json.todos)\n  );\n  let todos = data || []\n  let done = todos.filter((todo) => todo.isDone).length;\n\n  async function createTodo(event: React.FormEvent<HTMLFormElement>) {\n    event.preventDefault();\n    const textField = event.target['newTodoName'];\n    \n    await fetch('/api/todos', {\n      method: 'POST',\n      body: JSON.stringify({ text: textField.value })\n    })\n      .then((res) => res.json())\n      .then(refetch)\n      .then(() => void(textField.value = ''));\n  }\n\n  async function saveTodo(todo: TodoType) {\n    await fetch(`/api/todos/${todo.id}`, {\n      method: 'PATCH',\n      body: JSON.stringify(todo)\n    }).then(() => refetch())\n  }\n\n\n  // console.log({ todos });\n  return (\n    <div className='max-w-sm px-4 py-6 mx-auto bg-white rounded shadow-lg'>\n      <div className='flex items-center justify-between px-3'>\n        <h1 className='text-2xl font-bold'>Todos</h1>\n\n        <div className='text-blue-500'>\n          {status === 'loading' && (\n            <svg\n              className='w-4 h-4 fill-current'\n              viewBox='0 0 20 20'\n              data-testid='saving'\n            >\n              <path d='M16.88 9.1A4 4 0 0 1 16 17H5a5 5 0 0 1-1-9.9V7a3 3 0 0 1 4.52-2.59A4.98 4.98 0 0 1 17 8c0 .38-.04.74-.12 1.1z' />\n            </svg>\n          )}\n        </div>\n      </div>\n\n      <div className='mt-6'>\n        {status === 'loading' ? (\n          <p className='px-3 text-gray-500' data-testid='loading'>\n            Loading...\n          </p>\n        ) : (\n          <div>\n            <div className='px-3'>\n              <form onSubmit={createTodo} data-testid='new-todo-form'>\n                <input\n                  type='text'\n                  name=\"newTodoName\"\n                  placeholder='New todo'\n                  className='block w-full px-3 py-2 placeholder-gray-500 bg-white rounded shadow focus:outline-none'\n                />\n              </form>\n            </div>\n\n            {todos.length > 0 ? (\n              <ul className='mt-8'>\n                {todos.map((todo) => (\n                  <Todo todo={todo} onChange={() => saveTodo(todo)} key={todo.id} />\n                ))}\n              </ul>\n            ) : (\n              <p\n                className='px-3 mt-16 text-lg text-center text-gray-500'\n                data-testid='no-todos'\n              >\n                Everything's done!\n              </p>\n            )}\n\n            <div className='flex justify-between px-3 mt-12 text-sm font-medium text-gray-500'>\n              {todos.length > 0 ? (\n                <p>\n                  {done} / {todos.length} complete\n                </p>\n              ) : null}\n              {/* {done > 0 ? (\n                <button\n                  onClick={deleteCompleted}\n                  className='font-medium text-blue-500 focus:outline-none focus:text-blue-300'\n                >\n                  Clear completed\n                </button>\n              ) : null} */}\n            </div>\n          </div>\n        )}\n      </div>\n    </div>\n  );\n}\n\n\nfunction Todo({\n  todo,\n  onChange\n}: {\n  todo: TodoType;\n  onChange: ((event: React.ChangeEvent<HTMLInputElement>) => void) | undefined;\n}) {\n  let [isFocused, setIsFocused] = useState(false);\n  const handleSubmit = () => {\n    console.log('handleSubmit')\n    // onChange()\n  }\n  return (\n    <li\n      className={`\n        my-1 rounded focus:bg-white border-2 flex items-center relative\n        ${isFocused ? 'bg-white border-gray-300' : ''}\n        ${!isFocused ? 'border-transparent hover:bg-gray-200' : ''}\n        ${!isFocused && todo.isDone ? 'opacity-50' : ''}\n      `}\n      data-testid='todo'\n    >\n      <input\n        type='checkbox'\n        checked={todo.isDone}\n        onChange={onChange}\n        className='ml-2'\n      />\n\n      <form onSubmit={handleSubmit} className='relative w-full'>\n        <input\n          type='text'\n          value={todo.text}\n          onChange={onChange}\n          placeholder='New Todo'\n          onFocus={() => setIsFocused(true)}\n          onBlur={onChange}\n          className={`\n            bg-transparent focus:outline-none px-3 py-1 block w-full\n            ${todo.isDone && !isFocused ? 'line-through' : ''}\n          `}\n        />\n      </form>\n    </li>\n  );\n}\n```\n\nAnd we get:\n\n![reactquery2](https://user-images.githubusercontent.com/6764957/75601245-6cc29e00-5a87-11ea-80ab-1602c885f26b.gif)\n\n\nWell, that was 166 lines of code without even implementing async state tracking. Can we do better? \n\n## Making CRUD with React-Query\n\nSimilar to how the GraphQL world thinks about reading and interacting with data, you can do CRUD with `useMutation` of React Query. Let's change `createTodo` to use it:\n\n```tsx\n  const [postTodo, { status: postStatus }] = useMutation(async (value) =>\n    fetch('/api/todos', {\n      method: 'POST',\n      body: JSON.stringify(value)\n    })\n      .then((res) => res.json())\n      .then(refetch)\n  );\n  async function createTodo(event: React.FormEvent<HTMLFormElement>) {\n    event.preventDefault();\n    const textField = event.target['newTodoName'];\n\n    await postTodo({ text: textField.value }).then(\n      () => void (textField.value = '')\n    );\n  }\n```\n\nThat's great, but what did we really gain from the rewrite? Well, we get acces to all these other handy APIs:\n\n```tsx\nconst [mutate, { status, data, error }] = useMutation(mutationFn, {\n  onSuccess,\n  onSettled,\n  onError,\n  throwOnError,\n  useErrorBoundary,\n})\n\nconst promise = mutate(variables, {\n  onSuccess,\n  onSettled,\n  onError,\n  throwOnError,\n})\n```\n\nThis is super handy for controlling where to relay async status to your UI, and also to add callbacks for when certain events happen.\n\nThis callback stuff is so handy that I can even move my refetch code in there:\n\n```tsx\n  const [postTodo, { status: postStatus }] = useMutation(\n    async (value) =>\n      fetch('/api/todos', {\n        method: 'POST',\n        body: JSON.stringify(value)\n      })\n        .then((res) => res.json())\n        .then(refetch),\n    {\n      onSuccess: () => {\n        queryCache.refetchQueries('todos');\n        // other cache invalidation queries and state updates\n      }\n    }\n  );\n```\n\n\n## Conclusion\n\nIf you liked this, let me know what else I should explore, as I think I am only scratching the surface with what's possible with both libraries. But all in all, this is a pretty powerful pairing of tools to rapidly create CRUD frontends in React. "
  },
  {
    "slug": "android-vs-ios",
    "data": {
      "title": "Switching to Android after 13 years of iOS",
      "description": "I have used iPhone/iOS for the longest time. Last month I switched to OneTouch 6T/Android. Here are my impressions.",
      "tag_list": [
        "reflections"
      ]
    },
    "content": "\n- *Canonical URL: https://www.swyx.io/writing/android-vs-ios/*\n- *Epistemic effort: I have used Android daily for a month. Collected brief notes to self whenever I experienced delight or dismay.*\n\nI have used iPhone/iOS for the longest time. Last month I switched to OnePlus 6T/Android. Here are my impressions.\n\n## Methodology\n\nFrom a user perspective, it is unwise to compare purely based on hardware. I was switching from a $700 iPhone XR to a $300 OnePlus 6T - yes, there are differences in camera quality and chipset so on, but more importantly, the operating systems come into the mix as well as the default browser experiences that ship with each. I decided to evaluate my experience based on these defaults, since the overall experience is what matters.\n\nI have used iPhone since it was released in 2007, so the Android bears a significant disadvantage from my unfamiliarity (I may just be used to iOS idioms and see things as a negative) and ignorance (of how to mitigate any gripes I may have). I can't pretend to be neutral, but who can?\n\nWhere possible, I will list factors in terms of positives, since a positive for one ecosystem basically implies a negative for the other in relative terms. No sense in duplication.\n\n## Android\n\n- **Price**: dollar for dollar, you get better spec hardware on Android phones. I whittled my choices down to the Pixel 3A vs OneTouch, and went with OneTouch because I will trade off camera quality for faster processors.\n- **USB C**: charging means I can charge with the same charger as my (*ahem*) MacBook Pro.\n- **NFC/Google Pay**: Just Works™! Especially when walking up to the NYC Subway turnstiles, and slapping your phone on it to enter, without needing to pause to open up any app or pass on the phone. Magic. I do worry about security since it seems *too* easy...\n- **Real Filesystem**: I can **DOWNLOAD FILES**! *gasp*. I had to share some PDFs and MP3s and downloading from browser and opening up in another app (or browser) was a cinch. Good luck doing that in iOS without proper sharesheets.\n- **Captive Portal Wifi Signin**: This also works seamlessly! I don't exactly know why but it works seamlessly on Android whereas it regularly failed to cachebust in iOS.\n\n![https://4.bp.blogspot.com/-dw9MiaM5GmE/VaFd4t_QmZI/AAAAAAAAFuM/OTMfLTcdUeA/s320/starbucks.png](https://4.bp.blogspot.com/-dw9MiaM5GmE/VaFd4t_QmZI/AAAAAAAAFuM/OTMfLTcdUeA/s320/starbucks.png)\n\n- **Google Calendar**: I found the syncing of events on the Google Calendar app on Android to be much, much more reliable than the same app on iOS. I don't know the cause and it could well be to my complex 3-account setup, but my iOS GCal would regularly lie to me about my appointments and that was no good.\n- **Keyboard + Navigation**: This is a **NEGATIVE** for Android. The Navigation buttons are RIGHT NEXT to the keyboard when you type, so there is plenty of accidental triggers of navigation (and keyboard switching if you have multiple keyboard) when you type. What absolutely horrible design and almost enough to make me quit Android. Why TF are you putting the keyboard switcher right under the Enter key?\n\n![https://i.stack.imgur.com/LEgX5.png](https://i.stack.imgur.com/LEgX5.png)\n\n- **Screenshot Workflow** (minor): To screenshot, you just long-press the power button and pick the right button. This is preferable to the somewhat cramp inducing iPhone equivalent of squeeze power button + volume up.\n- **Custom Widgets** (minor): I wanted a new Weather widget to position on my home screen, I went on the app store and got it. Worked. Brilliant.\n- **Mobile Chrome browser**:\n  - I had switched to Android with the expectation of trying out the state of PWAs - but didn't find any compelling PWAs to install. In one month of daily Android usage there remains only one PWA I regularly use -Twitter - but it is because I am a developer trying to give PWAs a shot, and it still has noticeable deficiencies to the native Android app (which I also tried). This is so far a big letdown for my expectation of PWAs.\n  - Compared to iOS Safari, pages don't leave memory immediately when you navigate away. For a serial link hoarder like me, this means I can load pages in the browser, and not think about it until I have downtime like in a plane or train, and it *loads offline without reload*. This never works on iOS and was a big pain point for me.\n  - Even when not using Sign in With Google, if my email/password combos are saved to Google (eg from desktop browser), it autofills seamlessly on mobile. \n  - Better AMP link copying - I know, I know, save me your AMP rant - but we all hate when we search on Google and navigate on a page and copy a link and get the shitty AMP link rather than the real thing. I noticed that when I copy the URL of an AMP link, Mobile Chrome gives me the new link so I can paste with a clean conscience. I don't think Safari does this.\n  - Google account integration is fantastic. With the [Credential Management API](https://wccftech.com/google-chrome-one-tap-signup/), when I am logging into sites and apps, if I can use my Google account login, it pops up a nice prompt and I login with one tap. \n\n![https://cdn.wccftech.com/wp-content/uploads/2017/10/google-chrome-one-tap-sign-up-in.jpg](https://cdn.wccftech.com/wp-content/uploads/2017/10/google-chrome-one-tap-sign-up-in.jpg)\n\n\n## iOS \n\n- **Undo**: You can undo a mistake on iOS. Not on Android.\n- **Paste**: Copying and Pasting generally always works predictably on iOS. On Android I frequently have cases where I *think* I have copied something, and try to paste, only to find out I have pasted the wrong thing (and, as a plus, can't undo the paste!).\n- **Emoji keyboard**: The default emoji keyboard on iOS generally just guesses whatever you expect to find. Great. On Android you don't get emojis out of the box, and the custom keyboard I now use still doesn't work as intuitively as iOS, even accounting for my familiarity bias. (e.g. I type \"wave\" for 👋🏽 and my Android SwiftKey Keyboard gives me 🌊)\n- **Left Side Screen Real Estate**: Holds all useful notifications\n\n    ![https://www.imore.com/sites/imore.com/files/styles/xlarge/public/field/image/2016/08/how-to-home-today-view.jpeg?itok=Rkompocf](https://www.imore.com/sites/imore.com/files/styles/xlarge/public/field/image/2016/08/how-to-home-today-view.jpeg?itok=Rkompocf)\n\n    Whereas Android only offers Google search on that valuable real estate, and makes you swipe down to see notifications. You can [turn the Google App off](https://www.fonedog.com/android-toolkit/remove-google-search-bar-android.html) but that doesn't replace it with anything useful. You could probably fix this with a custom launcher but I haven't figured out one I like more than stock Android. *Update: a reader suggested Microsoft Launcher, which actually allows this and I have been enjoying so far!*\n- **App Priority**: This one is well known - because iOS has richer users, companies prioritize app development for iOS. e.g. Superhuman and GitHub develop iOS first and Android is a secondary consideration.\n- **Overcast**: Overcast is a category of its own because podcast listening is a big phone usecase for me, and Overcast is iPhone only. On Android I have been using Podcast Addict, but the design is too cluttered, it doesn't have nice detail to list flow, and Skip via headphones is not supported. Sound controls in general are better supported on any app in iOS. *EDIT: after this I found some settings in Podcast Addict that helped me resolve my Skip and sound controls issues*.\n- **Navigation**: generally just superior in every way to Android. I immediately missed \"Swipe from Edge to go Back\", which is default behavior in iOS, and [apparently available in Android 10](https://twitter.com/markdalgleish/status/1228188057616044034?s=20), but I didn't know that I would be locked to Android 9 by buying a locked OnePlus. There are gesture apps on the Play Store that seemingly purport to offer this feature but I wasn't able to get any of the free ones to work. \n- **Dark Mode**: Generally found this to be better supported on iOS. In Android there are a bunch of small places you have to dig into to properly turn everything dark.\n- **Assistive Touch**: I love this thing to toggle grayscale and invert colors as well as doing some other actions like shake phone. Android doesn't have any a11y function like this - it has a custom gesture thing, which is underpowered and failure prone.\n- **Camera**: Generally better than anything you get in Android\n- **Alarm app** (minor): The Alarm app in iOS answers to \"Alarm app\", which is great. Android doesn't think it has an alarm app - you have to look for \"Clock\" instead.\n- **iOS Safari**:\n  - You can cancel accidental link clicks by moving your thumb away\n  - Readability mode makes badly designed sites a lot more readable.\n\n## Conclusion\n\nAll in all, as you can see, it hasn't been an unmitigated win for me moving to Android. I think iOS is the better operating system, but Android has the better hardware and better browser. I also generally feel like iOS slows down your iPhone/iPad after 1-2 years in order to force you to upgrade ([the original trigger for me switching](https://twitter.com/swyx/status/1194284347446022144)) and hope that I won't experience this with Android, but don't yet have proof.\n\nI'll be sticking to Android for now, but am always eyeing my iPhone with faint jealousy."
  },
  {
    "slug": "digital-garden-tos",
    "data": {
      "title": "Digital Garden Terms of Service",
      "description": "This is my attempt to explicitly define a not-legally-binding \"terms of service\" for people who peruse Digital Gardens, and the people who Learn in Public with them. ",
      "tag_list": [
        "writing",
        "learninpublic"
      ]
    },
    "content": "\n- **Canonical URL**: *https://www.swyx.io/writing/digital-garden-tos/*\n- **Epistemic status**: *Interested in this as an experiment for myself, but happy to change, add, or prune over time.*\n- **Epistemic effort**: *Have had this in my head a while, but mostly wrote this on a flight.*\n\nThis is my attempt to explicitly define a not-legally-binding \"terms of service\" for people who peruse Digital Gardens, and the people who Learn in Public with them.\n\n![image](https://user-images.githubusercontent.com/6764957/75324412-e3bc2480-5844-11ea-90be-4a26b6a64000.png)\n\n\n## Table of Contents\n\n\n## Why We Need a Terms of Service\n\nThe [Learn In Public](https://www.swyx.io/writing/learn-in-public) movement has encouraged thousands of people to write, speak, draw, or otherwise *pick up what mentors put down*, with the end goal of lifelong [`L(N*P)` growth in personal knowledge and network](https://www.swyx.io/writing/big-l-notation/). A key part of this strategy is maintaining your own Digital Garden. \n\nA Digital Garden is your very own place (often a blog, or twitter account) to plant incomplete thoughts and disorganized notes in public - the idea being that these are evergreen things that grow as your learning does, warmed by constant attention and fueled by the unambiguous daylight of peer review.\n\nIt is in part a trick for creators to play on themselves: For perfectionists who stress over shipping anything less-than-polished and therefore never ship anything, it is a license to trade off self review for peer review and increased velocity. Many report both improved quality and quantity of output after giving themselves the permission to do this.\n\n### Potential Problems with Digital Gardens\n\nHowever, the social contract is not explicit, which causes understandable conflict because this is such a fringe movement. \n\n- We have been conditioned for decades to believe in the finality and authority of the printed word. The widely accepted Rules of Good Writing exhort writers to use active voice and drop weasel words. \n- Words can take on extra weight with context. Serif fonts and professional design can imply the journalistic standards of the New York Times, yet nobody assigns authority to words on a 4chan or Reddit anonymous discussion forum. \n- Readership also matters - it is common for people who Learn In Public to gain a large following, and there is often an expectation that \"With Great Subscriber Count Comes Great Responsibility\".\n\nQuite simply, nobody wants the Learn In Public movement to encourage a flood of overnight experts and blogspam in a content race to the bottom. However, we also don't want to discourage learners from learning.\n\n### Safe Space for Gardeners\n\nPeople with audiences do of course have some obligation to not do them a disservice, else they don't deserve that audience. However this doesn't mean that they must do exhaustive due diligence and be authoritative in every context - there needs to be space to experiment, grow, and quite frankly, be ignorant and wrong.\n\nThis need also extends to people who don't have a large audience. Even here, the social contract also needs to be more explicit. Often, people just getting started don't get the feedback they need. Experts can be very polite - they might see someone making a mistake, but not comment, because they don't want to embarrass the beginner or because unsolicited criticism can be rude. \n\nIf you keep your ego small, you can learn more by **explicitly inviting people to criticise your work**. You can even encourage feedback by proving that you are *coachable* - having a track record of acting upon feedback, correcting our notes and then following up by more clearly articulating what you got wrong for benefit of fellow beginners. This incentivises feedback by amplifying the expert's time invested in you, aided by the one thing you have that they don't: the beginner's mind.\n\n---\n\n## [Link to GitHub Repo](https://github.com/sw-yx/digital-garden-tos)\n\n[Here's the Digital Garden Terms of Service in a GitHub Repo](https://github.com/sw-yx/digital-garden-tos). Please feel free to suggest changes or fork to your liking.\n\n![image](https://user-images.githubusercontent.com/6764957/75324412-e3bc2480-5844-11ea-90be-4a26b6a64000.png)\n\n\n## For Visitors\n\nWelcome! You are now browsing a [Digital Garden](https://joelhooks.com/digital-garden). This is my personal space for [Learning In Public](https://www.swyx.io/writing/learn-in-public). I am a lifelong learner so everything is a Work-In-Progress like me, but I do not let perfectionism get in the way. That means that what you read here is not authoritative or complete, and is not representative of my best work. \n\nHowever, it is representative of my interests and current state of knowledge, and if you have the same interests, then this space is also yours to use as a reference. Feedback and social sharing is welcome - that is the whole point of being public!\n\n### 1. Right to Be Wrong\n\nI have a right to be wrong or incomplete in my Digital Garden, either due to paucity of time or knowledge. You will not hold this, or my readership, against me because I will keep learning, with your help. Everything in the Digital Garden is a living document and I will retract or rephrase things I no longer agree with.\n\n### 2. Constructive Criticism\n\nYou are expressly welcome to comment on, tear apart, counter-argue, or outright disagree on anything here. No compliment sandwich needed - I learn most from critics. I will listen to you but I don't promise to agree with you. Please also suggest what else I should include, read, watch, or listen to, or tell me what you would have written instead. \n\nBetter yet, write a better version of what I did and publish it on your own Garden. I'd love to read it.\n\n### 3. Attribute, don't Plagiarize\n\nDon't plagiarize. You're welcome to quote, with attribution and a link back here. I don't waive copyright for commercial purposes. But feel free to share ideas and riff off of them.\n\n---\n\n## For Gardeners\n\nCongratulations! You are now the proud owner of your own corner of the Internet. This is a second brain that exists outside your natural one - it is very good at remembering everything, searching and organizing facts and links, and saving you keystrokes. Your second brain is fertile ground: If you tend to it well, your Digital Garden will help you be a lifelong learner, catch you friends while you sleep, and bear fruits in unexpected ways.\n\nHowever there are some ground rules to being a responsible Digital Gardener, and some things you can do to increase the yield on your work.\n\n### 1. Consideration of Others\n\n- I will not publish private conversations or confidential information.\n- I will consider the feelings of others if I ever write negatively about something people have worked on or said. \n- I will do my best to cover my bases and check that I have not assumed incompetence or malice due to my ignorance of the full body of work. \n- I will err on the side of treating others as THEY want to be treated. \n- I will \"steelman\" arguments - the opposite of \"strawman arguments\" - instead of picking on the weakest piece of their argument, I will confront head on their best argument by seeking first to understand before trying to be understood.\n\n### 2. Epistemic Disclosure\n\n- I will report how strongly I hold my beliefs, always reserving the right to be wrong and change my mind. \n- I will report how much experience I have in the topic, by disclosing how much work I have done so far on it and linking to others who will know more.\n- I will link to further resources so that readers can discover influencing and contrasting opinions from the original source.\n\nIn the spirit of this, [the practice of disclosing epistemic status and effort originates from Devon Zuegel](https://devonzuegel.com/post/epistemic-statuses-are-lazy-and-that-is-a-good-thing). I don't think this always needs be disclosed, for example if it is obvious from context. But it never hurts.\n\n### 3. Response to Feedback \n\n- I will not get discouraged if I don't receive feedback. I plant ideas in my Digital Garden for my own use, not solely to get visitors.\n- I will reward feedback by listening and immediately correcting things I got wrong.\n- I do not promise to agree with or respond to all feedback. \n\n---\n\n## Call to Action\n\nLet us all adopt more explicit Digital Garden terms of service, and wear them proudly on our garden gates, and in so doing help a thousand Digital Gardens bloom all around the world by encouraging bold, curious, prolific gardeners, while responsibly inviting visitors to co-create and cross pollinate ideas. \n\nWith clearer boundaries there will hopefully be less infighting, more personal growth, and more great ideas - the oxygen on which our whole world relies for Progress.\n"
  },
  {
    "slug": "svelte-community-fixup",
    "data": {
      "title": "Fixing Up the Svelte Community Site",
      "description": "Adding GitHub Actions and Updating Data Dependencies",
      "tag_list": [
        "svelte"
      ]
    },
    "content": "\n*Canonical URL: https://www.swyx.io/writing/svelte-community-fixup/*\n\nOne of my longstanding projects that I'm always feeling guilty about not spending time on is the [Svelte Community site](https://svelte-community.netlify.com/code/). \n\n![image](https://user-images.githubusercontent.com/6764957/75218533-80f55b00-5768-11ea-98de-1e32a3ae01c9.png)\n\nAt it's surface it is just a simple static site with searchable links to resources, but the vision is that this would hold all the stuff that the official docs and chat would not be able to hold.\n\nObviously the Svelte maintainers are stretched thin, and I'm just one (busy) guy, so [you're welcome to swim on in to contribute](https://github.com/sveltejs/community) if you want to help grow the Svelte community!\n\nI took a couple hours to update it today and here are my public notes from what I did.\n\n## Updating the `ssg` dependency\n\nhttps://github.com/sveltejs/community/blob/master/package.json#L15\n\nI have been bumping up the ssg version as I tweak features and fix bugs, so just keeping this up to par. I anticipate biting the bullet to move off of Sapper sooner rather than later given I am now consistently running into scaling issues with it on my own site.\n\n## Updating the data and data fetching scripts\n\nhttps://github.com/sveltejs/community/commit/efc0953c5bc399b985d4d75ec9ca21bcb4ef094f\n\nWe scrape [GitHub stars and npm downloads](https://svelte-community.netlify.com/code/) to give a little indication of the maintenance and adoption status of the ecosystem, this needed to be done on a more regular basis but was written just for a onetime run, so I just ran it once.\n\n## Automating Data Scraping with GitHub Actions\n\n[I previously wrote about this](https://www.swyx.io/writing/github-scraping/) so I assumed this would be easy, but boy I wasted an hour going nuts with the unintuitive GitHub Actions UI:\n\n\n![2020-02-25 at 12 37 AM](https://user-images.githubusercontent.com/6764957/75218118-3de6b800-5767-11ea-9bb0-541c80246455.png)\n\n\nThis was because this is what happens when you add a valid GitHub Action:\n\n![image](https://user-images.githubusercontent.com/6764957/75218155-5eaf0d80-5767-11ea-9b1d-60ed56936fe2.png)\n\nThat's right, absolutely no recognition at all from GH Actions that you did something right. Ugh. Took me an hour to verify that nothing went wrong, the GH Actions syntax didnt change, I wasn't going crazy.\n\nThe other thing I found out was that you can't add an environment variable prefixed with `GITHUB_` as that is a reserved env var. Understandable, but [the docs say setting an environment variable or secret with the `GITHUB_` prefix will result in an error](https://help.github.com/en/actions/configuring-and-managing-workflows/using-environment-variables) and there was most certainly no error.\n\n## Work, Work\n\nAnyway, the fixup is done, at the cost of some hair pulling from the GH Actions debacle. There is [no manual trigger](https://github.community/t5/GitHub-Actions/GitHub-Actions-Manual-Trigger-Approvals/td-p/31504) (some hacks [here](https://dev.to/s_abderemane/manual-trigger-with-github-actions-279e) and you can [create a custom webhook event](http://www.btellez.com/posts/triggering-github-actions-with-webhooks.html) if you really need to but ugh) so the only way to tell if this thing will work is to... wait.\n\nThe remaining work to do on the Svelte Community Site is:\n\n- Add everything in Discord over the last few months\n- Make events easier to signup for and notify - wire it to a Google Sheet\n- Make the Showcase more impressive\n- Start writing Recipes\n- Look at the Vue folks for more ideas.\n\nOnce again, if you'd like to help, [you're welcome to join us](https://github.com/sveltejs/community)!"
  },
  {
    "slug": "js-tools-metrics-logs-traces",
    "data": {
      "title": "Metrics, Logs, and Traces in JavaScript Tools",
      "description": "The DX of JavaScript Developer Tools could be better if we added Metrics, Logs, and Traces",
      "tag_list": [
        "javascript",
        "node"
      ]
    },
    "content": "\n*Canonical URL: https://www.swyx.io/writing/js-tools-metrics-logs-traces/*\n\nI was listening to [the Official AWS Podcast's episode on Observability](https://aws.amazon.com/podcasts/357-deep-dive-into-observability/) and was struck by how much thought has been given towards improving tools for investigating when things go wrong. \n\nI realized that we could probably have something to learn by applying this lens to the JavaScript developer experience.\n\n> Note: This post originally framed these data types in terms of Observability - That was a mistake born out of my confusion on the topic from reading multiple sources - [Charity Majors weighed in](https://twitter.com/mipsytipsy/status/1231830827555672064?s=20) with far more info on the difference. \n>\n> To be clear, I don't claim any authority whatsoever on the topic - I'm just writing down my learning in public, and have since removed all mention of Observability from this post.\n\n## Table Of Contents\n\n*This will be autofilled by remark-toc on my site*\n\n## Data Types\n\nWe can break down the data types discussed into **metrics, logs, traces, and events**.\n\n- **Metrics**: Time series data, like CPU utilization\n- **Logs**: Structured or semistructured bits of text emitted by the application\n- **Traces**: A record of an API call that's made from one part of my application to another\n- **Events**: An indication of a state change of some type*\n\n*That last one is in a special category - we'll discuss that separately at the end.\n\nIn JavaScript we tend to just mush all this into \"stuff we console.log out\", but I think we can try to be a bit more sophisticated about it.\n\nI thought I should list out what each of these maps to in my mind given [my experience writing and teaching Node.js CLIs](https://egghead.io/courses/build-custom-cli-tooling-with-oclif-and-typescript).\n\n## JavaScript Metrics\n\n### Metric: Bundle Size\n\nWe are pretty good at Metrics in JavaScript. Of course the main one we think about in frontend is bundle size, and every bundler has this built in:\n\n![https://user-images.githubusercontent.com/2301847/37473737-63319738-2834-11e8-9c8e-0d804fd03635.png](https://user-images.githubusercontent.com/2301847/37473737-63319738-2834-11e8-9c8e-0d804fd03635.png)\n\nHowever we have all worked in situations where we ignored those warnings, and eventually too much crying wolf leads to habitual ignoring of warnings. Better to accept that most apps start from a bad place, and impose \"ratchet\" mechanisms to slowly improve things over time. \n\nAs Seb Markbage [has noted](https://twitter.com/sebmarkbage/status/1063585097545220096): \n\n> \"Tools that hold the line and don’t let things get worse are so powerful. E.g. test coverage, type coverage, bundle sizes, perf metrics, etc. Much of APIs are about how not to break something that was once tested. Under-invested in open source and business critical in Big Tech.\"\n\nFor example, [the prolific Jason Miller](https://twitter.com/_developit?lang=en) recently released [`compressed-size-action`, a GitHub action to hold the line on compressed bundle size](https://github.com/preactjs/compressed-size-action):\n\n![https://user-images.githubusercontent.com/105127/73027489-8413c900-3e01-11ea-8630-09172b247f82.png](https://user-images.githubusercontent.com/105127/73027489-8413c900-3e01-11ea-8630-09172b247f82.png)\n\n[Formidable Labs' Webpack Dashboard](https://formidable.com/open-source/webpack-dashboard/) can be a good tool to run in terminal as well:\n\n![https://i.imgur.com/qL6dXJd.png](https://i.imgur.com/qL6dXJd.png)\n\n\n### Metric: Speed\n\nEqually applicable on both frontend and backend is speed. We are fond of crapping on JS as an interpreted language, but it can often be fast enough if we avoid bad code. We want to be alert to regressions in speed, and we want to notice when our app slows down as a function of input or code size as that is predictive of future performance deterioration.\n\nParcel makes it a point to report the time it took for its work:\n\n![https://camo.githubusercontent.com/9e3c6612fe9bcd91027ba89833cd893f20592312/68747470733a2f2f692e696d6775722e636f6d2f766e704d504a322e706e67](https://camo.githubusercontent.com/9e3c6612fe9bcd91027ba89833cd893f20592312/68747470733a2f2f692e696d6775722e636f6d2f766e704d504a322e706e67)\n\nand you can [instrument Webpack to report it's own speed](https://www.npmjs.com/package/speed-measure-webpack-plugin):\n\n![https://raw.githubusercontent.com/stephencookdev/speed-measure-webpack-plugin/HEAD/preview.png](https://raw.githubusercontent.com/stephencookdev/speed-measure-webpack-plugin/HEAD/preview.png)\n\nHowever we shouldn't just be limited to bundlers to thinking about speed regressions in our code. \n\nWe can of course [generically log execution time in JavaScript](https://davidwalsh.name/console-time):\n\n```js\n// Kick off the timer\nconsole.time('testForEach');\n\n// (Do some testing of a forEach, for example)\n\n// End the timer, get the elapsed time\nconsole.timeEnd('testForEach');\n\n// 4522.303ms (or whatever time elapsed)\n```\n\nIf you're working in the browser you should use [the User Timing API instead](https://developers.google.com/web/tools/lighthouse/audits/console-time) for High-resolution timestamps, Exportable timing data, and Integration with the Chrome DevTools Timeline.\n\n![https://developers.google.com/web/tools/lighthouse/images/user-timing-measurement-in-devtools.png](https://developers.google.com/web/tools/lighthouse/images/user-timing-measurement-in-devtools.png)\n\nFor high precision alternatives, look at [`performance.now()`](https://developer.mozilla.org/en-US/docs/Web/API/Performance/now) in the browser and [`process.hrtime()`](https://blog.abelotech.com/posts/measure-execution-time-nodejs-javascript/) in Node.js.\n\nOf course, logging a bunch of things in the console is just the MVP - you will probably want to collect these timestamps and do some processing and persistence to output useful speed metrics for the end user.\n\nFor inspiration on what you can do here, check out [Brian Vaughn's progress-estimator](https://www.npmjs.com/package/progress-estimator):\n\n![https://user-images.githubusercontent.com/29597/48986949-474e2400-f0cf-11e8-86d7-d201f8ad8eca.gif](https://user-images.githubusercontent.com/29597/48986949-474e2400-f0cf-11e8-86d7-d201f8ad8eca.gif)\n\nIt lets you give an estimate, and persists execution data to adjust future estimates. You may want to be comfortable with [Temp folder creation utilities in Node.js](https://github.com/sw-yx/cli-cheatsheet/#user-content-processing) to easily accumulate this data between runs.\n\n### Other Metrics\n\nEspecially if you run production Node processes, there is a whole field of [Application Performance Management](https://en.m.wikipedia.org/wiki/Application_performance_management)/Monitoring software that you will want to look into that I (as a primarily frontend person) have no experience in - of course standard server metrics like load/response times must be measured. [Matteo Collina](https://twitter.com/matteocollina/status/1232199714348638208) is a Node TSC member and an outspoken advocate of best practices here and you would do well to check out everything he does. He works on [NodeClinic](https://clinicjs.org/) which helps you diagnose performance issues by automatically injecting probes to collect metrics, and even creates recommendations! Matteo as a service!\n\nQuite often, in OSS you just need to know what version numbers of everything the developer is using so you can track down obvious environment issues. \n\nI believe every GitHub Issue Template should include [Trevor Brindle's envinfo](https://www.npmjs.com/package/envinfo) tool. For example, when I run `npx envinfo --system --binaries --browsers --npmGlobalPackages --markdown` I get:\n\n```markdown\n## System:\n - OS: macOS Mojave 10.14.6\n - CPU: (4) x64 Intel(R) Core(TM) i7-7660U CPU @ 2.50GHz\n - Memory: 413.13 MB / 16.00 GB\n - Shell: 5.3 - /bin/zsh\n## Binaries:\n - Node: 10.17.0 - ~/.nvm/versions/node/v10.17.0/bin/node\n - Yarn: 1.19.2 - /usr/local/bin/yarn\n - npm: 6.13.4 - ~/.nvm/versions/node/v10.17.0/bin/npm\n## Browsers:\n - Chrome: 79.0.3945.130\n - Firefox: 71.0\n - Firefox Nightly: 73.0a1\n - Safari: 13.0.5\n## npmGlobalPackages:\n - @aws-amplify/cli: 4.12.0\n - diff-so-fancy: 1.2.7\n - eslint: 6.7.1\n - expo-cli: 3.11.9\n - netlify-cli: 2.32.0\n - now: 16.7.3\n - npm: 6.13.4\n - rincewind: 3.0.5\n - serve: 11.2.0\n - sharp-cli: 1.13.1\n```\n\n## JavaScript Logging\n\nIn JS we are pretty good, sometimes *too* good, about `console.log`ging everything, but it isn't good enough to dump a bunch of irrelevant unstructured crap in the terminal or browser console.\n\n### Logs: Streaming Logs\n\nIn Node, we should become a little more [comfortable with Node streams](https://dustinpfister.github.io/2018/08/17/nodejs-filesystem-create-write-stream/) - they seem alien at first but are actually pretty darn handy especially for memory efficient I/O.\n\nFor example, we can output work logs and error logs with streams:\n\n```js\nlet fs = require('fs');\n \nlet writer = fs.createWriteStream('applog.txt');\nlet errors = fs.createWriteStream('errlog.txt');\n \nwriter.write('hello world');\n\ntry {\n  // something risky\n} catch (err) {\n  errors.write(err)\n  console.error(err)\n}\n\n// etc.\n```\n\n### Logs: Structuring Logs\n\nIf your logs have some structure but not too much info, [a table](https://www.npmjs.com/package/ascii-table) might be appropriate:\n\n```js\nvar table = new AsciiTable('A Title')\ntable\n  .setHeading('', 'Name', 'Age')\n  .addRow(1, 'Bob', 52)\n  .addRow(2, 'John', 34)\n  .addRow(3, 'Jim', 83)\n \nconsole.log(table.toString())\n\n// .----------------.\n// |    A Title     |\n// |----------------|\n// |   | Name | Age |\n// |---|------|-----|\n// | 1 | Bob  |  52 |\n// | 2 | John |  34 |\n// | 3 | Jim  |  83 |\n// '----------------'\n```\n\nBut be mindful of whether you need your logs to be grep/awk/sed friendly (or maybe you just need to dump some JSON, up to you - [Bunyan helps you stream JSON to files](https://github.com/trentm/node-bunyan)).\n\nMaybe there are other tools for padding structured data with whitespace for logging, but I haven't come across them yet.\n\n### Logs: Log Levels\n\nI do have a strong opinion that you should not clutter the developer console with random logs from everywhere - but you should make it easy for yourself and others to turn more verbose logging on when needed. This is often addressed in CLI tools with a `--verbose` flag, but even that is not good enough.\n\nYou will want to have different **log levels** of abstraction so that you can enable the developer to request the correct density of logs for the problem they are trying to face. [Bunyan builds in the concept of Levels](https://github.com/trentm/node-bunyan#levels) and this idea is apparently [built into Rails](https://github.com/heroku/rails_stdout_logging). \n\n[Syslog](https://en.wikipedia.org/wiki/Syslog) is a more formally designed standard for message logging with an established [hierarchy of severity](https://en.wikipedia.org/wiki/Syslog#Severity_level):\n\n![https://i.pinimg.com/originals/21/20/41/2120413e9ba0cfef40936247fe725aa4.jpg](https://i.pinimg.com/originals/21/20/41/2120413e9ba0cfef40936247fe725aa4.jpg)\n\nOf course, as developers we will mostly surface levels 3-6, but spend the bulk of our time in level 7 - debugging. \n\nThere are 2 tools I strongly recommend for Level 7 logging. \n\nNode has an [inbuilt `util.debuglog`](https://nodejs.org/api/util.html#util_util_debuglog_section) function:\n\n```js\nconst util = require('util');\nconst debuglog = util.debuglog('foo');\n\ndebuglog('hello from foo [%d]', 123);\n\n// If this program is run with NODE_DEBUG=foo in the environment\n// then it will output something like:\n// \n// FOO 3245: hello from foo [123]\n```\n\nWhereas the aptly named [`debug`](https://www.npmjs.com/package/debug) tool takes this idea and adds timing output with pretty colors.\n\n```js\nvar a = require('debug')('worker:a')\n  , b = require('debug')('worker:b');\n \nfunction work() {\n  a('doing lots of uninteresting work');\n  setTimeout(work, Math.random() * 1000);\n}\n \nwork();\n \nfunction workb() {\n  b('doing some work');\n  setTimeout(workb, Math.random() * 2000);\n}\n \nworkb();\n```\n\n![https://user-images.githubusercontent.com/71256/29091700-a62a6888-7c38-11e7-800b-db911291ca2b.png](https://user-images.githubusercontent.com/71256/29091700-a62a6888-7c38-11e7-800b-db911291ca2b.png)\n\nIsn't that beautiful! You can control what shows by setting the `DEBUG` environment variable - which means you can arbitrarily make your program spit out logs for the feature you're focusing on without changing any code inside. This is infinitely scalable.\n\n**SERIOUSLY, EVERYBODY SHOULD USE `DEBUG`!!!**\n\n## JavaScript Traces\n\n> A record of an API call that's made from one part of my application to another\n\nYup, you can add that to `debug`.\n\nIf you care about readable stack traces, Node.js can be fairly scary with its impenetrable internals. Fortunately you can clean it up with [Sindre Sorhus' `clean-stack`](https://github.com/sindresorhus/clean-stack):\n\n```js\nconst cleanStack = require('clean-stack');\n\nconst error = new Error('Missing unicorn');\n\nconsole.log(error.stack);\n/*\nError: Missing unicorn\n    at Object.<anonymous> (/Users/sindresorhus/dev/clean-stack/unicorn.js:2:15)\n    at Module._compile (module.js:409:26)\n    at Object.Module._extensions..js (module.js:416:10)\n    at Module.load (module.js:343:32)\n    at Function.Module._load (module.js:300:12)\n    at Function.Module.runMain (module.js:441:10)\n    at startup (node.js:139:18)\n*/\n\nconsole.log(cleanStack(error.stack));\n/*\nError: Missing unicorn\n    at Object.<anonymous> (/Users/sindresorhus/dev/clean-stack/unicorn.js:2:15)\n*/\n```\n\n[`stack-utils`](https://github.com/tapjs/stack-utils) seems to also do the same thing but I haven't tried it out yet.\n\nSometimes you have to output something when your Node process ends, either gracefully or abruptly. [`node-cleanup`](https://www.npmjs.com/package/node-cleanup) can help you tie up any loose ends and do optional reporting to the developer.\n\nWhat other ideas do you have here? Let me know 😻\n\n> Recommended resources from readers:\n\n- [OpenTracing](https://opentracing.io/) - Vendor-neutral APIs and instrumentation for distributed tracing\n- [Thomas Watson — An introduction to distributed tracing](https://www.youtube.com/watch?v=g7XSEdriSFM&feature=youtu.be)\n\n## Events\n\n[According to Honeycomb](https://docs.honeycomb.io/learning-about-observability/events-metrics-logs/): \n\n> Events are built up over time, gaining context as they go. \n> Events provide both human and computer readable information, are not aggregated at write time, and allow us to ask more questions and combine the data in different ways.\n\nCharity also contrasted Events vs Metrics, Logs, and Traces in her [mega response thread](https://twitter.com/mipsytipsy/status/1231864655951216642?s=20):\n\n> we need to stop firing off metrics and log lines ad hoc-like and start issuing a single arbitrarily-wide event -- one per service, per request -- containing the *full context* of that request.\n>\n> EVERYTHING you know about it, did in it, parameters passed in to it, etc.\n> Anything that could help us find and identify the request later.  Think of that request like a needle in a haystack, and you need to be able to swiftly locate every damn needle in the stack.\n>\n> You especially want to stuff in any kind of IDs.  Userids, app id, shopping cart id, etc\n> Then, when the request is poised to exit or error the service, you ship that blob off to your o11y store in one very-wide blob.\n> \n> (A maturely instrumented service usually has a few HUNDRED dimensions per event, and it will have one event for each service that the request hits.)\n\nSo this is really a concept that you should build up using unique identifiers for events and a place to store and query events somewhere. \n\nHonestly I don't have a ton of experience creating Events, but if you need unique id's you can use [`uuid`](https://www.npmjs.com/package/uuid) to generate some, and event object creation is up to you I guess.\n\nYou can also [use `concordance` to compare, format, diff and serialize any JavaScript value](https://www.npmjs.com/package/concordance) to create events with just diff data."
  },
  {
    "slug": "devto-cms",
    "data": {
      "title": "Using DEV.to as a CMS",
      "description": "Blog on DEV.to, publish on your own domain, using the DEV.to API!",
      "tag_list": [
        "tech"
      ]
    },
    "content": "\nThis is a post on Dev.to that should also appear on my personal site. Canonical URL is manually set so that my site is the authoritative source, but the content lives in Dev.to.\n\nSee the comparison:\n\n- Dev.To URL: https://dev.to/swyx/using-dev-to-as-a-cms-3472/\n- Swyx.io URL: https://www.swyx.io/writing/devto-cms/\n\n[In September last year](https://twitter.com/bendhalpern/status/1176663688742395904?s=20), DEV started publicizing their API (I don't know exactly when they launched it). I've had the idea to use DEV as a headless CMS for a while, but today I actually did it. It is nice because it gets syndication and comments for people who use Devto, as well as a nice image upload solution that doesn't involve checking into Git.\n\n## Steps\n\nYou should be pretty comfortable wrangling APIs and piping content into your own blog. I [wrote my own static site generator](https://github.com/sw-yx/ssg/tree/master/packages/ssg) so I am pretty comfortable with this, but if you are new to this game you may need some extra help based on your setup.\n\n- Get your API key at https://dev.to/settings/account\n- See the docs at https://docs.dev.to/api/#operation/getUserPublishedArticles\n\nOur game plan is to only pull published articles from our account. The API is paginated, and although we can cheat by pulling a 1000-article page, I'm going to do it the \"right\" way by looping though each page until we reach an end. You could also store this and tweak this logic so you only fetch pages up to a certain preset date.\n\nHere's some code that I wrote for proof of concept\n\n\n```js\nrequire('dotenv-safe').config() // have DEV_TO_API_KEY in process.env\nconst fetch = require('node-fetch')\n;(async function() {\n  let allArticles = []\n  let page = 0\n  let per_page = 30 // can go up to 1000\n  let latestResult = []\n  do {\n    page += 1 // bump page up by 1 every loop\n    latestResult = await fetch(\n      `https://dev.to/api/articles/me/published?page=${page}&per_page=${per_page}`,\n      {\n        headers: {\n          'api-key': process.env.DEV_TO_API_KEY\n        }\n      }\n    )\n      .then(res => res.json())\n      .then(x => (allArticles = allArticles.concat(x)))\n      .catch(err => {\n        console.error(err) // very basic error handling, customize as needed\n        throw new Error(`error fetching page ${page}, {err}`)\n      })\n  } while (latestResult.length === per_page)\n  console.log(allArticles.length)\n})()\n```\n\nNow, this just gets you the basic data. You have more work to do to get stuff to show up on page nicely.\n\nNotes:\n\n- Dev.to's API only exports the raw markdown. You will have to do the postprocessing yourself, including stripping and processing frontmatter, and adding syntax highlighting and whatever else you would like. [See my devto source plugin for how I did it.](https://github.com/sw-yx/ssg/blob/master/packages/source-devto/src/index.ts)  (Update: [rhymes corrected me that the single post api endpoint gives you html with highlightjs syntax highlighting](https://dev.to/rhymes/comment/lp56), but I continue to do my own postprocessing for the other benefits I get with remark.)\n- In Dev.to, if you try to specify any datelike format in frontmatter, e.g. 2020-02-20 - you get a `base:  Tried to load unspecified class: Date`. I opted out of it by adding string quotes around it like \"2020-02-20\"\n- Dev.to lets you add any frontmatter you like, including stuff it already has, like `slug` and `subtitle`. it just wont recognize it. This is great for porting over frontmatter that isn't recognized by Dev.to.\n- I have 95 posts on my site, and 55 posts on Dev.to. Some were duplicate posts. I solved this by removing duplicate content from my site (leaving Dev.to the authortitative store, since it also had social metadata), and then merging the content in my site generator data layer. This took a while 😅.\n- There isn't a nice way to auto-set canonical url for Dev.to. Ideally I would like to just specify a slug, and that would populate the `slug` field and the `canonical_url` field. I may have to write my own Dev.to client for this - not hard given it doesnt require WYSIWYG. However I would need to replicate the Image Upload functionality."
  },
  {
    "slug": "stateful-serverless",
    "data": {
      "title": "Serverless Functions are Stateful",
      "description": "A reminder that serverless functions actually have a lot of state, and how the \"function\" analogy breaks down when you look through the abstraction",
      "tag_list": [
        "serverless"
      ]
    },
    "content": "\n*Canonical URL: https://www.swyx.io/writing/stateful-serverless/*\n\nWe are often taught that serverless functions should be written as small, stateless pieces of business logic. This might lead us to conclude that their environment is stateless as well. It's extremely easy to verify that they are not, and the resulting abstraction leak will teach you something about serverless functions you probably didn't know.\n\n## Write a Stateful Serverless Function\n\nI'll mostly assume you have some basic knowledge with writing Netlify Functions ([docs link](https://docs.netlify.com/functions/build-with-javascript/#format)) here, which have the same API and behavior as AWS Lambda Functions, but this also applies to other clouds.\n\nConsider this basic function:\n\n```js\n// functions/hello-world.js\nlet abc = 0;\nexports.handler = async (event, context) => {\n  abc += 1;\n  return {\n    statusCode: 200,\n    body: JSON.stringify({ message: `Hello, abc is ${abc}` })\n  };\n};\n```\n\nYou can see this in action here (https://github.com/sw-yx/stateful-serverless) with the deployed endpoint [here](https://stateful-serverless-demo.netlify.com/.netlify/functions/hello-world). (Note if you navigate to this in the server you may often double-ping the function with your OPTIONS requests).\n\nNow: What is the result of pinging the `hello-world` function repeatedly? You might reasonably expect that you'll get `{\"message\":\"Hello, abc is 1\"}` over and over.\n\nWell, let's see:\n\n```bash\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/hello-world\n{\"message\":\"Hello, abc is 1\"}\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/hello-world\n{\"message\":\"Hello, abc is 2\"}\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/hello-world\n{\"message\":\"Hello, abc is 3\"}\n```\n\nIf you thought serverless functions are stateless like me, this will be a deep shock. `let abc = 0` is only run once!\n\nThis means that we can kind of abuse this fact to build a crappy rate limited function:\n\n```js\n// functions/rate-limiting.js\nlet count = 0;\nlet firstInvoke = new Date();\nexports.handler = async (event, context) => {\n  let currentInvoke = new Date();\n  let diff = currentInvoke - firstInvoke;\n  if (diff < 5000) {\n    count += 1;\n  } else {\n    count = 1;\n    firstInvoke = currentInvoke;\n  }\n  if (count > 3) {\n    return {\n      statusCode: 429,\n      body: JSON.stringify({ message: `Too many requests! ${count}` })\n    };\n  } else {\n    return {\n      statusCode: 200,\n      body: JSON.stringify({ message: `Hello, count is ${count}` })\n    };\n  }\n};\n\n```\n\nLet's try it:\n\n```bash\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/rate-limiting\n{\"message\":\"Hello, count is 1\"}\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/rate-limiting\n{\"message\":\"Hello, count is 2\"}\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/rate-limiting\n{\"message\":\"Hello, count is 3\"}\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/rate-limiting\n{\"message\":\"Too many requests! 4\"}\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/rate-limiting\n{\"message\":\"Too many requests! 5\"}\n\n# wait 5 seconds...\n$ curl https://stateful-serverless-demo.netlify.com/.netlify/functions/rate-limiting\n{\"message\":\"Hello, count is 1\"}\n```\n\n## What's going on?\n\nYou probably had the same serverless mental model I had:\n\n![image](https://user-images.githubusercontent.com/6764957/75006590-3ad68980-5440-11ea-85b3-9e64fe3a099c.png)\n\nThis would map to each function being stateless.\n\nHowever, what actually happens is a little messier:\n\n![image](https://user-images.githubusercontent.com/6764957/75006585-3611d580-5440-11ea-9f18-84ff77509461.png)\n\nI first learned about this from [Guillermo Rauch's Stateful Serverless Applications talk at PrismaDay 2019](https://www.youtube.com/watch?v=lUyln5m6AhY&app=desktop) - and it forever changed the way I thought about Serverless.\n\nAs you can see from the [AWS Note on Container Reuse](https://aws.amazon.com/blogs/compute/container-reuse-in-lambda/), a significant amount of state in the environment can be reused, even if it can't be relied on. You can even **write to the filesystem** in `/tmp` and it will stick around! \n\nAs [Guillermo notes in his talk](https://youtu.be/lUyln5m6AhY?t=889), this means that other stateful processes in the container will also resume upon subsequent invocations of the same container:\n\n- `setTimeout` and `setInterval`\n- child processes (and their children)\n- Sockets might need to reconnect\n\n## Spot the bug\n\nIt was the nuances described above that caused me to face this bug today. \n\nHere is the pseudocode, see if you can spot the bug:\n\n```js\nexports.handler = async function(event, context) {\n  let data = JSON.parse(event.body || \"{}\");\n  sendData(data);\n  return { statusCode: 200, body: \"OK\" };\n};\nfunction sendData(data) {\n  const https = require(\"https\");\n  const options = {/* misc */};\n  const req = https.request(options);\n  req.write(data);\n  req.end();\n}\n```\n\nCan you spot the bug?\n\nGive up?\n\n`https.request` is an asynchronous operation, and the `handler` function returns/terminates before it has a chance to complete. It is only when the next function invocation gets called does the container wake up again and continue executing the request. So we only see the effect of the first `sendData` on the 2nd function invocation, and so on.\n\nFYI - [initialization is also free](https://twitter.com/alexbdebrie/status/1192120017137127425), so you can stick some heavy require code in there if you don't mind longer cold starts.\n\n## Oh, about Cold Starts\n\nIt is a myth that you can simply [periodically ping lambdas to avoid cold starts](https://serverless.com/blog/keep-your-lambdas-warm/) every [5-15 mins](http://stackoverflow.com/questions/42877521/is-it-possible-to-keep-an-aws-lambda-function-warm?noredirect=1#comment72860693_42877521), like you would a health check on a server. It helps but it doesn't solve it.\n\n[**Lambda cold starts** are about concurrent executions](https://hackernoon.com/im-afraid-you-re-thinking-about-aws-lambda-cold-starts-all-wrong-7d907f278a4f). It happens when Lambda decides it needs to initialize another container to handle your function invocation. This is why you can't rely on singleton state in your serverless functions, even though they are stateful. \n\n> Note: I tried to simulate this with Netlify Functions, but couldn't figure it out. They just always acted like they belonged to one container. I suspect that is Lambda optimizing for us, but can't be sure. Please hit me up if you can do it?\n\nHowever it is also why sending a periodic ping doesn't solve all your cold start problems - it just warms the functions you use the least. This is why [Brian LeRoux](https://twitter.com/brianleroux) has concluded the only reliable way to avoid cold starts is simply to make sure your function is [<1mb of JS](https://twitter.com/brianleroux) (you can do more with faster runtimes like Go).\n\n[Definitely read Yan Cui's article on this in it's entirety to internalize this.](https://hackernoon.com/im-afraid-you-re-thinking-about-aws-lambda-cold-starts-all-wrong-7d907f278a4f)\n\n## Appendix: Master List of Lambda Container Reuse and Cold Start facts\n\n- Cost of cold starts seem to be around 3 seconds\n- [Lambdas seem to reset around 14:00 UTC regardless of recency of use](https://read.acloud.guru/how-to-keep-your-lambda-functions-warm-9d7e1aa6e2f0)\n- [executions that are idle for a while would be garbage collected](https://read.acloud.guru/how-long-does-aws-lambda-keep-your-idle-functions-around-before-a-cold-start-bf715d3b810)\n- [executions that have been active for a while (somewhere between 4 and 7 hours) would be garbage collected too](https://hackernoon.com/im-afraid-you-re-thinking-about-aws-lambda-cold-starts-all-wrong-7d907f278a4f)"
  },
  {
    "slug": "clientside-webmentions",
    "data": {
      "title": "Clientside Webmentions",
      "description": "How you can enhance your blog with webmentions without adding heavy build times.",
      "tag_list": [
        "indieweb",
        "svelte",
        "webmentions"
      ]
    },
    "content": "\n*canonical_url: https://www.swyx.io/writing/clientside-webmentions*\n\n\n[Max Bock's blogpost on Webmentions](https://mxb.dev/blog/using-webmentions-on-static-sites/) was very influential in the Indieweb community, but it discusses a serverside only approach to pulling and displaying that data.\n\nI was interested in deferring that work and leaving it to the clientside. Clientside solutions of course forces clients to do more work, but explicitly designing for it can be more considerate by not dumping a ton of image requests and useless markup on the mobile user. Of course, build times and success rates are also a little more predictable if you take external dependencies out of the equation.\n\nI searched Webmention implementations on GitHub and found [Max Stoiber's impl](https://github.com/mxstbr/mxstbr.com/blob/49aceb93a43d1e87736f204f26c07e203cb2a0e1/components/WebMentions/WebMentionCounts.js). It fetches a simple count of webmentions, and then paginated full text responses. I figured I should try adapting that.\n\n> ⚠️ For this post I will assume you've already followed Max's advice on setting up [webmention.io](https://mxb.dev/blog/using-webmentions-on-static-sites/) (add the twitter link with `rel=\"me\"`) and then setting up the backfeed with [Bridgy](https://brid.gy/). I don't know any other services that perform these functions - we have to be thankful to folks like [Aaron Parecki](https://github.com/aaronpk) for the fact that these even work.\n\n## Clientside Webmention.io Widget\n\nYou can use their provided Webmention widget ([docs](https://webmention.io/)):\n\n```html\n<span data-webmention-count data-url=\"https://example.com/page/100\"></span> mentions\n<script type=\"text/javascript\" src=\"https://webmention.io/js/mentions.js\"></script>\n```\n\n## Simple Count\n\nThis is the endpoint to hit: `https://webmention.io/api/count.json?target=URL_TO_YOUR_POST/`. ⚠️ NOTE: You will need that trailing slash for this request to work! I probably wasted 2 hours figuring this out.\n\nThis is the API response you get back:\n\n```json\n{\n  \"count\": 1062,\n  \"type\": {\n    \"like\": 638,\n    \"mention\": 154,\n    \"reply\": 51,\n    \"repost\": 219\n  }\n}\n```\n\nSome of these are likes, and retweets, while others are replies and quote tweets. Max combines the last 2 and first 2 and I do so too.\n\n```html\n<script>\n  export let target // passed in as prop\n  const counts = fetch(`https://webmention.io/api/count.json?target=${target}`)\n    .then(res => res.json())\n    .then(x => x.type)\n</script>\n{#await counts}\n  <p>loading counts</p>\n{:then data}\n  {#if data === undefined}\n    Failed to load...\n  {:else}\n    ❤️ {data.like + data.repost || 0} 💬 {data.mention + data.reply || 0}\n  {/if}\n{/await}\n```\n\n## Paginated Mentions\n\nOf course, counts are nice, but real human contact lives in mentions. Here is the endpoint to hit: `https://webmention.io/api/mentions?page=0&per-page=20&sort-by=published&target=URL_TO_YOUR_POST/`\n\n> ⚠️ NOTE: You will need that trailing slash for this request to work! I probably wasted 2 hours figuring this out.\n\n> ⚠️ Note that the endpoint is `/mentions` - the docs say to hit `/mentions.jf2` but that did not work at all in my testing. \n\nNote that there is pagination, so you have to be able to increment the page and refetch the mentions. \n\nI originally chose to use Svelte's nifty [#await syntax](https://svelte.dev/docs#await) - this means only showing 20 at any time:\n\n```html\n<script>\n  let page = 0\n  export let target\n  const promiseFactory = () =>\n    fetch(`https://webmention.io/api/mentions?page=${page}&target=${target}`)\n      .then(x => x.json())\n      .then(x => x.links) // array\n  let promise = promiseFactory()\n  const dec = () => {\n    page -= 1\n    promise = promiseFactory()\n  }\n  const inc = () => {\n    page += 1\n    promise = promiseFactory()\n  }\n</script>\n{#await promise}\n  <p>Loading replies...</p>\n{:then links}\n  {#if links.length === 0}\n    <div>\n      No replies yet.\n      <a\n        href=\"https://twitter.com/intent/tweet/?text=My%20thoughts%20on%20{target}\">\n        Tweet about this post\n      </a>\n      and it will show up here!\n    </div>\n  {:else}\n    <div>\n      <button on:click={dec} disabled={page === 0}>-</button>\n      Page {page + 1}\n      <button on:click={inc}>+</button>\n    </div>\n    <h1>Replies</h1>\n    <ul>\n      {#each links as link}\n        <li>\n          <div width=\"40\">\n            <a\n              target=\"_blank\"\n              rel=\"noopener\"\n              href={link.data.url}\n              color=\"blue\">\n              <img\n                width=\"40\"\n                height=\"40\"\n                alt=\"avatar of {link.data.author.name}\"\n                src={link.data.author.photo} />\n            </a>\n          </div>\n          <div>\n            <a\n              target=\"_blank\"\n              rel=\"noopener\"\n              href={link.data.url}\n              color=\"blue\">\n              <div font-family=\"system\" color=\"text\" font-weight=\"bold\">\n                {link.data.author.name}\n                <span color=\"tertiary\">\n                  · {new Date(link.data.published)}\n                </span>\n              </div>\n            </a>\n            <div>\n              <p font-family=\"system\" color=\"tertiary\" font-size=\"2\">\n                {@html link.activity.sentence_html}\n              </p>\n            </div>\n          </div>\n        </li>\n      {/each}\n    </ul>\n  {/if}\n{:catch error}\n  <!-- promise was rejected -->\n  <p>Something went wrong: {error.message}</p>\n{/await}\n```\n\nBut this means losing the previous 20 that you see and also some nasty re-layout issues.\n\nBecause I am SSRing, I also had some nasty `fetch is undefined` error issues.\n\nInstead I switched to another implementation that appended infinitely to an array, and loaded `onMount`:\n\n```html\n<script>\n  let page = 0\n  export let target\n  let counts\n  let mentions = []\n  let fetchState = 'fetching'\n  import { onMount } from 'svelte'\n  onMount(() => {\n    counts = fetch(`https://webmention.io/api/count.json?target=${target}/`) // trailing slash impt\n      .then(res => res.json())\n      .then(x => x.type)\n    getMentions().then(x => {\n      mentions = x\n      fetchState = 'done'\n    })\n  })\n  function getMentions() {\n    return fetch(\n      // `https://webmention.io/api/mentions?page=${page}&per-page=20&sort-by=published&target=${target}`,\n      `https://webmention.io/api/mentions?page=${page}&per-page=50&target=${target}/` // trailing slash impt\n    )\n      .then(x => x.json())\n      .then(x => x.links.filter(x => x.activity.type !== 'like'))\n  }\n  const fetchMore = () => {\n    page += 1\n    getMentions().then(x => {\n      if (x.length) {\n        mentions = [...mentions, ...x]\n      } else {\n        fetchState = 'nomore'\n      }\n    })\n  }\n</script>\n<!-- etc -->\n\n    {#if fetchState !== 'nomore'}\n      <li>\n        <button class=\"FetchMore\" on:click={fetchMore}>\n          Fetch More...\n        </button>\n      </li>\n    {:else}\n      <li>\n        No further replies found.\n        <a\n          href=\"https://twitter.com/intent/tweet/?text=My%20thoughts%20on%20{target}\">\n          Tweet about this post\n        </a>\n        and it will show up here!\n      </li>\n    {/if}\n```\n\nAnd that's the clientside webmentions you see live now 👇🏽."
  },
  {
    "slug": "big-l-notation",
    "data": {
      "title": "Big L Notation",
      "description": "In this post I sketch out `Big L` notation, which plots your learning as a function of `N` years of experience, with `P` peers.",
      "tag_list": [
        "learninpublic"
      ]
    },
    "content": "\n*Canonical URL: https://www.swyx.io/writing/big-l-notation/*\n\nIf you are a lifelong/[infinite learner](https://www.sachinrekhi.com/how-to-be-an-infinite-learner), you want to get the most out of the time you dedicate to learning. What are the determinants of how much you learn over time?\n\nIn the formal study of algorithms, [Big O](https://en.wikipedia.org/wiki/Big_O_notation) is often used to succinctly state how costs scale as a function of the amount of work to do. I think we can also take this notation to talk about different forms of learning over time.\n\nBig O is handy because it allows engineers to talk in terms of orders of magnitude (which is more important for architectural decisions) rather than get bogged down in precise details (which are hard to predict and don't matter at scale).\n\nIn this post I sketch out `Big L` notation, which plots your learning as a function of `N` years of experience, with `P` peers. Note that while `Big O` is a cost curve (higher is worse), `Big L` is a benefit curve (higher is better).\n\n![Alt Text](https://dev-to-uploads.s3.amazonaws.com/i/63yl17w3d4eep4shubr3.png)\n\n## Learning in Private\n\n### L(1) - The One Trick Pony\n\nThis person learns one thing and chooses to do it over and over again for the rest of their career. You might consider jobs like being a forklift operator or long haul driver or taxi driver or TPS report filer as amenable to this, but you can get `L(1)` attitudes in high variance jobs like teaching and programming as well. Their competencies don't noticeably scale with years of experience.\n\n**You know people like this.** They may be stuck in a rut due to unfortunate circumstance, but also due to limitations in personal ambition.\n\n### L(log N) - The Lossy Learner\n\nThis person's learning function is lossy - they learn things and forget things and have to relearn them again, re-commit mistakes made. They be uncurious - learning reactively as a result of randomly distributed events, rather than having a self directed learning goal. \n\n**This is the normal learner.**\n\n### L(N) - The Infinite Learner\n\nThis person learns something new every year and keeps it. Mistakes are not repeated. This requires constantly pushing boundaries and building a system ([principles](https://www.principles.com/), [habits](https://jamesclear.com/habits), or [a second brain](https://www.buildingasecondbrain.com/)) for the stuff the human mind naturally forgets.\n\n**This is most people's idea of an ideal learner.**\n\n### L(N^2) - The Deep Learner\n\nThis may not be a realistic goal for a human to do, but it is an interesting thought experiment. What does it take for someone to scale *faster* the more years of experience they have in something?\n\nI agree with the [breadth vs depth split proposed by Preet](https://twitter.com/preetster/status/1226768072343638021). You can be diffuse in your efforts, or you can concentrate your efforts every day building on what you had done the previous day. Take one step each for 360 degrees and still be in the same spot, or take 360 steps in one direction and go places.\n\nThe choice might seem obvious, but only if you go 360 steps in the *wrong* direction it might be a waste of time. This is why I choose the neutral term [Learning Gears](https://www.swyx.io/writing/learning-gears) for the different modes of learning.\n\nI think for true `L(N^2)` learning, some introspection should take place as well. You ought to be looking back over your prior years and articulating something more than the sum of the parts. Writing a comprehensive historical/technical reference, coming up with a grand explanatory model of Why Stuff Happens, these are all activities of `L(N^2)` learning.\n\nLearning can compound *forwards* as well as backwards. If the `L(N)` learner builds systems to keep themselves from repeating mistakes, the `L(N^2)` learner automates to save future work, and builds mental models that *anticipate* future needs, inferring from prior cycles.\n\n## Learning in Public\n\nWhen it comes to learning in public, we introduce another input `P` to learning. It means whatever you want it to mean - \"People\", \"Peers\", \"Possible Mentors\", or [Community](https://www.swyx.io/writing/scaling-coding-communities). \n\nOf course, these exist when you are learning in private. You have to learn *from* someone. Most people's `P` are constant and they don't pay any attention to it.\n\nBut you could also learn *with* someone. When you Learn in Public, growing `P` becomes an explicit, monitored goal with milestones you track like you do New Years' Eve.\n\nYou should also mentally scale it up or down according to how much you think one additional `P` contributes vs. one additional `N`.\n\n### L(PN) - The Networked Learner\n\nThe Networked Learner learns with a group of peers, sharing notes as they go, tapping into them when they get stuck, and learning by answering questions they've never thought to ask. This ability to learn from people either just behind you, slightly ahead of you, or where you are, is profoundly multiplicative.\n\nThere is probably a [Dunbar limit](https://en.wikipedia.org/wiki/Dunbar%27s_number) to how high `P` can go, but I find active relationships wax and wane such that adding learning peers is still always additive. In other words, *Concurrent* `P` matters more than *Lifetime* `P`.\n\nIt's actually *harder* to be a Networked, yet Lossy, Learner, or `L(P log N)`. If you do it right, people keep you accountable.\n\n### L((PN)^2) - The #LearnInPublic Grand Slam\n\nAlright, we're firmly outside the bounds of human reality here. But what does `L((PN)^2)` look like?\n\nFirst of all, you're a Deep Learner. That's a given. \n\nBut you're also compounding learning from people proportional to the *square* of the number of people (aka [Metcalfe scaling](https://en.wikipedia.org/wiki/Metcalfe%27s_law)). The only way to do this is to have people learn from each other independently of you, AND still have you benefit from it.\n\nI think this means actively growing a [Community](https://www.swyx.io/writing/scaling-coding-communities) that helps you serve a bigger goal/mission. It can also take the form of [Open Source Knowledge](https://www.swyx.io/speaking/sedaily-nocode). Wikipedia beat the traditional Encyclopedias at a fraction of their budget because it was able to grow at `L((PN)^2)` vs `L(N)` or `L(PN)`.\n\nThere are people scaling factors beyond `N^2` - [Reed's law](https://www.swyx.io/writing/eponymous-laws#business-of-tech) scales at `2^N`. But that is again a function of community.\n\n## P(N) - Reflexivity in Learning\n\nOf course, \"Big L\" is just a model of reality, it isn't actual reality. One of the ways it falls apart is in pretending that `N` and `P` are independent variables. They aren't. `P` is likely to grow with `N` as a function of your seniority and influence - making a good Networked Learner grow on the order of `L(N^2)` which is plenty great. But if you can figure out how to grow `P` superlinearly with `N` - by writing, speaking, teaching, etc - you can make your career explode in possibility.\n\n## Little L\n\nI've talked a big game about learning, but I do also acknowledge that learning is not everything. You have to be doing, and you have to be living. [How will you measure your life?](https://hbr.org/2010/07/how-will-you-measure-your-life) It's also likely a function of the people you meet and the years you live productively. What insights can we glean there?\n\nMaybe *that* is the *real* Big L we should be chasing, at the end of the day.\n\n\n> This thought [originated in a tweet](https://twitter.com/swyx/status/1226762243917991936?s=20)."
  },
  {
    "slug": "fight-link-rot-with-server-and-client-side-redirects-netlify-and-gatsby-29jn",
    "data": {
      "title": "Fight Link Rot with Server- and Client-side Redirects (Netlify and Gatsby)",
      "description": "Why you need redirects on clientside and serverside and how to set that up with Gatsby and Netlify",
      "tag_list": [
        "gatsby",
        "netlify",
        "redirects"
      ]
    },
    "content": " \n\n\nLinks break. Here's how to fight [link rot](https://en.wikipedia.org/wiki/Link_rot) on your site with redirects.\n\n## Why you need server-side redirects\n\nURL architectures change all the time as the needs of a site grows. What starts out as `/my-post` can become `@swyx/my-post` or `posts/my-post` or `news/2019/my-post` in future.\n\nConsider the user experience. Imagine you're doing some deep research and after hours of scrounging through the back pages of your Google results, you find [a link to something which could solve all your problems](http://oh-no-broken-link.netlify.com). You click it, and the site is still up, but all you see is a 404 page! If the content is still up, hopefully the site has a search function to find it, or Google indexes it. A minor annoyance, sure, but one you as a responsible webmaster could avoid for your users. \n\nThe principle of \"Don't Break the Web\" becomes even more pressing considering automated workflows like [social media unfurls](https://medium.com/slack-developer-blog/everything-you-ever-wanted-to-know-about-unfurling-but-were-afraid-to-ask-or-how-to-make-your-e64b4bb9254) and Email/RSS/site scrapers will simply break.\n\nAt the most basic form, you will want to redirect from URL A to URL B:\n\n```\n/my-broken-url   ->  /posts/my-new-url\n```\n\nThe way this is typically done is by setting up a [.htaccess](http://www.htaccess-guide.com/) file or [server redirect](https://expressjs.com/en/4x/api.html#res.redirect).\n\nOne-to-one redirection is very customizable, however may fail to scale for large groups of posts that you may need to redirect.\n\nNetlify offers more powerful redirect configuration with [Netlify Redirects](https://www.netlify.com/docs/redirects/?utm_source=blog&utm_medium=devto&utm_campaign=devex). You can use [placeholders](https://www.netlify.com/docs/redirects/?utm_source=blog&utm_medium=devto&utm_campaign=devex#placeholders) to declaratively rearrange URLs. You can [proxy serverless functions](https://www.netlify.com/docs/redirects/?utm_source=blog&utm_medium=devto&utm_campaign=devex#rewrites-and-proxying). You can use [cookie-based language redirects](https://www.netlify.com/docs/redirects/?utm_source=blog&utm_medium=devto&utm_campaign=devex#geoip-and-language-based-redirects). My favorite, partly because it is fun to say, is [the splat feature](https://www.netlify.com/docs/redirects/?utm_source=blog&utm_medium=devto&utm_campaign=devex#splats):\n\n```\n/posts/*  /news/:splat\n```\n\nWhich is kind of like [the spread operator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_syntax) of URLs. \n\nSearch engine indexes will update accordingly as your redirects get visited.\n\nHowever, in the age of modern Single Page Apps, this isn't the full story.\n\n## Why you need client-side redirects\n\nServer-side redirects take care of **inter-site linking**: the case of **other** sites navigating into your site on a broken link. \n\nClient-side redirects address the case of **intra-site linking**: when your own site links to other pages in your site, rendered via JavaScript so it doesn't refresh via the server, and the link breaks.\n\nSingle Page Apps use [client-side routing](https://codeburst.io/client-side-routing-done-right-3464275778bf) in order to avoid a full page refresh (which I recently learned [isn't always faster](https://carter.sande.duodecima.technology/javascript-page-navigation/)! TIL). This has two primary implications for link rot.\n\nThe first and simplest case is that a basic Single Page App, like one set up by `create-react-app`, can actually not need a long list of complex server-side redirects. Just setting up [a simple Single Page App catchall](https://www.netlify.com/docs/redirects/?utm_source=blog&utm_medium=csstricks&utm_campaign=devex#history-pushstate-and-single-page-apps) and letting clientside routing handle everything means we can also set up redirects only on the clientside. All JavaScript frameworks support this, from [React Router](https://github.com/ReactTraining/react-router/blob/master/packages/react-router/docs/api/Redirect.md) to [Vue Router](https://router.vuejs.org/guide/essentials/redirect-and-alias.html). This allows us to manage our redirects (old routes) in the same codebase/location as our routing (current routes).\n\nOne drawback of this approach is that the process of resolving the page upon a full page refresh is rather roundabout. First you hit `/my-old-url`, which the server then serves the client bundle for `/`, which then parses and renders on the clientside, which then reads `/my-old-url`, which then redirects to `/posts/my-new-url`.\n\nThe second implication of client-side routing for link rot is that modern JavaScript static site generators like Gatsby face a hybrid problem where the bundles for each statically generated page must do client-side routing, however we do not want to configure server-side catchalls (in the simple Single Page App way) or we will lose the whole benefit of using a static site generator in the first place.\n\nIt is extraordinarily easy to set up client-side redirects with Gatsby, as [`createRedirect`](https://www.gatsbyjs.org/docs/actions/#createRedirect) is a first-class API. This is made easier by several plugins like [gatsby-plugin-client-side-redirect](https://www.gatsbyjs.org/packages/gatsby-plugin-client-side-redirect/) or [gatsby-redirect-from](https://www.gatsbyjs.org/packages/gatsby-redirect-from/) which all make the redirects slightly easier to write. There are even hacky [redirect plugins using meta tags](https://www.gatsbyjs.org/packages/gatsby-plugin-meta-redirect/?=redirect) and for [serving Gatsby on an Express server](https://www.gatsbyjs.org/packages/gatsby-plugin-express/?=redirect).\n\n## Flash of NotFound Content\n\nThe drawback of using SSG with client-side redirects and no server-side redirects is that you will first render the not-found page first until the JavaScript loads and takes over. Leading to the dreaded **Flash of NotFound Content** (kidding, I made it up):\n\n![](https://thepracticaldev.s3.amazonaws.com/i/f0abqrjbvzb2fh0we4jv.gif)\n\n## Using both Server-side and Client-side redirects\n\nOstensibly, the solution here is to set up parallel server-side and client-side redirects. You may wish to [autogenerate the .htaccess file](https://www.gatsbyjs.org/packages/gatsby-plugin-htaccess-redirects/?=redirect) if you are using an Apache server, or use [Netlify Redirects](https://www.netlify.com/docs/redirects/?utm_source=blog&utm_medium=devto&utm_campaign=devex) to set up a parallel implementation of redirects. \n\nYou might even write a tool to output these redirects automatically... 🤔"
  },
  {
    "slug": "tutorial-on-deploying-a-create-react-app-website-to-netlify-418l",
    "data": {
      "title": "Tutorial on Deploying a Create-React-App Website to Netlify with HTTPS",
      "description": "Tutorial on Deploying a Create-React-App Website to Netlify with HTTPS",
      "tag_list": [
        "reactnetlify"
      ]
    },
    "content": "\n## TL;DR\n\nClick this:\n\n<!-- Markdown snippet -->\n[![Deploy to Netlify](https://www.netlify.com/img/deploy/button.svg?utm_source=blog&utm_medium=devto&utm_campaign=devex)](https://app.netlify.com/start/deploy?repository=https://github.com/netlify-labs/deploy-create-react-app&utm_source=blog&utm_medium=devto&utm_campaign=devex)\n\nand click yes to everything!\n\n## Is that it?\n\nYeah pretty much. This process creates a fork of <https://github.com/netlify-labs/deploy-create-react-app> (simply a site that has the output of `npx create-react-app deploy-create-react-app`) that you can then clone locally and edit. When you're done and `git push origin master`, your changes will be automatically redeployed.\n\n## What do I get?\n\n- [SSL out of the box](https://www.netlify.com/docs/ssl/?utm_source=blog&utm_medium=devto&utm_campaign=devex)\n- Free [Custom Domains](https://www.netlify.com/docs/custom-domains/?utm_source=blog&utm_medium=devto&utm_campaign=devex) (you can buy a domain there or use one bought elsewhere)\n- [Continuous Deployment](https://www.netlify.com/docs/continuous-deployment/?utm_source=blog&utm_medium=devto&utm_campaign=devex)\n- [Redirects](https://www.netlify.com/docs/redirects/?utm_source=blog&utm_medium=devto&utm_campaign=devex#history-pushstate-and-single-page-apps) useful for Single-Page-Apps!\n\nAnd [many more features](https://www.netlify.com/docs/welcome/?utm_source=blog&utm_medium=devto&utm_campaign=devex)\n\n## Other ways to deploy\n\nThese [Deploy Buttons](https://www.netlify.com/docs/deploy-button/?utm_source=blog&utm_medium=devto&utm_campaign=devex) are merely one way to deploy to Netlify.\n\n- **If you have an existing repo on GitHub/GitLab/BitBucket** you can simply [create a new site from that repo.](https://app.netlify.com/start?utm_source=blog&utm_medium=devto&utm_campaign=devex)\n- **If you have a local project** you can use the [Netlify CLI](https://www.netlify.com/docs/cli/?utm_source=blog&utm_medium=devto&utm_campaign=devex) and run `netlify init` to initialize a site.\n- **If you don't want to use Netlify's buildbot and continuous deploy** you can run your own build and directly deploy a folder to production with `netlify deploy --prod`\n\n## Why?\n\nThis tutorial was made in response to [this Reddit thread](https://www.reddit.com/r/reactjs/comments/c8m8z0/tutorial_on_deploying_a_createreactapp_website_to/)."
  },
  {
    "slug": "add-netlify-identity-authentication-to-any-react-app-in-5-minutes-with-react-context-hooks-and-suspense-5gci",
    "data": {
      "title": "Add Netlify Identity Authentication to any React App in 5 minutes with React Context, Hooks and Suspense",
      "description": "Adding authentication is a pain point for many React beginners. We’ve made it ridiculously easy to add Netlify Identity onto any React app, including create-react-app, Gatsby, Next.js, or any other setup you may have, by wrapping it all into one simple React Hook! However, this article is more about effective design patterns for introducing authentication into React apps, and any reader should be able to write similar wrappers for their preferred provider.",
      "tag_list": [
        "react",
        "netlifyidentity"
      ]
    },
    "content": "\n> 💁🏼‍♂️Bottom Line Up Front: [Here's a demo](https://react-netlify-identity-widget.netlify.com/) of the identity widget we'll see in this article, although you are free to write your own authentication UI!\n> 💁🏼‍♂️If you want to skip the explanation and just get to the quickest start, skip to the [React Netlify Identity Widget](#reactlazy-and-suspense-with-raw-reactnetlifyidentitywidget) section below!\n\nAdding authentication is a pain point for many React beginners. We’ve made it ridiculously easy to add Netlify Identity onto any React app, including `create-react-app`, Gatsby, Next.js, or any other setup you may have, by wrapping it all into one simple React Hook! However, this article is more about effective design patterns for introducing authentication into React apps, and any reader should be able to write similar wrappers for their preferred provider.\n\nFor the purposes of our examples though, we will use [Netlify Identity](https://www.netlify.com/docs/identity/?utm_source=blog&utm_medium=devto&utm_campaign=devex). This is a very simple authentication service provided by Netlify, with a [generous free tier](https://www.netlify.com/pricing/?utm_source=blog&utm_medium=devto&utm_campaign=devex#identity). You can use this for [gated content](https://www.netlify.com/blog/2018/01/23/getting-started-with-jwt-and-identity/?utm_source=blog&utm_medium=devto&utm_campaign=devex), [site administration](https://www.netlify.com/blog/2019/02/21/the-role-of-roles-and-how-to-set-them-in-netlify-identity/?utm_source=blog&utm_medium=devto&utm_campaign=devex), [authenticated functions](https://www.netlify.com/blog/2018/03/29/jamstack-architecture-on-netlify-how-identity-and-functions-work-together/?utm_source=blog&utm_medium=devto&utm_campaign=devex), and more. Users primarily access this functionality through `GoTrue-JS`, the [3kb JS client](https://www.netlify.com/blog/2018/12/07/gotrue-js---bringing-authentication-to-static-sites-with-just-3kb-of-js/?utm_source=blog&utm_medium=devto&utm_campaign=devex) for accessing Netlify Identity. Because it is just an open source API (that you can [self-host](https://github.com/netlify/gotrue)), you don’t need to host your app on Netlify, nor even have a [JAMstack app](http://jamstack.org?utm_source=blog&utm_medium=devto&utm_campaign=devex), to be able to use it.\n\n## Hooks and Context with `react-netlify-identity`\n\nFor React users, we recently wrapped up all the functionality of `GoTrue-JS` into a very easy to use React Hook. This made Netlify Identity dramatically easier to use by being a drop-in authentication solution for most React apps.\n\nAssuming you have an existing Netlify site instance (if you don’t, you can set that up by clicking [here](https://app.netlify.com/start/deploy?repository=https://github.com/netlify/create-react-app-lambda&stack=cms&utm_source=blog&utm_medium=devto&utm_campaign=devex)) and [have enabled Netlify Identity](https://www.netlify.com/docs/identity/?utm_source=blog&utm_medium=devto&utm_campaign=devex#getting-started) on it, you get started by installing:\n\n```bash\n    npm i react-netlify-identity\n```\n\nThe library has an `IdentityContext` internally, but you never have to manipulate it manually. Instead wrap the `IdentityContextProvider` around the root of your app:\n\n```js\nimport React from \"react\"\nimport { IdentityContextProvider } from \"react-netlify-identity\"\nfunction App() {\n  const url = \"https://your-identity-instance.netlify.com/\" // supply the url of your Netlify site instance with Identity enabled. VERY IMPORTANT\n  return <IdentityContextProvider url={url}>{/* rest of your app */}</IdentityContextProvider>\n}\n```\n\nThat’s all the setup you need!\n\nNow you can use the exposed identity methods anywhere in your app (they are documented in [the README](https://github.com/sw-yx/react-netlify-identity), but also you can get autocompletion hints since the library is written in TypeScript):\n\n```js\nimport { useIdentityContext } from \"react-netlify-identity\"\n\n// log in/sign up example\nfunction Login() {\n  const { loginUser, signupUser } = useIdentityContext()\n  const formRef = React.useRef()\n  const signup = () => {\n    const email = formRef.current.email.value\n    const password = formRef.current.password.value\n    signupUser(email, password)\n      .then((user) => console.log(\"Success! Signed up\", user))\n      .catch((err) => console.error(err))\n  }\n  // write similar logic for loginUser\n  // return a form attached to formRef, with email and password fields\n}\n```\n\nNormally this is where I point you to a [working demo](https://netlify-gotrue-in-react.netlify.com/) with [source code](https://github.com/sw-yx/react-netlify-identity/tree/master/example) and leave you to “go forth and write your authenticated apps”, but even this is too much work to be done especially for “quick & easy” demos.\n\nWhen we said 5 minutes, we meant 5 minutes.\n\n## Sidebar: Is that a Hook or a Context?\n\nIf you’re squinting at `useIdentityContext` and wondering what that is, you’re not alone. If it is a Context, why not export an `IdentityContext` so that the user can call `useContext(IdentityContext)`? If it is a Hook, why did you need to wrap an `IdentityContextProvider` at the app root in the first place?\n\nShort answer: It’s both.\n\n`react-netlify-identity` exports a Custom Provider _and_ a Custom Consumer Hook, a pattern [popularized by Kent C Dodds](https://kentcdodds.com/blog/how-to-use-react-context-effectively). The Custom Provider lets us initialize required info (the Netlify Identity instance) once, while The Custom Consumer Hook lets us take care of the nitty gritty details of null checks, as well as allows us to refine types for TypeScript users.\n\n## React.lazy and Suspense with `react-netlify-identity-widget`\n\nWhere `react-netlify-identity` exports reusable authentication behavior for your apps, it has no opinion at all on your authentication UI. This can halt your productivity while you futz around designing the auth UI of your dreams, meanwhile not getting feedback from real users on the core app or site that you actually want to show.\n\nWhat `react-netlify-identity-widget` aims to do is to provide a “good enough” authentication UI for you to get going fast, while offering customizability in styling and being a drop-in solution on virtually any app. To be a drop-in solution, the best UI paradigm is using a modal, which comes with its own accessibility issues, so we lean on the excellent [Reach UI](https://ui.reach.tech/) project to provide accessible components.\n\nTo get going, install:\n\n```bash\n    ## this re-exports react-netlify-identity, no separate install needed\n    npm i react-netlify-identity-widget\n    ## peer dependencies, if you don't already have them\n    npm i @reach/dialog @reach/tabs @reach/visually-hidden\n```\n\nTo use this widget, you set up the `IdentityContextProvider` exactly as above:\n\n```js\nimport { useIdentityContext, IdentityContextProvider } from \"react-netlify-identity-widget\"\n\nfunction App() {\n  const url = \"https://your-identity-instance.netlify.com/\"\n  return <IdentityContextProvider value={url}>{/** rest of your app **/}</IdentityContextProvider>\n}\nexport default App\n```\n\nThe only new things you need to do pertain to rendering the Modal widget, which the default export of the library, as well as (optionally) importing the CSS, if you don’t want to write your own. It is a controlled component, so you just need to pass in a boolean to `showDialog` to indicate if you want it open or closed (as well as give it an `onCloseDialog` callback to close itself):\n\n```js\nimport \"react-netlify-identity-widget/styles.css\"\n// code split the modal til you need it!\nconst IdentityModal = React.lazy(() => import(\"react-netlify-identity-widget\"))\n\nfunction Main() {\n  const identity = useIdentityContext()\n  const [dialog, setDialog] = React.useState(false)\n  const isLoggedIn = identity && identity.isLoggedIn\n  return (\n    <div className=\"App\">\n      <button className=\"btn\" onClick={() => setDialog(isLoggedIn)}>\n        {isLoggedIn ? \"LOG OUT\" : \"LOG IN\"}\n      </button>\n      <React.Suspense fallback=\"loading...\">\n        <IdentityModal showDialog={dialog} onCloseDialog={() => setDialog(false)} />\n      </React.Suspense>\n    </div>\n  )\n}\n```\n\nWhat is that `React.lazy` function and `React.Suspense` component? These are [relatively new React features for code splitting](https://reactjs.org/blog/2018/10/23/react-v-16-6.html) by making [dynamically imported](https://webpack.js.org/guides/code-splitting/#dynamic-imports) components declarative. This way, even though `react-netlify-identity-widget` is a trim [6kb min+gzipped](https://bundlephobia.com/result?p=react-netlify-identity-widget@0.1.1), your user doesn’t pay unnecessary JS import cost until they try to log in, making your app that much snappier to load.\n\nRun your app ([example here](https://react-netlify-identity-widget.netlify.com/?)), click your log in button, and get this modal:\n\n![](https://paper-attachments.dropbox.com/s_33B1E747B4EA51EE372C4DD79ED96CA43B598BF484C23D59491C5FFCCC760A2E_1559884212463_image.png)\n\nThe widget helps to bake in a litany of authentication UI standards you’ll want to consider (or avoid implementing for your MVP’s):\n\n- Login\n- Signup\n- Logout\n- Email confirmation\n- [External Provider authentication](https://www.netlify.com/docs/identity/?utm_source=blog&utm_medium=devto&utm_campaign=devex#external-provider-login) with Google/GitHub/GitLab/Bitbucket\n- (pending) Password Recovery\n\nMore importantly, it takes all the decision-making out of adding an authentication UI on top of your existing app at very little cost. To check against authentication information anywhere in your app (eg for [Protected Routes](https://www.gatsbyjs.org/docs/building-a-site-with-authentication/) or getting a canonical user ID), you simply call `useIdentityContext` just like before.\n\n## Conclusion\n\nWhile this article uses Netlify Identity for its authentication provider, the design patterns we describe can easily be used by any other provider like Auth0, Okta, or one you roll yourself. We simply think these are excellent use cases for combining the best of React’s new features for a fantastic developer experience for authentication, traditionally a time-sucking, undifferentiated feature to add and do well. It is possible that this library may evolve to accept multiple adapters for authentication providers in future - if you are interested in collaborating on one unified API for all authentication in React, [get in touch](https://github.com/sw-yx/react-netlify-identity/issues/new)!\n"
  },
  {
    "slug": "reinforcement-learning-game-theory-336a",
    "data": {
      "title": "Reinforcement Learning: Game Theory",
      "description": "RL with multiple actors",
      "tag_list": [
        "machinelearning",
        "reinforcement"
      ]
    },
    "content": "\n*This is the 19th and last in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n## Why Game Theory in Reinforcement Learning\n\nWhat we know about RL so far has been entirely based on single actors in an environment. However, optimal strategies change significantly when the behavior of other actors is taken into account, which may change the expected payoffs of our own actions. This is why we incorporate Game Theory in RL.\n\n## Minimax\n\nWe define a simple game between two players with a range of choices, with zero sum payoffs (one player's win is another's loss), deterministic results, and perfect information. \n\n```\n           B\n      L    M     R\n\n    L 1   -3     0\nA\n    R 0    3    -1\n```\n\nEven in this situation it is not clear which choice B or A should take. One heuristic to adopt is for A (which wants the lowest number, for example) to always look for the maximum number, and minimize that. And vice versa for B. Each considers the worst case response from the other. So A picks L. This is called minimax.\n\nHowever if B also only pays attention to itself and does minimax (it wants higher values), it will pick M, which ironically hands A the -3 it wants. So B must consider what A is likely to do in formulating it's own actions. In this case it is best to pick L where the worst case scenario isn't so bad.\n\n## Von Neumann's theory\n\n> In a 2 player, zero sum deterministic game of perfect information, `minimax` === `maximin` and there always exists an optimal **pure strategy** for each player.\n\nThis is to say, there is always a definite value of the game if we assume the players follow `minimax` or `maximin`, and they are the same.\n\nMore reading: https://cs.stanford.edu/people/eroberts/courses/soco/projects/1998-99/game-theory/neumann.html\n\nas well as the [Theory of Games and Economic Behavior](https://en.wikipedia.org/wiki/Theory_of_Games_and_Economic_Behavior)\n\nand [Andrew Moore on Zero-Sum Games](http://www.cs.cmu.edu/~./awm/tutorials/gametheory.html)\n\n## Relaxing assumptions\n\nIf we relax the requirement of **determinism**, we get the same result under Von Neumann - just that all choices are made based on expected value.\n\nIf we relax the requirement of **perfect information**, a lot of complexity comes into the picture. Von Neumann is broken.\n\nThe reliance on a **pure** strategy is important for Von Neumann. And if each player is too consistent, they become predictable, and thus exploitable. Thus a **mixed** strategy varies strategies based on a probability distribution. However if for every `p` selected by A, a corresponding mixed strategy governed by `p'` can be chosen by B to offset it, there is still a probabilistic equilibrium where the payoffs meet:\n\n{% youtube d2gr9VrSNTQ %}\n\nLastly, if we relax the assumption of a **zero-sum game**, we can get situations like the [Prisoner's Dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma) where cooperation can result in better average outcomes than each can independently get, if they don't betray the other. However because the betrayal strategy dominates in each scenario independently, \n\nSee [Andrew Moore's slides on Non Zero Sum Games](http://www.cs.cmu.edu/~./awm/tutorials/nonzerosum.html)\n\n## Nash Equilibrium\n\nGiven `n` players with strategies, you know you are in **Nash Equilibrium** if you take any player and changed their strategy, and they would be worse off. No one person has any reason to change their strategy. [There are a few other minor results that come out of this definition](https://www.youtube.com/watch?v=rz9cuSIrwGs). \n\n## Repeated games\n\nMost games discussed up til this point are finite, one-off games. What if we could play several rounds of the game, taking into account information from prior rounds? This causes a form of communication of strategy to take place as well as establishes consequences of taking selfish strategies where a positive sum result could have been possible. As such, it causes strategies to lean towards cooperating.\n\nThis can be modeled as \"trust\", but only makes sense if the total number of games is unknown. (By induction, if you know you are at the last round of a repeated game, then it is a one-off game, so the second last round needs to take that into consideration, and so on all the way to the first round. So `n` repeated games with a finite, known `n` simply lead to `n` repeated Nash Equilibria.).\n\nSo assuming an unknown number of games (with a probability of ending `gamma`, which looks remarkably similar to a discount factor), a great strategy to encourage collaboration is the [Tit for Tat strategy](https://en.wikipedia.org/wiki/Tit_for_tat). This establishes communication and consequences between rounds, socializing the result of the net suboptimal outcome. It turns out that TfT is best suited for gamma values above a certain level dependent on the payoffs, because if not then the benefit of being selfish outweighs the odds of consequences.\n\n## The Folk Theorem of Repeated Games\n\nTit for Tat is an example of the general idea that **the possibility of retaliation opens the door for cooperation**. In game theory, the term \"[folk theorem](https://en.wikipedia.org/wiki/Folk_theorem_(game_theory))\" refers to a specific set of payoffs that can result from Nash strategies in repeated games.\n\nThe **Minmax profile** is a pair of payoffs, one for each player, that represent the payoffs that can be achieved by a player defending itself from a malicious adversary. ([More description](https://www.youtube.com/watch?v=Yn4zmWwy9mA))\n\nThe **feasible payoff profile** is any mix of available strategies: \n\n![https://vknight.org/Year_3_game_theory_course/Content/images/L10-img01.png](https://vknight.org/Year_3_game_theory_course/Content/images/L10-img01.png)\n\nSome range of feasible payoffs is going to be better than whatever can be individually achieved (incentive for cooperation), leading to the Folk Theorem:\n\n> Any feasible payoff profile that strictly dominates the minmax/security level profile can be realized as a Nash equilibrium payoff profile, with sufficiently large discount factor.\n\nIn other words, if you refuse to cooperate, I can push you down to the minmax profile, instead of letting you get benefits better than you can get on your own.\n\n## Subgame Perfect\n\nThe threat cannot be too extreme as to be implausible - to the extent of harming your own interests just to punish my actions. The formal term for a plausible threat is [subgame perfect](https://en.wikipedia.org/wiki/Subgame_perfect_equilibrium) - where you are giving the best response independent of history. \n\nImplausible threats like the [Grim trigger](https://en.wikipedia.org/wiki/Grim_trigger) are not subgame perfect because you can always drop the strategy to improve your own interests.\n\nThe combination of TfT vs TfT is *not* subgame perfect, because instead of instant retaliation you can pick forgiveness.\n\nThere is a variant based on agreement/disagreement called the [Pavlov strategy](https://www.frozenevolution.com/pavlov-strategy) that offers more memory, that -is- subgame perfect, because disagreements lead to defections which lead to agreements which lead to cooperation. Wonderful! Now it becomes a plausible threat that is better to enforce than TfT. ([More on this](http://www.pnas.org/content/93/7/2686.short)) \n\n## Stochastic Games (or Markov Games)\n\nThis is a generalization of MDPs and Repeated Games that is a formal model for Multiagent Reinforcement Learning.\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [class notes](https://www.ritchieng.com/machine-learning-game-theory/)\n- Paper on [Computational Folk theorem](http://jmvidal.cse.sc.edu/library/littman03a.pdf) which shows you can construct subgame perfect Nash eq. for any game in polynomial time.\n- Coco values - adding side payments to encourage nash equilibria ([paper](http://proceedings.mlr.press/v28/sodomka13.pdf))\n\nHopefully that was a good introduction to Game Theory. Feedback and questions are welcome. The entire series is viewable here:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - [Clustering](https://dev.to/swyx/unsupervised-learning-clustering-42mi)\n    - [Feature Selection](https://dev.to/swyx/unsupervised-learning-feature-selection-84f)\n    - [Feature Transformation](https://dev.to/swyx/unsupervised-learning-feature-transformation-pcf)\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - [Game Theory](https://dev.to/swyx/reinforcement-learning-game-theory-j1o)"
  },
  {
    "slug": "unsupervised-learning-feature-transformation-pcf",
    "data": {
      "title": "Unsupervised Learning: Feature Transformation",
      "description": "Presenting the same information a different way... helps! Plus, one algorithm that does better than Principal Components Analysis!",
      "tag_list": [
        "machinelearning",
        "unsupervisedlearning"
      ]
    },
    "content": "\n*This is the 16th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n![https://sw-yx.tinytake.com/media/9b47c0?filename=1553152842514_21-03-2019-01-20-41.png&sub_type=thumbnail_preview&type=attachment&width=699&height=431&&salt=MzQwMjExMV8xMDE3NjQ0OA](https://sw-yx.tinytake.com/media/9b47c0?filename=1553152842514_21-03-2019-01-20-41.png&sub_type=thumbnail_preview&type=attachment&width=699&height=431&&salt=MzQwMjExMV8xMDE3NjQ0OA)\n\n## Feature Transformation\n\nThe problem of preprocessing a set of features to create a new (smaller/more compact) feature set, while retaining as much (relevant? useful?) information as possible. For example in linear algebra terms, you can create new features that are some linear combination of existing features.\n\n## Why do Feature Transformation as a standalone topic?\n\nAny problem with a large amount of features that have plenty of false positives and negatives will want some form of Feature Transformation.\n\nOne example: The information retrieval, or ad hoc retrieval problem, like with Google needing to look up documents based on given search terms. For google, all the words in a document are features (give or take some cleaning). That's a lot of words, bringing us the curse of dimensionality. But most relevantly here, the words also aren't very good indicators (because of polysemy and synonymy) which cause a lot of false positives and negatives. Doing feature selection will not solve this problem.\n\n## [Principal Components Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)\n\nIt is an example of an [Eigenproblem](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors). PCA finds the directions of maximum variance that are mutually orthogonal. You are able to perfectly reconstruct (or describe) your data with PCA, but most commonly\nif you take a subset of the principal components you will minimize the L2 error for the dimensions you pick. \n\n**Eigenvalues and Dimensionality Reduction**\n\nEigenvalues indicate importance of each PC, and a 0 eigenvalue means that dimension is irrelevant. PCA does dimensionality reduction by sorting eigenvalues and picking the top `k` values. You can visualize this with a [Scree Plot](https://en.wikipedia.org/wiki/Scree_plot)\n\n![https://upload.wikimedia.org/wikipedia/commons/a/ac/Screeplotr.png](https://upload.wikimedia.org/wikipedia/commons/a/ac/Screeplotr.png)\n\nYou use PCA to transform features to a new space where you know how to do the filtering.\n\n## Independent Components Analysis\n\nPCA is about finding correlation by maximizing variance.\n\nICA tries to maximize independence. Transforming a set of features X into new features Y with no mutual information between any Y feature, but maximum mutual info between X and Y (being able to reconstruct the data).\n\nThis is applied in the Blind Source Separation problem (aka the Cocktail Party Problem) - where microphones try to isolate sound of individual people despite picking up extra noise. Each microphone receives some linear combination of the multiple sources, and you can model it with ICA and work backwards to create sound streams of individual voices.\n\nIn contrast to PCA, ICA is local and unsorted. It just focuses on unique traits, which is a very nice feature to have for sound and natural images - they detect edges in images! When you apply this to our original problem of documents, ICA gives you topics - collections of words that select for specific topics.\n\n## Alternatives\n\n**RCA: Random Components Analysis** instead of finding dimensions with highest variance, it just picks random dimensions. It works remarkably well if the next thing you do is a form of classification because you still maintain information, but have lower dimensions so you still pick up some correlations. It's less efficient but faster than PCA.\n\n**LDA: Linear Discriminant Analysis** finds a projection that discriminates based on the label (like supervised learning) - separates based on label. After the 90's, Latent Dirichlet Allocation has become the more popular LDA.\n\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [Independent Component Analysis](http://mlsp.cs.cmu.edu/courses/fall2012/lectures/ICA_Hyvarinen.pdf)\n- [Great short set of slides on PCA vs ICA](http://compneurosci.com/wiki/images/4/42/Intro_to_PCA_and_ICA.pdf)\n\nHopefully that was a good introduction to Feature Transformation. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - [Clustering](https://dev.to/swyx/unsupervised-learning-clustering-42mi)\n    - [Feature Selection](https://dev.to/swyx/unsupervised-learning-feature-selection-84f)\n    - [Feature Transformation](https://dev.to/swyx/unsupervised-learning-feature-transformation-pcf)\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "unsupervised-learning-feature-selection-84f",
    "data": {
      "title": "Unsupervised Learning: Feature Selection",
      "description": "Breaking the Curse of Dimensionality!!",
      "tag_list": [
        "machinelearning",
        "unsupervisedlearning"
      ]
    },
    "content": "\n*This is the 15th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n![https://pedropb.github.io/machine-learning-nanodegree/classes/unsupervised-learning/images/filtering-wrapping-comparison-2.png](https://pedropb.github.io/machine-learning-nanodegree/classes/unsupervised-learning/images/filtering-wrapping-comparison-2.png)\n\n## The Goal of Feature Selection\n\nThere are often too many features which might be important in our data. Since even in the binary case these cause 2^N permutations, it is good to try to reduce N. Imagine out of 1000 possibly important features you were able to narrow it down to 10 that really matter. Intuitively, this is the same as interpreting the features (knowledge discovery).\n\nFeature Selection (aka optimization over arbitrary number of parameters) is known to be NP-hard. Solving it takes exponential time.\n\nThere are two general approaches to tackling this.\n\n## Filtering and Wrapping\n\n**Approach 1: Filtering**\n\nReducing **many features to a set of fewer features** through a maximizing (aka \"search\") algorithm of some sort, that you then pass to your learning algorithms. The scoring is buried in the search algorithm, without reference to the learner.\n\nThe process flows forward, which is simple to set up, but the problem is that there isn't any feedback.\n\nAlthough you can't run the learner ahead of time in filtering, you can still evaluate the features, for example for information gain. You could in fact have your search algorithm just be a decision tree, and pass the nodes used in the DTree to your learner as your reduced features. So that is like using the inductive bias of DTrees to choose features, then passing it on to your other learner (eg NN or kNN) with some other bias. This is particularly helpful when this other learner (like NN or kNN) suffers from the curse of dimensionality.\n\nYou can also use other things as filtering criteria:\n\n- Information Gain\n- proxies of Information Gain, e.g. Variance, Entropy, running a neural net and pruning away low weight features so \"useful\" features remain\n- Indepdenent/Non Redundant Features\n\n**Approach 2: Wrapping**\n\nRun your learning algorithm over a subset of features, **the learning algorithm reports a score, and that is the driver for your maximizing (aka \"search\") algorithm**. The search for features is wrapped around whatever your learning algorithm is. Feedback is inbuilt.\n\nYou can use a few methods to do Wrapping search without paying exponential (2^N) cost:\n\n- any randomized optimization method (e.g. Hill Climbing)\n- forward sequential selection\n- backward elimination\n\nForward search involves running sequentially through each feature, and seeing which adds to the best you currently have. Repeat (running again through each remaining feature) until your score no longer improves by enough. This is a sort of hill climbing.\n\nBackward search involves starting with ALL N features and looking for which you can eliminate. Whichever set of N-1 features work out best go on to the next round and you drop the unlucky feature. Repeat until the score drops _too_ much.\n\n**Comparison**\n\nFiltering is faster than wrapping because you can apply any algorithm that only looks at the features and applies filtering (no learning involved). Although it is an exponential problem, you can choose your filtering to be as fast as you want it to be.\n\nWrapping takes into account model and bias, but is muuuuch slooooower because the learning has to be done.\n\nFiltering only cares about labels, while Wrapping takes advantage of the learner, but with an exponential search problem.\n\n## Relevance vs Usefulness\n\nThere are formal terms we want to define here.\n\nRelevance measures the effect on the Bayes Optimal Classifier (BOC):\n\n- `Xi` is strongly relevant if removing it degrades BOC\n- `Xi` is weakly relevant if\n  - not strongly relevant\n  - there is a subset of features S such that adding Xi to S improves BOC\n- else it is irrelevant\n\nRelevant is about _information_.\n\nUsefulness measures effect on _particular predictor_, more precisely, the effect on error given a particular classifier.\n\nFeature selection is often an exercise in selecting the most relevant features, but often what we actually want is usefulness.\n\nWrapping is slow but **useful**, Filtering tries to optimize for relevance but ignores the bias in our learner that we actually want.\n\n\n## Next in our series\n\nHopefully that was a good introduction to Feature Selection. I am planning more primers and would love your feedback and questions on:\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - [Clustering](https://dev.to/swyx/unsupervised-learning-clustering-42mi)\n    - [Feature Selection](https://dev.to/swyx/unsupervised-learning-feature-selection-84f)\n    - [Feature Transformation](https://dev.to/swyx/unsupervised-learning-feature-transformation-pcf)\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "unsupervised-learning-clustering-42mi",
    "data": {
      "title": "Unsupervised Learning: Clustering",
      "description": "Single Linkage, K-Means, Soft Clustering, and Kleinberg Impossibility",
      "tag_list": [
        "machinelearning",
        "unsupervisedlearning"
      ]
    },
    "content": "\n*This is the 14th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n![https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/450px-ClusterAnalysis_Mouse.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/450px-ClusterAnalysis_Mouse.svg.png)\n\n## Defining the basic Clustering Problem\n\nWe want to take a set of objects and put them into groups.\n\nFormally,\n\n```\nGiven:\n- set of objects X\n- inter object distances D(x,y)\n\nOutput:\n- Partition Pd(x) = Pd(y)\n  if x & y in same cluster\n```\n\n`D`, The distance algorithm, can be any similarity metric you choose depending on your domain knowledge.\n\n## Single Linkage Clustering\n\nThe simplest clustering algorithm is Single Linkage.\n\n- start by considering each object a cluster (so you have n objects and n clusters)\n- define intercluster distance as the distance between the closest 2 points in the two clusters\n- merge the two closest clusters\n- repeat `n-k` times to arrive at `k` clusters.\n\nThis helps you build a Hierarchical Agglomerative Cluster Structure. By cutting off the tree at `k` levels from the top you can get a predetermined number of clusters, which is handy.\n\nInstead of the _closest_ 2 points, you can also modify this algorithm to use the _furthest_ 2 points or the average (mean, median) point of each cluster.\n\nSLC is deterministic, and also generates a minimum spanning tree. The running time is `O(n^3)`. However it generates some nonintuitive clusters because of the \"closest points\" rule. So we need something better.\n\n## K-Means Clustering\n\nTry:\n\n- Pick k \"centers\" at random\n- Each center \"claims\" all its closest points\n- recompute centers by averaging the clustered points\n- repeat until converged (recomputing doesn't have any change)\n\nCenters do not have to be any of the points, its just the \"center\" of the cluster.\n\nK-Means can be viewed as an optimization problem, in that we are always trying to get better and better answers with each round. To use optimization terminology, we are saying that we can **score** each set of **configurations** (the centers), and we try to move from **neighborhood** to neighborhood trying to improve.\n\nIf we assume we know the \"true\" partitions, `P(x)`, we can sum the squared errors between `center of P(x) - x` for an Error score to minimize. \"Neighborhood\" is a bit less intuitive - its the set of possible moves at the end of a round, basically, you can either move the partition, or you can move the centers.\n\nYou can prove that K-means Clustering:\n\n- always improves or statys the same at each step (when you move partitions, you only move if error goes down, and when you move centers, by recomputing the center you immediately jump to the least error possible given the partitions) aka \"monotonically non-increasing in Error\"\n- converges (because you have a finite number of configurations) as long as you have a way to do consistent tie breaking\n\nIf you recall the Randomized Optimization chapter, this sounds a lot like hill-climbing, where you pick and move to neighbors based on their score. This also means it can get stuck in local optima (bad clustering based on the random starting point), so the only fix is random restarts.\n\nThere's also an edge case where if a point is somehow equidistant between two perfectly converged clusters, it would nondeterministically be either part of one or the other. To deal with that we'll use an algorithm where sharing is allowed...\n\n## Soft Clustering (Best!) also known as Expectation Maximization or Gaussian Mixing\n\nThe main trick here is to do like we did with MIMIC and assume points come from a probability distribution. Try:\n\n- Assume the data was generated by 1 of `k` possible Gaussians with known variance\n- Sample Xi from that Gaussian\n- Repeat n times\n\nThe task is to find a hypothesis `h=<u1, u2,... uk>` of \"k means\" that maximizes the probability of the data (aka Maximum Likelihood). The Max Likelihood mean of the gaussian is just the mean of the data, so there's nothing really new here. However you can use `k` Hidden Variables (`[0,1]` membership in Partition 1 to `k`) run the algorithm in a similar way to k-Means Clustering. Instead of always improving in the Error metric, you're improving in the probabilistic metric. Running the algorithm is a process of expectation maximization.\n\nProperties of Expectation Maximization:\n\n- monotonically non-decreasing likelihood\n- theoretically might not converge (because Gaussian has an infinite extent, but in practice it does)\n- cannot diverge\n- can get stuck\n- works with any distribution\n\n## Practical issue: Picking `k`: Elbows, Silhouettes, and more\n\nAlthough you can lean on domain knowledge to pick `k`, there can algorithms to help pick [the \"Elbow\", the \"best\" point given a tradeoff between number of clusters vs explanatory power](https://en.wikipedia.org/wiki/Elbow_method_%28clustering%29).\n\n![https://upload.wikimedia.org/wikipedia/commons/c/cd/DataClustering_ElbowCriterion.JPG](https://upload.wikimedia.org/wikipedia/commons/c/cd/DataClustering_ElbowCriterion.JPG)\n\nYou can choose any number of \"good\" metrics to quantify cluster quality: [adj. Mutual Information](https://en.wikipedia.org/wiki/Adjusted_mutual_information), Homogeneity, Completeness, and [V_Measure](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html) ([see original paper](http://www.aclweb.org/anthology/D07-1043)).\n\nMany ways to do this exist, see [the Wikipedia article for more](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set).\n\n[Silhouette profiles](https://en.wikipedia.org/wiki/Silhouette_(clustering)) can help visually inspect the appropriateness of clusters.\n\n## Practical issue: Visualizing Clusters\n\nUsing `t-SNE` is very helpful:\n\n{% youtube NEaUSP4YerM %}\n\nA better alternative may have emerged in Feb 2018: [UMAP](https://github.com/lmcinnes/umap), and [the benefits are worth considering](https://github.com/lmcinnes/umap#benefits-of-umap). Here is the author explaining the idea:\n\n{% youtube nq6iPZVUxZU %}\n\n## (theoretical aside) Kleinberg Impossibility Theorem\n\nThere are three desirable properties of clustering algorithms:\n\n- Richness (can describe any clustering as long as we tweak variables)\n- Scale invariance (works the same regardless of units)\n- Consistency (clusters the same every time)\n\nSLC is consistent and but not Rich.\nYou can modify SLC's stopping point to make it richer, but you will lose scale invariance.\n\nSo on and so forth. This is a proven theorem, that these three properties are mutually contradictory.\n\nJon Kleinberg's original paper is called [An impossibility theorem for clustering](https://www.cs.cornell.edu/home/kleinber/nips15.pdf).\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [The Expectation Maximization Algorithm](https://www.cc.gatech.edu/~dellaert/em-paper.pdf)\n- [K-Means vs Gaussian Mixture Models](https://davidrosenberg.github.io/ml2015/docs/13.mixture-models.pdf) - a nice slide deck\n- [Supervised Learning of Gaussian Mixture Models for\nVisual Vocabulary Generation](https://basurafernando.github.io/papers/pr1.pdf) - a useful application\n\nHopefully that was a good introduction to Clustering. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - [Clustering](https://dev.to/swyx/unsupervised-learning-clustering-42mi)\n    - [Feature Selection](https://dev.to/swyx/unsupervised-learning-feature-selection-84f)\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "unsupervised-learning-information-theory-recap-4iem",
    "data": {
      "title": "Unsupervised Learning: Information Theory Recap",
      "description": "A small detour to catch up on the basics of Information Theory we'll need",
      "tag_list": [
        "machinelearning",
        "unsupervisedlearning"
      ]
    },
    "content": "\n*This is the 13th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n> ⚠️This is a more theoretical chapter, feel free to skip if not directly interested.\n\n## The Central Question\n\nIn a machine learning problem, we want to be able to ask questions like \"how is X1 or X2 related to Y\", or \"are X1 and X2 related\" (mutual information). In general these machine learning problems can be modeled as a probability density function. So **information theory is a mathematical framework which allows us to compare these density functions.**\n\n## Information Theory History\n\n[Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon). End of story 😅\n\n## Entropy and the Measurement of information\n\nWe measure information in terms of the minimum number of bits we need to encode messages. 10 Coin Flips = 10 bits.\n\n![http://lisathorne.files.wordpress.com/2009/10/tree-diagram.png?w=300&h=115](http://lisathorne.files.wordpress.com/2009/10/tree-diagram.png?w=300&h=115)\n\nAnother way of looking at amount of information is the number of granular questions you have to ask in order to determine what the message is. The number of bits per symbol is also know as \"entropy\".\n\n## Variable Length Encoding\n\n*[Wikipedia reading](https://en.wikipedia.org/wiki/Variable-length_code)*\n\nHowever encoding doesnt have to be strictly based on uniform distribution and even trees - we can change our encoding around based on the probability of more frequent bits of information to save on the *average* bits per message (aka expected size).\n\n![http://computationstructures.org/lectures/info/slides/Slide18.png](http://computationstructures.org/lectures/info/slides/Slide18.png)\n\n## Joint and Conditional Entropy\n\nSome variables have covariance with others. For example, the probability that it is raining (X) and the probability that there will be thunder (Y). This can also be expressed as Joint vs Conditional Entropy:\n\n![https://slideplayer.com/slide/6013317/20/images/18/Joint+and+conditional+entropy.jpg](https://slideplayer.com/slide/6013317/20/images/18/Joint+and+conditional+entropy.jpg)\n\nIf X and Y are independent, then the conditional entropy is just the same as the individual entropies, and the joint entropy is the SUM of the individual entropies.\n\n## Mutual Information\n\nAlthough Joint and Conditional entropy give us some insight of independence, they aren't direct measures of *de*pendence. For example, if `H(Y|X)` is small, we don't know if that's because `X` gives a lot of information about `Y`, or if `H(Y)` is just small to begin with.\n\nA better measure normalizes this: \n\n```\nI(X,Y) = H(Y) - H(X|Y)\n```\n\nand we call it **Mutual information.**\n\nSee [further notes on derivation of the Mutual Information formula](http://www.robotvisions.org/4641/downloads/InfoTheory.fm.pdf).\n\n\n## Example\n\nGiven this system of equations you can verify that, for 2 independent coins:\n\n- Individual entropy is 1\n- Conditional entropy is 1\n- Joint entropy is 2 (additive)\n- Mutual Information is 0 (no shared information)\n\n{% youtube LQJ4PQBhDd4 %}\n\nWhich are intuitive. [Vice versa for the fully dependent case](https://www.youtube.com/watch?v=P5GpR-9XVWQ).\n\n## Kullback-Leibler (KL) Divergence\n\nMutual Information is just a special case of KL Divergence - it is a measure of distance or divergence between any two distributions.\n\n![https://wikimedia.org/api/rest_v1/media/math/render/svg/726edcd02293461b82768ea2fd299c3a3ef16112](https://wikimedia.org/api/rest_v1/media/math/render/svg/726edcd02293461b82768ea2fd299c3a3ef16112)\n\nNote it is always non-negative and equals zero only when `p = q` for all x.\n\nIn Machine Learning we can use KL Divergence as an alternative to least-squares fitting of our dataset. (to be explained in later chaptesr)\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [Maxwell's demon](https://en.wikipedia.org/wiki/Maxwell%27s_demon) - an early thought experiment that tried to contradict the second law of thermodynamics, and therefore reverse entropy.\n- [Wikpedia on KL Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n- [An Introduction to Information Theory and Entropy](https://github.com/pushkar/4641/raw/master/downloads/gentle_intro_to_information_theory.pdf) - [with a clearer 8-page summary here](https://github.com/pushkar/4641/raw/master/downloads/InfoTheory.fm.pdf)\n\nHopefully that was a good introduction to Information Theory. I am planning more primers and would love your feedback and questions on:\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - [Clustering](https://dev.to/swyx/unsupervised-learning-clustering-42mi)\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "unsupervised-learning-randomized-optimization-4c1i",
    "data": {
      "title": "Unsupervised Learning: Randomized Optimization",
      "description": "Hill Climbing, Simulated Annealing, Genetic Algorithms, oh my!",
      "tag_list": [
        "machinelearning",
        "unsupervisedlearning"
      ]
    },
    "content": "\n*This is the 12th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n> This marks the start of a new miniseries on Unsupervised Learning, [the 2nd of 3 sub disciplines within Machine Learning](https://dev.to/swyx/machine-learning-an-overview-216n).\n\n## Our Goal: Optimization\n\nGiven an input space `X`, and an objective (aka fitness) function `f(x)`, Find `x` from `X` such that `f(x)` is the maximum possible it can be. \n\nThis is similar to the [argmax](https://en.wikipedia.org/wiki/Arg_max) function we have used in prior formula, but now we are actually exploring methods to do it.\n\n## When Traditional Methods fail\n\nGiven a small `X`, we can simply run a `for` loop [like this one](https://repl.it/@swyx/ML-optimization-practice) and keep track of the highest values.\n\nGiven an infinite `X` but with a solvable `df(x)`, we can use calculus to help us find optima.\n\nIf `df(x)` can't be solved, we can use the [Newton Raphson method](https://en.wikipedia.org/wiki/Newton%27s_method) to iteratively get closer and closer to the optimum. However, it can get stuck in local optimum.\n\nBut what happens when we don't have a derivative to work with? (for example, you don't even know what f(x) is, you just know the output without knowing its theoretical maximum) and possibly many local optima?\n\n## Method 1: Random Restart Hill Climbing\n\nThe simple solution is, to guess a few starting points and then guess a direction and try to do some [random restart hill climbing](https://en.wikipedia.org/wiki/Hill_climbing). \n\n![https://i.stack.imgur.com/HISbC.png](https://i.stack.imgur.com/HISbC.png)\n\n\nHowever, to do it right, you'd have to do a number of restarts and it turns out to be not a lot better than just looping through the entire space especially for spaces with narrow global optima basins.\n\nCan we do better than that?\n\n## Method 2: Simulated Annealing\n\nRandom Restart Hill Climbing combines **exploring** (random restart) with **exploiting** (climbing). Maybe we can split that up. \n\n[Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing) takes its name from metallurgy where repeatedly heating up and cooling down a sword makes it stronger than before.\n\n![http://2.bp.blogspot.com/--kOlrodykkg/UbfVZ0_l5HI/AAAAAAAAAJ4/0rQ98g6tDDA/s1600/annealingAtoms.png](http://2.bp.blogspot.com/--kOlrodykkg/UbfVZ0_l5HI/AAAAAAAAAJ4/0rQ98g6tDDA/s1600/annealingAtoms.png)\n\nThe pseudocode for this looks like:\n\n```\nStart with a temperature T\nFor a finite set of iterations:\n\n- sample new point x+\n- jump to a new point with probability given by an acceptance probability function P(x, x+, T)\n- decrease temperature T\n\nwhere \n\nP(x, x+, T) = 1 if f(x+) >= f(x) else e^((f(x+) - f(x))/T)\n```\n\nSo a high T makes the algorithm behave like a random sampler, and as it cools, it starts to behave more like a hill climber.\n\n![https://slideplayer.com/slide/8038378/25/images/6/Convergence+of+simulated+annealing.jpg](https://slideplayer.com/slide/8038378/25/images/6/Convergence+of+simulated+annealing.jpg)\n\nA remarkable analytical result about SA is that the probability of it ending at X is equal to `e ^ (f(x) / T) / Z(T)` ([a Boltzmann distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution)) which makes the likelihood of its ending point directly related to its ending point's fitness.\n\n## Method 3: Genetic Algorithms\n\nEvolution is a pretty good optimizer, and we can simulate that by \"breeding\" individual points to produce hopefully better offspring and \"mutating\" them by doing local search. Over multiple \"generations\", the hope is that we evolve towards the globally optimum result. One feature that helps a lot is if the individual dimensions/attributes can be additively combined, so that breeding two parents can regularly produce better offspring.\n\n![https://image.slidesharecdn.com/gasbytheseatofyourgenes-160629032611/95/genetic-algorithms-programming-by-the-seat-of-your-genes-6-638.jpg?cb=1467171105](https://image.slidesharecdn.com/gasbytheseatofyourgenes-160629032611/95/genetic-algorithms-programming-by-the-seat-of-your-genes-6-638.jpg?cb=1467171105)\n\nTo contrast this with the other methods, GAs are like random restarts done in parallel (since each individual in a population is like a thread, and each generation is like a restart), EXCEPT that the \"crossover\" of information when \"breeding\" happens helps to direct the general direction of evolution towards better places.\n\nThe pseudo-code looks like:\n\n```\nStart with an initial population of size K\nRepeat until converge:\n  - compute fitness of all population\n  - select \"most fit\" individuals\n  - pair up individuals, replacing \"least fit\" via crossover/mutation\n```\n\nAs you might imagine, the secret sauce is in the breeding. Two methods of doing preserving individuals are:\n\n- truncation selection - for example, only keeping the top half of the population\n- roulette wheel selection - taking a probabilistic approach to keeping each individual according to their fitness\n\nCrossover/Mutation also has nuances. Deciding on what attributes can be treated as a group involves domain knowledge and implicit assumptions that may or may not reflect reality. A special case is \"one point crossover\", where if an individual has n bits, a child takes n/2 bits from one parent and the remaining 1-n/2 bits from another parent. This implicitly assumes some grouping in the left and right halves which may or may not reflect reality. \"Uniform crossover\" makes every bit randomly inherited from each parent instead.\n\n## More Randomized Optimization\n\nAs you can see, there are many ways to tackle the problem of optimization without calculus, but all of them involve some sort of random sampling and search. \n\nThose we have explored don't have much in the way of memory or of actually learning the structure or distribution of the function space, but there are yet more algorithms that explore those abilities:\n\n- [Tabu Search](https://en.wikipedia.org/wiki/Tabu_search) adds memory to the search (stay away from \"taboo\" regions)\nhttps://www.cc.gatech.edu/~isbell/tutorials/mimic-tutorial.pdf\n- [MIMIC: Finding Optima by Estimating Probability Densities](https://www.cc.gatech.edu/~isbell/papers/isbell-mimic-nips-1997.pdf) estimates a probability distribution in lieu of a genetic algorithm metaphor.\n\n## MIMIC\n\nMIMIC has a simple idea at its core - define a uniform probability distribution of points with a fitness over a threshold `theta`:\n\n![https://i.ytimg.com/vi/49Y2C_mTWD0/maxresdefault.jpg](https://i.ytimg.com/vi/49Y2C_mTWD0/maxresdefault.jpg)\n\nWhen theta is at a minimum, that means we uniformly sample over the entire space.\n\nWhen theta is at a maximum, that means we converge on one or more global optima.\n\n\n```\nat every iteration:\n\n- Generate samples from population consistent with the distribution so far\n- set `theta` to the nth percentile\n- retain only those samples where f(x) > `theta`\n- estimate the new distribution based on these samples,\n- repeat\n```\n\nTherefore, under MIMIC, our task is to continually raise `theta` until we just have small regions around our optima left. In the process of doing so, we infer a (uniform) probability distribution around our optima. Because we use the `nth` best percentile as our cutoff factor, this is similar to genetic algorithms, however there is no longer a concept of breeding or genes. Instead, we stay laser focused on performance against the fitness function as our one metric.\n\nThe comparison with genetic algorithms isn't accidental - the best version of MIMIC estimates distributions by assuming that the conditional distributions inherent in the space fit to \"dependency trees\", which are [Bayesian Networks](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72) where every node (but one) has exactly one parent. Finding these trees is out of scope but [check](https://www.youtube.com/watch?v=R-Mf9-tKC5o) [the](https://www.youtube.com/watch?v=u2V7esTMhKc) [videos](https://www.youtube.com/watch?v=rCxGiKQoe4w) for the math, but the theoretical grounding of this approach is rooted in maximizing [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) (in information theory terms) and building a [Maximum Spanning Tree](http://shodhganga.inflibnet.ac.in/bitstream/10603/33822/4/chapter4.pdf). While not the only way to estimate the next probability distribution, Dependency Trees allow capturing of parent-child structure without having exponential cost (Max Spanning Trees are only polynomial in cost). This is what MIMIC stands for: Mutual-Information-Maximizing Input Clustering!\n\n\nBenefits:\n\n- The fact that MIMIC's \"rising theta\" approach can be used with any underlying probability distribution is pretty cool - here's a [simple applied example](https://www.youtube.com/watch?v=4DtVLCC73bA).\n- Being able to learn structure via the distribution assumption helps with generalizability\n- Empirical tests show MIMIC using 2-3 orders of magnitude less iterations to converge compared to Simulated Annealing and other alternatives, however, this is offset by each iteration taking much longer because of the need to estimate and generate from probability distributions. Thus, in terms of overall time taken, MIMIC works better when cost of evaluating `f(x)` is high (e.g. rocket simulation, antennae design, human test subjects)\n\n\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [Wikipedia on Randomized Optimization](https://en.wikipedia.org/wiki/Random_optimization)\n- [The original MIMIC paper](https://www.cc.gatech.edu/~isbell/papers/isbell-mimic-nips-1997.pdf)\n- [No Free Lunch Theorem of Optimization](https://github.com/pushkar/4641/raw/master/downloads/nfl-optimization-explanation.pdf) - an explanation of why optimizations can only improve on others if they build in certain assumptions about structure\n\nHopefully that was a good introduction to Randomized Optimization. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "unsupervised-learning-randomized-optimization-d2j",
    "data": {
      "title": "Unsupervised Learning: Randomized Optimization",
      "description": "Hill Climbing, Simulated Annealing, Genetic Algorithms, oh my!",
      "tag_list": [
        "machinelearning",
        "unsupervisedlearning"
      ]
    },
    "content": "\n*This is the 12th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n> This marks the start of a new miniseries on Unsupervised Learning, [the 2nd of 3 sub disciplines within Machine Learning](https://dev.to/swyx/machine-learning-an-overview-216n).\n\n## Our Goal: Optimization\n\nGiven an input space `X`, and an objective (aka fitness) function `f(x)`, Find `x` from `X` such that `f(x)` is the maximum possible it can be. \n\nThis is similar to the [argmax](https://en.wikipedia.org/wiki/Arg_max) function we have used in prior formula, but now we are actually exploring methods to do it.\n\n## When Traditional Methods fail\n\nGiven a small `X`, we can simply run a `for` loop [like this one](https://repl.it/@swyx/ML-optimization-practice) and keep track of the highest values.\n\nGiven an infinite `X` but with a solvable `df(x)`, we can use calculus to help us find optima.\n\nIf `df(x)` can't be solved, we can use the [Newton Raphson method](https://en.wikipedia.org/wiki/Newton%27s_method) to iteratively get closer and closer to the optimum. However, it can get stuck in local optimum.\n\nBut what happens when we don't have a derivative to work with? (for example, you don't even know what f(x) is, you just know the output without knowing its theoretical maximum) and possibly many local optima?\n\n## Method 1: Random Restart Hill Climbing\n\nThe simple solution is, to guess a few starting points and then guess a direction and try to do some [random restart hill climbing](https://en.wikipedia.org/wiki/Hill_climbing). However, to do it right, you'd have to do a number of restarts and it turns out to be not a lot better than just looping through the entire space especially for spaces with narrow global optima basins.\n\nCan we do better than that?\n\n## Method 2: Simulated Annealing\n\nRandom Restart Hill Climbing combines **exploring** (random restart) with **exploiting** (climbing). Maybe we can split that up. \n\n[Simulated Annealing](https://en.wikipedia.org/wiki/Simulated_annealing) takes its name from metallurgy where repeatedly heating up and cooling down a sword makes it stronger than before.\n\n![http://2.bp.blogspot.com/--kOlrodykkg/UbfVZ0_l5HI/AAAAAAAAAJ4/0rQ98g6tDDA/s1600/annealingAtoms.png](http://2.bp.blogspot.com/--kOlrodykkg/UbfVZ0_l5HI/AAAAAAAAAJ4/0rQ98g6tDDA/s1600/annealingAtoms.png)\n\nThe pseudocode for this looks like:\n\n```\nStart with a temperature T\nFor a finite set of iterations:\n\n- sample new point x+\n- jump to a new point with probability given by an acceptance probability function P(x, x+, T)\n- decrease temperature T\n\nwhere \n\nP(x, x+, T) = 1 if f(x+) >= f(x) else e^((f(x+) - f(x))/T)\n```\n\nSo a high T makes the algorithm behave like a random sampler, and as it cools, it starts to behave more like a hill climber.\n\n![https://slideplayer.com/slide/8038378/25/images/6/Convergence+of+simulated+annealing.jpg](https://slideplayer.com/slide/8038378/25/images/6/Convergence+of+simulated+annealing.jpg)\n\nA remarkable analytical result about SA is that the probability of it ending at X is equal to `e ^ (f(x) / T) / Z(T)` ([a Boltzmann distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution)) which makes the likelihood of its ending point directly related to its ending point's fitness.\n\n## Method 3: Genetic Algorithms\n\nEvolution is a pretty good optimizer, and we can simulate that by \"breeding\" individual points to produce hopefully better offspring and \"mutating\" them by doing local search. Over multiple \"generations\", the hope is that we evolve towards the globally optimum result. One feature that helps a lot is if the individual dimensions/attributes can be additively combined, so that breeding two parents can regularly produce better offspring.\n\n![https://image.slidesharecdn.com/gasbytheseatofyourgenes-160629032611/95/genetic-algorithms-programming-by-the-seat-of-your-genes-6-638.jpg?cb=1467171105](https://image.slidesharecdn.com/gasbytheseatofyourgenes-160629032611/95/genetic-algorithms-programming-by-the-seat-of-your-genes-6-638.jpg?cb=1467171105)\n\nTo contrast this with the other methods, GAs are like random restarts done in parallel (since each individual in a population is like a thread, and each generation is like a restart), EXCEPT that the \"crossover\" of information when \"breeding\" happens helps to direct the general direction of evolution towards better places.\n\nThe pseudo-code looks like:\n\n```\nStart with an initial population of size K\nRepeat until converge:\n  - compute fitness of all population\n  - select \"most fit\" individuals\n  - pair up individuals, replacing \"least fit\" via crossover/mutation\n```\n\nAs you might imagine, the secret sauce is in the breeding. Two methods of doing preserving individuals are:\n\n- truncation selection - for example, only keeping the top half of the population\n- roulette wheel selection - taking a probabilistic approach to keeping each individual according to their fitness\n\nCrossover/Mutation also has nuances. Deciding on what attributes can be treated as a group involves domain knowledge and implicit assumptions that may or may not reflect reality.\n\n## More Randomized Optimization\n\nAs you can see, there are many ways to tackle the problem of optimization without calculus, but all of them involve some sort of random sampling and search. \n\nThose we have explored don't have much in the way of memory or of actually learning the structure or distribution of the function space, but there are yet more algorithms that explore those abilities:\n\n- [Tabu Search](https://en.wikipedia.org/wiki/Tabu_search) adds memory to the search (stay away from \"taboo\" regions)\nhttps://www.cc.gatech.edu/~isbell/tutorials/mimic-tutorial.pdf\n- [MIMIC: Finding Optima by Estimating Probability Densities](https://www.cc.gatech.edu/~isbell/papers/isbell-mimic-nips-1997.pdf) estimates a probability distribution in lieu of a genetic algorithm metaphor.\n\n## MIMIC\n\nMIMIC has a simple idea at its core - define a uniform probability distribution of points with a fitness over a threshold `theta`:\n\n![https://i.ytimg.com/vi/49Y2C_mTWD0/maxresdefault.jpg](https://i.ytimg.com/vi/49Y2C_mTWD0/maxresdefault.jpg)\n\nWhen theta is at a minimum, that means we uniformly sample over the entire space.\n\nWhen theta is at a maximum, that means we converge on one or more global optima.\n\n\n```\nat every iteration:\n\n- Generate samples from population consistent with the distribution so far\n- set `theta` to the nth percentile\n- retain only those samples where f(x) > `theta`\n- estimate the new distribution based on these samples,\n- repeat\n```\n\nTherefore, under MIMIC, our task is to continually raise `theta` until we just have small regions around our optima left. In the process of doing so, we infer a (uniform) probability distribution around our optima. Because we use the `nth` best percentile as our cutoff factor, this is similar to genetic algorithms, however there is no longer a concept of breeding or genes. Instead, we stay laser focused on performance against the fitness function as our one metric.\n\nThe comparison with genetic algorithms isn't accidental - the best version of MIMIC estimates distributions by assuming that the conditional distributions inherent in the space fit to \"dependency trees\", which are [Bayesian Networks](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72) where every node (but one) has exactly one parent. Finding these trees is out of scope but [check](https://www.youtube.com/watch?v=R-Mf9-tKC5o) [the](https://www.youtube.com/watch?v=u2V7esTMhKc) [videos](https://www.youtube.com/watch?v=rCxGiKQoe4w) for the math, but the theoretical grounding of this approach is rooted in maximizing [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information) (in information theory terms) and building a [Maximum Spanning Tree](http://shodhganga.inflibnet.ac.in/bitstream/10603/33822/4/chapter4.pdf). While not the only way to estimate the next probability distribution, Dependency Trees allow capturing of parent-child structure without having exponential cost (Max Spanning Trees are only polynomial in cost).\n\n\nBenefits:\n\n- The fact that MIMIC's \"rising theta\" approach can be used with any underlying probability distribution is pretty cool - here's a [simple applied example](https://www.youtube.com/watch?v=4DtVLCC73bA).\n- Being able to learn structure via the distribution assumption helps with generalizability\n- Empirical tests show MIMIC using 2-3 orders of magnitude less iterations to converge compared to Simulated Annealing and other alternatives, however, this is offset by each iteration taking much longer because of the need to estimate and generate from probability distributions. Thus, in terms of overall time taken, MIMIC works better when cost of evaluating `f(x)` is high (e.g. rocket simulation, antennae design, human test subjects)\n\n\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [Wikipedia on Randomized Optimization](https://en.wikipedia.org/wiki/Random_optimization)\n- [The original MIMIC paper](https://www.cc.gatech.edu/~isbell/papers/isbell-mimic-nips-1997.pdf)\n\nHopefully that was a good introduction to Randomized Optimization. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-bayesian-inference-4l72",
    "data": {
      "title": "Supervised Learning: Bayesian Inference",
      "description": "Or, the unreasonable effectiveness of dumb rules",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 11th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n> This chapter builds on the previous one on Bayesian Learning, and is skimpy because we skipped a lot of basic probability content.  \n\n> This is also the end of a miniseries on Supervised Learning, [the 1st of 3 sub disciplines within Machine Learning](https://dev.to/swyx/machine-learning-an-overview-216n).\n\n\n## What is Bayesian Inference?\n\nRepresenting probabilities, and calculating them. For example, what is the probability of X happening given Y? But on steroids.\n\n## Bayesian Networks\n\n> Also known as Belief Networks or Graphical Models.\n\nThe idea is to represent conditional relationships as nodes on a directed acyclic graph. Edges are therefore dependencies to be considered in your model.\n\n![https://cdn-images-1.medium.com/max/1600/1*9OsQV0PqM2juaOtGqoRISw.jpeg](https://cdn-images-1.medium.com/max/1600/1*9OsQV0PqM2juaOtGqoRISw.jpeg)\n\nThis can look like a neural net. Dependencies can skip levels and therefore the network's connections can grow exponentially with every variable:\n\n![http://www.pr-owl.org/images/bn_wisepilot.jpg](http://www.pr-owl.org/images/bn_wisepilot.jpg)\n\nThis makes inference in complex Bayesian networks hard to do.\n\nOne thing to note about dependencies is that they don't necessarily reflect cause-and-effect relationships, just ones that are conditionally dependent on the other. Perhaps more importantly, *lack* of dependencies are very good, because they reflect [conditional independence](https://en.wikipedia.org/wiki/Conditional_independence).\n\nThe acyclic nature of belief networks mean you can do a [topological sort](https://en.wikipedia.org/wiki/Topological_sorting) of the nodes to order your calculations.\n\n## Inferencing Rules\n\nThree handy rules we use in Bayesian Inference are:\n\n- Marginalization (intuitively, adding up the conditional probabilities)\n\n![https://image.slidesharecdn.com/02introtoprobabilitylukas-121203104049-phpapp02/95/introduction-to-probability-7-638.jpg?cb=1354531567](https://image.slidesharecdn.com/02introtoprobabilitylukas-121203104049-phpapp02/95/introduction-to-probability-7-638.jpg?cb=1354531567)\n\n- Chain rule (joint distribution of two attributes is an attribute 1's probability, times probability of other attributes given attribute 1)\n- Bayes rule (re-expressing conditional probabilities)\n\n![https://slideplayer.com/slide/5071518/16/images/12/Doug+Downey+%28adapted+from+Bryan+Pardo%2C+Northwestern+University%29.jpg](https://slideplayer.com/slide/5071518/16/images/12/Doug+Downey+%28adapted+from+Bryan+Pardo%2C+Northwestern+University%29.jpg)\n\n## Naive Bayes\n   \nNaive Bayes is a special case of Bayesian Networks, that assumes ALL attributes are conditionally independent of each other, i.e. a very very simple network with just one layer:\n\n![https://www.researchgate.net/profile/Julian_Ortiz4/publication/226687183/figure/fig1/AS:393643681697806@1470863376002/Bayesian-network-representing-the-Naive-Bayes-classifier-with-attributes-B-1-B-2.png](https://www.researchgate.net/profile/Julian_Ortiz4/publication/226687183/figure/fig1/AS:393643681697806@1470863376002/Bayesian-network-representing-the-Naive-Bayes-classifier-with-attributes-B-1-B-2.png)\n\nIf you take the top node as a class, and take all the child nodes as attributes, you can reverse the direction of the Bayes Net and infer the class from the attributes, arriving at a Naive Bayes Classifier:\n\n![https://shirishkadam.files.wordpress.com/2016/04/selection_005.png?w=760](https://shirishkadam.files.wordpress.com/2016/04/selection_005.png?w=760)\n\n## Naive Bayes: Pros and Cons\n\nThere are a number of benefits to this approach:\n\n- It makes inference (normally a np-hard problem) cheap\n- It is linear, not exponential, in number of attributes\n- It is easy to estimate these parameters with labeled data (by simple count)\n- connects inference and classification - instead of only generating probabilities of attributes, you can flip it and generate classification\n- Empirically it is very successful - [Google has a patented version](https://patents.google.com/patent/US8364766) of Naive Bayes used for spam filtering.\n\nHowever:\n\n- its \"naïveté\" comes from assuming that there are no interrelationships between any of the attributes, which is hard to believe\n    - the answer is: yes, its inaccurate, but [like we said in the last lesson](https://dev.to/swyx/supervised-learning-bayesian-learning-403l) we don't care about getting the exact right hypothesis to estimate probabilities, we just care about getting the right answer in classification. So you just need to be directionally correct.\n- Relying on an empirical count means missing attributes have zero estimated probability\n    - this exposes you to inductive bias and overfitting to your data\n    - yes, this is a problem, so in practice people \"smooth probabilities\" by initializing them with a small nonzero weight.\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [Wikipedia on Bayesian Inference](https://en.wikipedia.org/wiki/Bayesian_inference)\n\nHopefully that was a good introduction to Bayesian Inference. I am planning more primers and would love your feedback and questions on:\n\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-bayesian-learning-403l",
    "data": {
      "title": "Supervised Learning: Bayesian Learning",
      "description": "Lets update our priors! Oh wait we have none.",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 10th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n> This chapter is less theoretical than VC Dimensions, but still more focusing on the theoretical basis of what we do in Supervised Learning.\n\n## The Core Question\n\nWhat we've been trying to do is: learn the best hypothesis that we can, given some data and domain knowledge.\n\nWe are going to refine what our notion of \"best\" is to be \"most probable\" hypothesis, out of the hypothesis space.\n\n## Bayes Rule\n\nI'll assume you know this ([more info](https://en.wikipedia.org/wiki/Bayes%27_theorem), wonderful [counterintuitive application here](https://www.youtube.com/watch?v=77jH-M2StRM)):\n\n![https://cdn-images-1.medium.com/max/1600/1*LB-G6WBuswEfpg20FMighA.png](https://cdn-images-1.medium.com/max/1600/1*LB-G6WBuswEfpg20FMighA.png)\n\nApplied to ML:\n\n- A is our hypothesis\n- B is our Data\n- our hypothesis given the data is `P(A|B)`, what we want to maximize\n- our data sampling is `P(B)`\n- our prior domain knowledge is represented by `P(A)`\n- our updating (the running of the algorithm) is `P(B|A)`, also considered accuracy\n\nSince `P(B)` is constant among hypotheses and we just care about maximizing `P(A|B)`, we can effectively ignore it.\n\nWe've had many ways of representing domain knowledge so far - SVM kernels, choosing to use kNN (closer is more similar) - `P(A)` is a more general form of this.\n\n**Maximum Likelihood**: In fact if we further assume we have no priors (aka assume a uniform distribution for `P(A)`), `P(A)` can effectively be ignored. This means maximizing `P(A|B)` (our most likely hypothesis given data) is the same as maximizing `P(B|A)` (the most likely data labels we see given our hypothesis) if we don't have a strong prior. This is the philosophical justification for making projections based on data, but as you can see, even a little bit of domain knowledge will skew/improve our conclusions dramatically.\n\nHowever, it is impractical to iterate through all hypotheses and mechanically update priors like this. What we are aiming for with Bayes is more of an intuition about the levers we can pull rather than an actual applicable formula.\n\n## The Relationship of Bayes and Version Spaces\n\n**No Noise**: [This proof](https://www.youtube.com/watch?v=ycjPR-C07yM) is too mundane to write out, but basically given no priors, it is a truism that the probability of any particular hypothesis given data (`P(h|D)`) is exactly equal to `1/|VS|` (VS for version space). AKA if all hypotheses are equally likely to be true, then the probability of any one hypothesis being true is 1 / the number of hypotheses. Not exactly exciting stuff, but it derives from Bayes.\n\n**With Noise**: If we assume that noise has a normal distribution, we then use [this proof](https://www.youtube.com/watch?v=F1wRTC9vcDU) to arrive at the sum of squares best fit formula:\n\n![https://qph.fs.quoracdn.net/main-qimg-ebbed10139d890ebc9f5e2c55cc0ab04](https://qph.fs.quoracdn.net/main-qimg-ebbed10139d890ebc9f5e2c55cc0ab04)\n\n## The Minimum Description Objective\n\nBy adding a logarithm (a monotonic function, so it doesnt change the end result), we can translate, decompose, and flip the maximization goal we defined above to a minimization with tradeoffs:\n\n![https://wikimedia.org/api/rest_v1/media/math/render/svg/ee09398b5087ee5624cff6bcb57127ed384e95d3](https://wikimedia.org/api/rest_v1/media/math/render/svg/ee09398b5087ee5624cff6bcb57127ed384e95d3)\n\nHere `L` stands for Length, for example the length of a decision tree, which indicates the complexity of the model/hypothesis. `L(h)` is the complexity of the model we have selected, while `L(D|h)` is the data that doesn't fit the model we have selected, aka the error. We want to minimize this total \"description length\", aka seek the [minimum description length](https://en.wikipedia.org/wiki/Minimum_description_length). This is a very nice mathematical expression of [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor) with a penalty for oversimplification.\n\nThere are practical issues with applying this, for example the unit comparability of errors vs model complexity, but you can decide on a rule for trading them off.\n\n## The Big Misdirection\n\nDespite everything we laid out here and in prior chapters, **we don't -really- care about finding the single best or most likely hypothesis**. We care about finding the single most likely **label**. A correct hypothesis will help us get there every time, sure, but if every remaining hypothesis in our version spaces in aggregate point towards a particular label, then that is the one we actually want. \n\nThus, Bayesian Learning is merely a stepping stone towards Bayesian Classification, and building a [Bayes Optimal Classifier](https://svivek.com/teaching/machine-learning/fall2018/slides/prob-learning/bayes-optimal-classifier.pdf).\n\nWe will explore this in the next chapter on Bayesian Inference.\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [a nice introduction to Bayesian Learning with more detail](https://wso2.com/blog/research/part-one-introduction-to-bayesian-learning)\n- [application to neural networks, aka Bayesian Networks](http://www.cs.cmu.edu/afs/cs/project/theo-20/www/mlbook/ch6.pdf)\n- [great discussion of Bayesian nonparametric learning](http://fastml.com/bayesian-machine-learning/)\n\nHopefully that was a good introduction to Bayesian Learning. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-vc-dimensions-10b",
    "data": {
      "title": "Supervised Learning: VC Dimensions",
      "description": "It's not venture capital, that's for sure",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 9th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n> ⚠️ This chapter is a LOT more theoretical than the others but set up the theoretical foundations for machine learning - skip or persevere, your choice but be warned\n\n## The Core Question\n\nIn our prior chapter we learned to use [Haussler's Theorem](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h) to bound our true error as a function of the number of examples drawn. \n\n![https://image.slidesharecdn.com/lecture5xing-150527174556-lva1-app6891/95/lecture5-xing-16-638.jpg?cb=1432750316](https://image.slidesharecdn.com/lecture5xing-150527174556-lva1-app6891/95/lecture5-xing-16-638.jpg?cb=1432750316)\n\nHowever this formula does rely on the hypothesis space being finite (i.e. risking that the \"true\" hypothesis is something we aren't considering). In this chapter we consider the question of having **infinite** hypothesis spaces.\n\n## Infinite Spaces Everywhere\n\nIf the previous chapter got you excited, this is going to get you down. Infinite spaces are everywhere. Not every problem has binary input spaces. Anything with continuous values has infinite spaces.\n\nHowever we can make the distinction between a *syntactic* hypothesis space (all the things you could possibly guess) and a *semantic* hypothesis space (all the functions you can possibly represent) and collapse infinite spaces down to discrete ones. We do this with Decision Trees on continuous values, where we pick a split point for a continuous value based on having a meaningful difference despite the theoretically infinite possible values.\n\nPicking two points on a number line to split on doesn't significantly increase the complexity of the hypothesis space, because one number is necessarily greater than the other and vice versa. You can be either less than or greater than 5, but it is impossible to be less than 3 AND greater than 7. So the number line is a fairly \"weak\" hypothesis space.\n\n## VC Dimension and the Power of a Hypothesis Space\n\nIn fact, picking a split point on a continuous number line is such an easy split that we can generalize this idea and ask:\n\n> What is the largest set of inputs that the hypothesis class can label in all possible ways (aka shattering)?\n\nIn other words, what is the *largest* number of variables you can use to semantically express the same hypothesis space as the real (infinite) hypothesis space? This quantity is called the VC dimension.\n\nVC stands for [Vapnik](https://en.wikipedia.org/wiki/Vladimir_Vapnik) (the same guy that invented SVMs) and [Chervonenkis](https://en.wikipedia.org/wiki/Alexey_Chervonenkis). [VC theory](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory) relates the VC Dimension of a class to the amount of data you need to be able to learn effectively in that class.\n\n## VC Dimension of intervals\n\nThis is a very simple example just for learning.\n\nIf H: The set of all intervals from `[a,b]`:\n  - and X is the set of Real Numbers in one dimension\n  - So this is an infinite hypothesis space\n  - VC dimension asks: what is the largest set of inputs that the hypothesis class can label in all possible ways using hypotheses from H.\n    - You can fit an interval with just one input point (just fit around or outside it)\n    - You can fit an interval with two input points (just fit around one or either or both or neither)\n    - But you can't fit an interval with three input points that have a `+ - +` sequence since the interval has to be contiguous\n    - therefore the VC dimension is 2.\n\n[**Shattering**](https://www.quora.com/Explain-VC-dimension-and-shattering-in-lucid-Way) is a small but critical concept and denotes assigning different labels to points. For example, if the points are all on top of each other they could not be shattered.\n\n## VC Dimension of linear separators\n\nLinear separators, like the ones we have looked at with [the most basic SVM example](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk), separate a plane into a positive and a negative side. We've already established these are infinite hypothesis spaces.\n\nIf H: The set of all separators (lines) from `y=mx+b`:\n  - and X is the set of Real Numbers in two dimensions\n  - So this is an infinite hypothesis space\n  - VC dimension asks: what is the largest set of inputs that the hypothesis class can label in all possible ways using hypotheses from H.\n    - You can fit a line with just one input point\n    - You can fit a line with two input points (just fit around one or either or both or neither)\n    - You can fit a line with three input points (except if they're all on the same line, aka not shatterable)\n    - But you can't fit a line with four input points that are in a `+ - + -` 2x2 matrix since the lines split a plane that has to be contiguous\n    - therefore the VC dimension is 3.\n\n## More dimensions\n\nThere's a pattern here:\n\n| separator      | VC Dimension | Inputs |\n| ----------- | ----------- | ----------- |\n| 0d point      | 1       | theta       |\n| 1d line   | 2        | a,b       |\n| 2d surface   | 3        | w1,w2,theta       |\n| 3d space   | 4        | w1,w2,w3,theta       |\n\nIn fact VC dimension often is the dimensionality of the space in which the hypothesis inputs live. For any d-dimensional hyperplane hypothesis class, the VC dimension is `d+1` - the weights for each dimension + the theta (for greater than or equal to overlap)\n\nMore separators and their VC dimension examples and the reasoning behind them [can be found here](https://www.spsc.tugraz.at/system/files/vc_examples.pdf).\n\nWe can have nonlinear separators as well - 2d circles have a VC Dimension of 3 for example.\n\n## Infinite VC Dimension\n\nDespite recasting the conversation from infinite spaces to dimensionality, it is still possible to form hypotheses with infinite dimension. A trivial example is a hypothesis class for points inside a convex polygon:\n\n{% youtube TYHFOpgCBek %}\n\n(best watched to be understood)\n\nSo it is that a circle has a VC Dimension of 3 but a convex polygon has infinite VC dimension (because it can have arbitrarily many input points)\n\n## Sample Complexity (infinite case) and VC Dimension\n\nNow we have developed a concept of VC Dimension, we can connect it back to our former formula for Sample Complexity (completely skipping the proof...):\n\n![https://images.slideplayer.com/16/4991939/slides/slide_12.jpg](https://images.slideplayer.com/16/4991939/slides/slide_12.jpg)\n\n## The VC of Finite Hypothesis Space\n\nIf we denote the VC of Finite Hypothesis Space by `d`, there has to be `2^d` distinct concepts (as each different labelling can be captured by a different hypothesis in a class) - therefore `2^d` is less than or equal to the number of hyptheses `|H|`. \n\nRearranging, `d <= log2 (|H|)`. So a finite hypothesis class gives us finite VC dimensions, and therefore make things \"PAC Learnable\". There is a further proof that this goes the other way - `H` is PAC Learnable *if and only iff* the VC Dimension is finite. VC Dimension captures the notion of PAC Learnability in a single concept.\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [Tom Mitchell lecture on this topic](https://scs.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=558031db-3afe-461a-8c04-8f417fa66dd2)\n\nHopefully that was a good introduction to VC Dimensions. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-computational-learning-theory-160h",
    "data": {
      "title": "Supervised Learning: Computational Learning Theory",
      "description": "What's the big O of machine learning? Lets put some formal theory around HOW we learn!",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 8th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n> ⚠️ This chapter is a LOT more theoretical than the others but set up the theoretical foundations for machine learning - skip or persevere, your choice but be warned\n\nWe've learned about various machine learning algorithms - but what do we know about evaluating algorithms and the theory of learning itself? Here, we want to:\n\n- define learning problems\n- show whether specific algorithms are effective/ineffective with regard to the problem\n- show some problems are fundamentally hard (aka unsolvable)\n\n## The Core Question\n\n**What's the Big O of machine learning?**\n\nWe know about Big O of regular algorithms from Complexity Theory, to estimate how resource consumption scales with the problem size (e.g. for space and time). With ML, those constraints are also true, but the new limited resource is data.\n\n## Resource usage in ML\n\nMuch like regular algorithms have a Big(O), ML algorithms also have Big(O)s for time and space, but also data consumption.\n\nWhen we evaluate algorithms, we might want to know things like:\n\n1. probability of successful training (`1 - delta`)\n2. number of examples to train on (`m` or `n`)\n3. complexity of hypothesis class (`complexity of H`)\n4. Accuracy we are approximating target concept (`Epsilon`)\n5. presenting training examples (big batch, or incremental/iterative)\n6. selecting training examples (explained below)\n\n## Training Examples aren't always random\n\nHaving control over the questions being asked (and therefore the examples to train on) can vary data consumption wildly. For example: \n\n- if the person who knows the answer controls the questions (and therefore examples), they can ask very specific questions to get you to the right concept based on limited examples\n- if the learner controls the questions, then they can design questions (and therefore examples) to eliminate as much of the hypothesis space as possible each time (like in a [decision tree](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n- However if the data is controlled by nature (aka randomness, or sampling) then a lot more examples (how many?) will be needed to infer the right concept\n- You can also consider an adversarial teacher that deliberately shows you real but unrepresentative examples to mislead you. FAR more data would be required in this case.\n\nIn practice, most training examples are randomly distributed. But it is worth considering how efficiently we can learn from other distributions.\n\nAn additional consideration is balance. We learn most from positive examples (\"Absence of evidence is not evidence of absence\"), so if those are few and far between then we will need a LOT of examples to learn the right concept (aka falsify wrong hypotheses).\n\n## Mistake Bounds\n\nInstead of asking how accurate our learning is to the \"real\" underlying concept, we can flip the script and try to set an upper bound on the **mistakes** we make while learning. This is a fascinating idea for linear binary classification models with no noise. \n\nFor example, let's say we have a possibility space of K bits (`1` or `0`). So 10 bits have 1024 (`2 ^10`) possibilities. We start by extreme overfitting to the first example we see (so that the first example we saw was the ONLY possible result from our current hypothesis). Then we continually relax the conditions (widening the hypothesis) as we come across more examples. Because each bit only has three possible states (positive, negative, and absent), new examples which conflict with our hypothesis *must not matter* to the answer, and so we can switch them to `absent`.\n\nIn this way we guarantee that our learning will never make more than `K+1` mistakes, for a `2^K` possibility space. A cute trick, but it doesn't quite put upper bounds on how many examples we still need, which is the original question.\n\n \n## Questions revisited\n\nWe now have fleshed out some competing measures of \"big O for Machine Learning\":\n\n- **Computational Complexity**: How much computational effort is needed for a learner to converge? (closest to the classical \"big O\")\n- **Sample Complexity**: How many training examples in a batch are needed for a learner to create a successful hypothesis?\n- **Mistake bounds**: How many mistakes can a learner make over an infinite run?\n\nFor practical purposes we will ultimately just focus on Sample Complexity, but it is worth considering these other forms.\n\n## Version Spaces\n\nSome definitions:\n\n- The hypothesis space `H` is the set of all possible hypotheses we are willing to consider\n- The True hypothesis `c` is the \"real\" one (aka the \"target concept\")\n- The candidate hypothesis `h` is what we're currently considering\n- The training set `S` is example data (for example uniformly randomly drawn from the population)\n- A **consistent learner** is one that produces a hypothesis that is consistent with the data its seen so far\n- The **version space** is the set of all hypotheses that are consistent with the data.\n\nBasically, its \"what we haven't ruled out yet\". Silly to define, but worth having a term for.\n\n## PAC Learning\n\nMore definitions:\n\n- The **Training error** is the fraction of **training examples** misclassified by `h`. The true hypothesis would have a training error of 0. \n- The **True error** is the fraction of examples that **would be** misclassified on sample data.\n\n![https://slideplayer.com/slide/4983569/16/images/5/Protocol+Given%3A+Learner+observes+sample+S+%3D+%7B+%EF%83%A1+xi%2C+c%28xi%29+%EF%83%B1+%7D.jpg](https://slideplayer.com/slide/4983569/16/images/5/Protocol+Given%3A+Learner+observes+sample+S+%3D+%7B+%EF%83%A1+xi%2C+c%28xi%29+%EF%83%B1+%7D.jpg)\n\n\n- `C`: Concept Class\n- `L`: Learner\n- `H`: Hypothesis space\n- `n`: size of hypothesis space `|H|`\n- `D`: distribution over inputs\n- `episilon`: error goal (less than or equal to 0.5)\n- `delta`: certainty goal (with probability `1-delta`, the algorithm will produce a true error less than or equal to `epsilon`)\n\nBecause we are always learning from samples, we can never reduce our uncertainty to zero, nor our error, and thus our goal is better phrased as looking for a **Probably Approximately Correct** hypothesis. \n\n> Formally: `C` is said to be `PAC-learnable` by `L` using `H` if and only if `L` will, with probability `1-delta`, output a `h` from `H` such that `error(h) < epsilon` in time and samples polynomial in `1/epsilon, 1/delta, and n`.\n\n## Epsilon exhaustion\n\nA version space is epsilon exhausted iff all remaining hypothesis have low error. Then you can choose any of the hypothesis and be acceptably fine according to your chosen error rate. \n\nIn other words, it is **Probably Approximately Correct**.\n\n## Haussler's Theorem\n\nThis is a theorem for bounding the **true error** as a function of the number of examples that are drawn. We'll leave the derivation to a video if you're interested:\n\n{% youtube TpQoiUQSPB0 %}\n\n{% youtube KtZTIvuRdss %}\n\nThe TL;DR conclusion is that the upper bound for a version space _not_ being epsilon exhausted after `m` samples is:\n\n```\nm >= 1/epsilon * (ln n + ln (1/delta))\n```\n\nSo we can work out, for example, that for an input space of 10 bits (1024 possibilities), target epsilon of 0.1, delta of 0.2, we will need at least `m = 1/10 * (ln 10 + ln (1/0.2)) ~= 40` training examples, or less than 4% of the possible space.\n\nThe theorem also shows what levers to pull and the consequences of those levers. For example, to reduce epsilon down to 1%, we'd then need 400 examples, or 40% of the total possible space.\n\nWe are now able to place **sample complexity bounds for PAC learning** with this equation.\n\n## Next in our series\n\nFurther notes on this topic:\n\n- [Wikipedia](https://en.wikipedia.org/wiki/Computational_learning_theory)\n- [A professor's survey article of CLT](http://eliassi.org/COLTSurveyArticle.pdf)\n\nHopefully that was a good introduction to Computational Learning Theory. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-support-vector-machines-3mgk",
    "data": {
      "title": "Supervised Learning: Support Vector Machines",
      "description": "Transforming dimensions, nearest neighbors, and boosting, all in service of \"drawing the best line\".",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 7th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n## Intuition for why\n\nDrawing lines to separate between groups is actually a fairly weak condition. Now that we know [how to boost our algorithms](https://dev.to/swyx/supervised-learning-ensemble-learning-lim) to arbitrary complexity, the next frontier to solve is choosing the \"best\" lines to draw. There are still a wide range of valid lines (aka weak learners) that completely separate groups, but subjectively some lines are better than others. \n\n![https://onionesquereality.files.wordpress.com/2009/03/infinte-linear-separators.jpg](https://onionesquereality.files.wordpress.com/2009/03/infinte-linear-separators.jpg)\n\nGiven group A and group B, you'd prefer a line that is equidistant between A and B, rather than just barely on the border of either. The theory is that A and B are just samples of a larger population, so if you draw the lines too close to either, they will likely **fail to generalize**. \n\nThe other way to phrase this is that it is the hypothesis line that is supported by the data but commits the least to it.\n\nFinding this optimal delineation is the intuition for SVMs.\n\n## Finding the margin\n\nWe find the widest possible band of possible space between groups, find a line that is perpendicular to each side of the band at each point, and choose the midpoint of the perpendicular line. We call this line the **margin**.\n\nFormally, a classifier `f` can be boiled down to:\n\n```\nwX + b = C\n\nwhere \n\n- b is a constant\n- w is a simple, linear vector\n- X is one or more dimensions of input variables\n- C is the classifier output, either -1 or +1\n```\n\nSo we can set up\n\n```\nwX1 + b = 1\nwX2 + b = -1\n```\n\nfor any two sets of X variables. The equation for the margin, or the perpendicular distance between them, `(w/mod(w))X1-X2` gives us `2/mod(w)`, which is also the distance between the two planes. We want to maximize `2/mod(w)` *while classifying everything correctly*, which is hard to solve, so we transform the problem to minimizing `1/2 * mod(w) ^ 2`, which is a [quadratic programming problem](https://en.wikipedia.org/wiki/Quadratic_programming#Solution_methods) that we do know how to solve, with the same answer. \n\nThe solution looks like this:\n\n![https://cdn-images-1.medium.com/max/1600/0*OEjd8IeZaCx8SQWz.jpg](https://cdn-images-1.medium.com/max/1600/0*OEjd8IeZaCx8SQWz.jpg)\n\nand you can check the [associated post](https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78) to further understand the alpha parameters.\n\nIt turns out that the quadratic programming solution only relies on a few data points, the ones at the edge of the groups. In other words, most of the alpha parameters turn out to be near 0, except for those `xi, xj` combinations that are closes to the edges of the groups. This is intuitive - the points inside the groups simply don't provide as much information as to where the boundary of the group is. We call these outer data points [support vectors](https://onionesquereality.wordpress.com/2009/03/22/why-are-support-vectors-machines-called-so/) - as you can see here, these are the black dots closest to the lines:\n\n![https://onionesquereality.files.wordpress.com/2009/03/optimal-margin-classifier.jpg](https://onionesquereality.files.wordpress.com/2009/03/optimal-margin-classifier.jpg)\n\nHere's another picture just to be sure:\n\n![https://www.mathworks.com/help/stats/svmhyperplane.png](https://www.mathworks.com/help/stats/svmhyperplane.png)\n\nThis sounds a lot like [K Nearest Neighbors](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)! Except here you are finding separators for entire groups of datapoints.\n\n## The Kernel Trick\n\nInside the quadratic programming solution there is a projection between the two points on either side of the margin that involves a similarity score. In the simplest case it can be a simple dot product (0 if perpendicular, 1 if they point the same way, -1 if they point exactly opposite), but you can generalize it to almost any similarity function you can think of. This can be useful for adding dimensions to transform your existing data into linearly separable spaces.\n\nSo even though we can only draw straight lines with our SVM methods, and might be unable to draw lines for a given set of data, we can transform our datapoints so that lines -can- be drawn in the higher dimension:\n\n![https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2017/02/kernel.png](https://blog-c7ff.kxcdn.com/blog/wp-content/uploads/2017/02/kernel.png)\n\nHere, we were unable to draw a line in 2 dimensions to carve out the circle, but once we added an extra dimension it became an easy task. You can do this for almost anything:\n\n![https://qph.fs.quoracdn.net/main-qimg-cd6cde306c8273b2af183f57f25c259d](https://qph.fs.quoracdn.net/main-qimg-cd6cde306c8273b2af183f57f25c259d)\n\nWhat is cool is that the transformation doesn't have to be explicit - if we choose the new dimension such that the total new datapoints' dot product has a nice closed form (hard to get wrong), we can use that as our kernel *instead* of the linear dot product and we will be solving that optimization instead. There are any number of kernels to use:\n\n![https://i.ytimg.com/vi/OmTu0fqUsQk/maxresdefault.jpg](https://i.ytimg.com/vi/OmTu0fqUsQk/maxresdefault.jpg) \n\nall with different characteristics. So your SVMs can even do this:\n\n![https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRUPB_k4gT1YtQTsctdyhGsOp6OSObJtD3_dbBjDAb8tqlFJJKPDw](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRUPB_k4gT1YtQTsctdyhGsOp6OSObJtD3_dbBjDAb8tqlFJJKPDw)\n\nor this:\n\n![https://www.researchgate.net/publication/258395951/figure/fig3/AS:325609399046184@1454642739017/Comparison-between-LS-SVMs-and-FLSA-SVMs-a-two-spiral-pattern-recognized-by-the_Q320.jpg](https://www.researchgate.net/publication/258395951/figure/fig3/AS:325609399046184@1454642739017/Comparison-between-LS-SVMs-and-FLSA-SVMs-a-two-spiral-pattern-recognized-by-the_Q320.jpg)\n\nWhat ultimately matters is that your choice of kernel reflects your domain knowledge, your judgment of what datapoints (and projections of those datapoints) are valid members of the group based on features you identify, based on their similarity.\n\nSide note: All SVM Kernels must satisfy the [Mercer condition](http://www.svms.org/mercer/) ([a great answer as to why](https://www.quora.com/Why-should-a-kernel-function-satisfy-Mercers-condition) - basically, ensure that the matrix is PSD which means there is a global optimum to converge to)\n\n## SVMs and Boosting\n\nIn [our Boosting chapter](https://dev.to/swyx/supervised-learning-ensemble-learning-lim) we talked about how Boosting tends not to overfit. We can introduce the idea of a difference between the concepts of *error* and *confidence* in our training results. In early stages of training, confidence goes up as error goes down. This is analogous to SVMs searching for and widening their margins, the wider the better.\n\nHowever, what if we get to a point where no errors are left? (Because we have boosted our hypothesis so much we get perfect separation) Error is at 0, and has nowhere further to go. As we keep adding more and more weak learners, however, the *confidence* in our result continues to go up as the separation becomes clearer and clearer (we continue redraw the space in which we separate the two sides). The boostign algorithm is effectively continuing to widen the margin, which tends to minimize overfitting as we inferred above.\n\nBoosting does overfit in some cases, for example when its learner is based on a very complex multilayered artificial neural network. In practice, this isn't much of a concern.\n\n## Next in our series\n\nFurther notes on this topic:\n\n- ICML tutorial on SVMs: https://www.cs.rochester.edu/~stefanko/Teaching/09CS446/SVM-ICML01-tutorial.pdf\n- Burges' SVMs for pattern recognition: http://research.microsoft.com/pubs/67119/svmtutorial.pdf\n- Scholkopf's SVMS and kernel methods: https://github.com/pushkar/4641/raw/master/downloads/svm-scholkopf.ps\n- MIT's SVM lecture: https://www.youtube.com/watch?v=_PwhiWxHK8o (recommended)\n- An Idiot's Guide to SVMs: http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf\n\nHopefully that was a good introduction to SVMs. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-ensemble-learning-lim",
    "data": {
      "title": "Supervised Learning: Ensemble Learning and AdaBoost",
      "description": "Better together - how bootstrapping samples of data can work better than the entire dataset, and how to boost it even further, and faster",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 6th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n## Intuition\n\nThe purpose of [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) is to learn weights for a bunch of features that have some unknown relationship to whatever you are trying to classify.\n\nFor example, classifying email into spam/not spam could use some factors like the mention of Viagra or cryptocurrency or money from Nigeria. Ensemble models take all these little factors and learn the weights in order to spit out the broader result.\n\nThis can be helpful if, for example, the fundamental thing we are trying to classify presents in different ways, for example financial spam is different from NSFW spam. In other words, some rules are better for some **subsets** of the data than others. So naturally some rules will work better than others depending on context, however at the end of the day we just want an overall rule to throw out spam. So our ensemble learning in this case would learn something close to OR logic. If the rules have some contingent dependencies, i.e. the mail is spam if Rule 1 and Rule 5 are both active, then the ensemble learning approximates AND logic. And so on.\n\nI've portrayed the rules here as something consciously picked. But to go one level deeper, we should **apply learners to subsets of data** and let it pick the rules.\n\n## Bagging/Bootstrap Aggregation\n\nPicking subsets is actually a bigger assumption than it seems. How we pick and separate subsets is up for debate, and whether we should even pick a subset at all assumes that the data is separable that way.\n\nPicking data uniformly randomly actually works pretty well (see [random forests](https://en.wikipedia.org/wiki/Random_forest) vs the decision trees we have already studied). Taking the standard [UCI Housing Data](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/) example, taking a simple average ensemble of 3rd order polynomials from 5 random subsets of 5 data points, beats a 3rd order polynomial trained over the whole thing. (\"beats\" here means it does better in the out of sample test/validation set). The intuition here is better generalization/less overfitting by significantly reducing the impact of noise since noise is likely less prevalent in a random subset. This is called **Bagging/Bootstrap Aggregation**.\n\nHowever, you do tend to lose interrelationships that you might have picked up on otherwise, and the rules from each subset have correspondingly lower statistical power because of the smaller sample size.\n\n## Boosting/AdaBoost\n\nInstead of uniformly randomly picking subsets, we should focus each round of subset-picking on the \"hardest\" examples. \n\nInstead of combining multiple rules by simple average, we can use a \"weighted\" mean.\n\nTo define what we mean by \"hard\" and how we \"weight\", we need to revisit the definition of error. Typically this is the number of wrong classifications (in the discrete classification use case) or the square of departures from the model (in the continuous regression use case). However, some errors don't happen often, and therefore we don't get much opportunity to train on them. The dominance of some states can lead machine learning to overlook important, rarer states. So if 99% of your email is spam, then a simple spam classifier could just learn to classify ALL your mail as spam and have a pretty good 1% error rate.\n\nSo we should, in our ensemble learning, try to progressively focus on the examples that are \"harder\" to get right as compared to the subsets of data that are well taken care of.\n\nFocusing on the classification case, this means modifying each round's distribution focusing on where the model disagrees with our training data (aka the \"hardest examples\"). Formally, you can represent like this:\n\n![https://sw-yx.tinytake.com/media/951a43?filename=1548621831322_27-01-2019-10-43-44.png&sub_type=thumbnail_preview&type=attachment&width=509&height=248&&salt=MzI2MTczMl85NzcxNTg3](https://sw-yx.tinytake.com/media/951a43?filename=1548621831322_27-01-2019-10-43-44.png&sub_type=thumbnail_preview&type=attachment&width=509&height=248&&salt=MzI2MTczMl85NzcxNTg3)\n\nWhere `y` and `h(x)` represent training data and model, and are either +1 or -1. So multiplying them together with a constant `alpha` term lets us raise (or **boost**) the importance of `Dt` by a positive exponent if they **disagree**, and lower it if they **agree**. For a fuller explanation of the adaptive boosting (AdaBoost) algorithm, [read Robert Schapire's overview paper here.](http://www.cis.upenn.edu/~mkearns/teaching/COLT/schapire.pdf) ([alt link](https://www.cs.princeton.edu/courses/archive/spring07/cos424/papers/boosting-survey.pdf))\n\nThe final hypothesis sums up the results of all the models trained so far and just uses the sign of the result.\n\n## Weak Learners\n\nA **weak learner** is a learner that will do better than chance when it tries to learn your hypothesis rules on any data distribution. This means that it will have some level of information gain (as we defined in [our Decision Trees chapter](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)), as compared to chance which has no gain. This includes Decision Trees, Perceptrons, and any other classification/regression model you can image. This makes Boosting a meta-machine learning algorithm built on top of existing algorithms.\n\nThe emphasis **on any data distribution** is important - it means that your rules need to separate the possible space of states such that it makes it possible for a weak learner to do better than chance *under any distribution*. ([see this answer for more](https://www.youtube.com/watch?time_continue=187&v=lj-IO4uuVR8)) So despite the name, this is a fairly important condition, however it is weak enough for our use.\n\nSo boosting relies on weak learners at each step of the way as a hypothesis. They don't need to cleanly slice the dataset in half each time. They just need to do better than random at classifying some part of the dataset - after which, we can further refine and change the distribution to focus on the remaining disagreements. AdaBoost is thus an algorithm for constructing a ”strong” classifier as linear\ncombination of “simple” “weak” classifiers.\n\nThe actual **boosting** - upweighting of disagreements - doesn't actually even have to happen for this method to work, but it does speed up the convergence by disproportionately focusing on disagreements.\n\nWe have done a lot of theory here - so please check this visual example to make sure you nail the underlying intuition in boosting and hypothesis forming.\n\n{% youtube u1MXf5N3wYU %}\n\nAs you can see here, the power of ensemble learning to compose very simple rules together to map complex spaces is useful. In some way this is analogous to binary search reducing an O(N) problem to an O(log N) problem - by slicing up the dataset progressively each time, we ensure that the final error rate - stuff we cannot classify wrong - is small, and can almost always be improved (more than random) by adding one more layer.\n\n## Boosting rarely Overfits\n\nOverfitting is a constant worry in models (we've discussed many times before) - where training error continues to go down but the test/validation error starts going up. Curiously enough, with Boosting models this doesn't happen. We'll discuss more in the next chapter after our SVM discussion, but Schapire's various talks and papers ([here](http://media.nips.cc/Conferences/2007/Tutorials/Slides/schapire-NIPS-07-tutorial.pdf) and [here](http://rob.schapire.net/papers/explaining-adaboost.pdf)) discuss some theoretical backing for this.\n\n## Next in our series\n\nSchapire has also extended Boosting to the Multi-class case, which has unique challenges, [in this lecture.](https://www.youtube.com/watch?v=L6BlpGnCYVg)\n\nFurther notes on this topic [can be found here](https://storage.googleapis.com/supplemental_media/udacityu/367378584/Intro%20to%20Boosting.pdf), with [a specific slide deck on AdaBoost here](https://github.com/pushkar/4641/raw/master/downloads/adaboost_matas.pdf).\n\nHopefully that was a good introduction to Ensemble Learning and AdaBoost. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge",
    "data": {
      "title": "Supervised Learning: Instance-based Learning and K-Nearest Neighbors",
      "description": "Regression isn't the only way. What if we were far, far... lazier about it?",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 5th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n\n## What is \"Instance Based\"?\n\nRecall that Supervised Learning [approximates a function](https://dev.to/swyx/machine-learning-an-overview-216n). Then projections are made by plugging in values to the function, without any reference to the actual data.\n\nAn alternative approach just puts all the raw data (\"all instances\") in a database, and, when queried, looks up the corresponding output. No time is spent doing any learning, so it is fast and simple.\n\nHowever, of course, we lose out on **generalization** (for example if we are asked something close to but not exactly what we have data for) and we are prone to **overfitting** (for example if our data is noisy).\n\nSo Instance Learning looks at the nearest neighbors to decide what any queried point should be. Specifically, the `k` nearest neighbors.\n\n## `K` Nearest Neighbors\n\nGiven:\n\n- Training Data `D = {Xi, Yi}`\n- Distance Metric `d(q, x)` (representing your domain knowledge - a way to quantify the similarity of one thing to another) \n- Number of neighbors, `k` (also relying on your domain knowledge as to what matters)\n- A query point, `q`\n\nThe algorithm is simply - given `D`, find the `k` nearest neighbors (K-NN) based on your distance metric `d(q, x)`.\n\nYou can do both **classification** and **regression** this way:\n\n- Classification: based on vote of `Yi`'s - your point is classified by whatever the most neighbors are\n- Regression: the average of the `Yi`'s.\n\nInstead of a simple vote or simple average that weighs all neighbors the same, you can also weight them by closeness so that closer neighbors count more.\n\n## Lazy vs Eager\n\nYou can loosely break up these algorithms into **learning** and **querying** stages:\n\n- The learning stage computational complexity of K-NN is `O(1)`, while Linear Regression is `O(N)`.\n- The querying stage computational complexity of K-NN is `O(log N)`, while Linear Regression is `O(1)`\n\nThus more work is frontloaded by Linear Regression, making it an \"eager\" learning algorithm, whereas K-NN does more work while querying, making it \"lazy.\n\n## In Python\n\nFor some light programming practice, try solving the problems pictured here: \n{% youtube eBt8vTvmsV4 %}\n\n[Here's a sample Repl](https://repl.it/@swyx/PracticalHarmfulOutsourcing) solution calculating nearest neighbors by Euclidian and Manhattan distance.\n\nNote that the real answer ([based on the hidden function](https://www.youtube.com/watch?time_continue=319&v=X8tm6x2k_gQ)) was `18`, which K-NN doesn't get close to by any of these methods.\n\n## KNN's biases\n\nKNN's preference bias (its beliefs about what makes a good hypotheses) are:\n\n- Locality - assumes that Near Points are \"similar\"\n- Smoothness - using averaging\n- All features matter equally <-- this belies the Curse...\n\n## Curse of Dimensionality\n\n> As the number of **features** or **dimensions** grows, the amount of data we need to **generalize accurately** grows exponentially!\n\n[More on wikipedia](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\n\n## A blended approach\n\nInstead of a Euclidean/Manhattan weighted/unweighted average approach to estimating the final result, we can also combine a KNN with regression to create [locally-weighted regression](https://en.wikipedia.org/wiki/Local_regression) to have the best of both worlds. This isn't just limited to regression - within the defined nearest neighbors, you can use any of the methods we have covered so far, like neural networks or decision trees.\n\n## Optimal Data Structure\n\nWe can also consider more efficient data structures for kNN than a simple lookup table. [Ball Trees](https://en.wikipedia.org/wiki/Ball_tree) are promising in this regard.\n\n## Next in our series\n\nHopefully that was a good introduction to Instance-based Learning and K-Nearest Neighbors. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-neural-networks-mpo",
    "data": {
      "title": "Supervised Learning: Neural Networks",
      "description": "That one time we tried to emulate our brains with computer chips",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 4th in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n\n## Neurons and Perceptrons\n\nBehold, a Neuron. You hopefully have a few of these.\n\n![https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/1200px-Neuron.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/1200px-Neuron.svg.png)\n\nInformation travels from the cell body in \"spike trains\" down the axon and causes synapse excitation on other neurons. So Neurons are a complicated computational unit. They can be tuned or trained through a learning process to fire on different things, and other neurons can fire on that, and so on. Brains are a tight wad of 100 billion neurons, or a natural neural network, if you will, all firing one after the other to make conscious and unconscious decisions.\n\nArtificial neurons (aka Perceptrons) try to emulate this:\n\n![https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/600px-ArtificialNeuronModel_english.png](https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/600px-ArtificialNeuronModel_english.png)\n\nA vector of inputs are fed into the neuron, which multiplies them by a vector of weights and sums them up. If the sum is above a certain firing threshold, the neuron is activated and fires an output of 1. If it is not activated, it has an output of 0.\n\n## How Powerful is a Perceptron Unit?\n\nIt can represent half planes in multiple dimensions. \n\n- A simple perceptron with 2 binary inputs and equal weights with a threshold higher than either weight can represent the AND logical operator. \n- If the threshold is equal to either weight, it represents the OR logical operator.\n- Similarly you can also make NOT.\n\nFor more you should really check this and related videos:\n\n{% youtube VwCMm3llk1s %}\n\n\nTo make XOR, you have to combine perceptrons by applying a strong negative weight on an AND perceptron together with an OR perceptron:\n\n\n{% youtube 2KUq_Ou-7FY %}\n\n\n## Perceptron Training\n\nInstead of setting weights by hand like we have been doing, we should automate that. The two ways to do this are:\n\n- Perceptron Rule\n- Gradient descent/delta rule \n\n## Perceptron Rule\n\nSetting a single unit so it matches a training set.\n\n![https://slideplayer.com/slide/5098348/16/images/13/Perceptron+Training+Rule.jpg](https://slideplayer.com/slide/5098348/16/images/13/Perceptron+Training+Rule.jpg)\n\nThe **weight change** equals the difference of the **target** (what we want it to be, either 0 or 1) and the **output** (what the network currently spits out, also 0 or 1), multiplied by the **input** and the **learning rate** (which is a small number, like 0.01).\n\nIn other words, if the output of the perceptron is correct, there will be no change to the weight. But if the output is wrong, the weight will be moved a small step in the direction of the difference in target vs output.\n\nIf a dataset is [linearly separable](https://en.wikipedia.org/wiki/Linear_separability), the perceptron rule will find the right plane to separate them. That's cool!\n\n![http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/img40.gif](http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/img40.gif)\n\nHowever, if the dataset is not linearly separable (and we have no way to tell ahead of time, especially in multiple dimensions), the Perceptron Rule gets stuck in an infinite loop. We need something more robust.\n\n## Gradient Descent\n\n![https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/700px-Gradient_descent.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/700px-Gradient_descent.svg.png)\n\nIn contrast to the Perceptron Rule, Gradient Descent throws out the idea of a threshold. It uses the same mechanism of incremental updating but uses partial derivatives instead. In fact, the formulas end up looking very similar:\n\n![https://pedropb.github.io/machine-learning-nanodegree/classes/supervised-learning/images/comparison-of-learning-rules.png](https://pedropb.github.io/machine-learning-nanodegree/classes/supervised-learning/images/comparison-of-learning-rules.png)\n\nThe Perceptron Rule guarantees finite convergence if the data is linearly separable. Gradient Descent is more robust, but can only converge toward a local optimum. \n\nThe reason we can't use gradient descent on the Perceptron Rule's `y_hat` is because the activation threshold behavior is a discontinuous function that is undifferentiable. What if we changed that to a function that kinda sorta looks like the activation function but that *is* differentiable?\n\n## Sigmoid function\n\n![https://wikimedia.org/api/rest_v1/media/math/render/svg/9537e778e229470d85a68ee0b099c08298a1a3f6](https://wikimedia.org/api/rest_v1/media/math/render/svg/9537e778e229470d85a68ee0b099c08298a1a3f6)\n\nThe [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) in equation form looks simple enough and has a very appealing chart form:\n\n![https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png)\n\nIf you make it narrow enough it looks like the binary activation function! and if you take its derivative, it turns out that it is `sigmoid(a)` multiplied by `(1 - sigmoid(a))`:\n\n![https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRaEM_DbtCG6GiKYXoJa8uIlpMG0uUYPS3cTpvuoQsnZ-naQCi_2w](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRaEM_DbtCG6GiKYXoJa8uIlpMG0uUYPS3cTpvuoQsnZ-naQCi_2w)\n\nWhich is a very neat closed form solution for gradient descent purposes.\n\n## Networking Perceptrons\n\nHaving a network of differentiable perceptrons is very useful. Despite having multiple hidden layers between input and output, we [can backpropagate](https://en.wikipedia.org/wiki/Backpropagation) the errors to update weights in the entire network!\n\n![https://pedropb.github.io/machine-learning-nanodegree/classes/supervised-learning/images/backpropagation.png](https://pedropb.github.io/machine-learning-nanodegree/classes/supervised-learning/images/backpropagation.png)\n\n## Optimizing Weights\n\nIn gradient descent you can often get stuck in local optima.\n\nTo break this, you can:\n\n- use a [momentum term](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum)\n- use [randomized optimization](https://en.wikipedia.org/wiki/Random_optimization)\n- penalize complexity (similar to what we discussed previously with regression/decision tree overfitting)\n    - more nodes\n    - more layers\n    - large numbers (nonobvious!)\n\n## Bias in Neural nets\n\n**Restriction bias** (the set of hypotheses we will consider):\n\n- Perceptrons: Half planes\n- Sigmoid units: Much more complex - they can represent boolean, continuous (one hidden layer), and arbitrary functions (two hidden layers).\n\nSo Neural nets dont have much restriction bias at all. It also means that it is very possible to overfit. To guard against this, we also use cross-validation and look for divergences in error to identify overfitting.\n\n**Preference bias** (the algorithm's preference for one representation over another):\n\nWe have specified everything for gradient descent except for how it is initialized! Most often we use *small random values*. This is because large weights let us represent arbitrarily complex functions and that is not good.\n\n## Read More\n\nMore is available in [the PDF summary here](https://s3-us-west-2.amazonaws.com/gae-supplemental-media/neural-networkspdf/Neural-Networks.pdf).\n\n## Next in our series\n\nHopefully that was a good introduction to Neural Networks. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "supervised-learning-regression-4d17",
    "data": {
      "title": "Supervised Learning: Regression",
      "description": "Drawing lines among dots and more!",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 3rd in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n## Regression & Function Approximation\n\nWe now turn our attention to continuous, instead of discrete variables. We use the word **regression** in the [statistical](https://en.wikipedia.org/wiki/Regression_analysis) sense of the word.\n\n![https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/440px-Linear_regression.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/440px-Linear_regression.svg.png)\n\nI will assume you know Linear Regression here (including all the linear algebra bits), but [here's a pdf summary](https://s3-us-west-2.amazonaws.com/gae-supplemental-media/linear-regression-reviewpdf/Linear-Regression-Review.pdf) if you need to get caught up.\n\nIn general - you want to pick a good order polynomial for regression that fits your data, but doesn't overfit it in a way that doesn't generalize well. (eg an 8th order polynomial for a 9 point dataset)\n\n## Errors\n\nTraining data often has errors. Where do they come from?\n\n- Sensor error\n- Malicious/bad data\n- Human error\n- Unmodeled error\n\nErrors cause noise, and regression helps us approximate functions without that noise.\n\n## Cross Validation\n\nThe goal is always to generalize. We need a good check for doing our regressions to be sure we are generalizing properly. We can't use the Test set because that is \"cheating\", so the solution is to split out yet another set of our data (from our training set) for the sole purpose of [cross validation](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Cross-validation)\n\n## Errors vs Polynomial\n\nThe usefulness of cross validation for guarding against overfitting is helpful here:\n\n![https://cdn-images-1.medium.com/max/1600/1*Y2ahYXQfkLioau03MLTQ1Q.png](https://cdn-images-1.medium.com/max/1600/1*Y2ahYXQfkLioau03MLTQ1Q.png)\n\nInitially both processes start out with moderate errors at low order polynomials. (The data is **underfit**). As this increases, the fit gets increasingly better. However past a certain point, polynomial continues to fit the training set better and better, but does worse on the CV set. this is where you know you have started to **overfit**.\n\n## Other Input Spaces\n\nWe've so far discussed scalar inputs with continuous outputs, but this same approach can be applied for vector inputs, that have more input features. If you want to sound pretentious, you can call it a hyperplane:\n\n![https://cdn-images-1.medium.com/max/1600/1*ZpkLQf2FNfzfH4HXeMw4MQ.png](https://cdn-images-1.medium.com/max/1600/1*ZpkLQf2FNfzfH4HXeMw4MQ.png) \n\nBut really it is the multidimensional analogue of the 2 dimensional line chart.\n\nYou can encode discrete values as well into regressions, as scalar values, or as a vector of booleans.\n\n## Next in our series\n\nUnfortunately, I'm a former Math major and didn't find much worth noting or explaining in this part of the series. If you need a proper primer on regression, see the linked resources above or seek out your own tutorials. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "machine-learning-classification-learning--decision-trees-1mbh",
    "data": {
      "title": "Supervised Learning: Classification Learning & Decision Trees",
      "description": "The simplest form of Classification algorithm",
      "tag_list": [
        "machinelearning",
        "supervisedlearning"
      ]
    },
    "content": "\n*This is the 2nd in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n## Classification Learning\n\nHere are some useful terms to define in classification:\n\n- **Instances** (or input): values that define whatever your input space is, e.g. the pixels that make up your picture, credit score, other raw data values\n- **Concept**: a function that actually maps inputs to outputs (or mapping an input space to a defined output space), taking Instances to some kind of output like True or False. You can also view them as defining membership in a set, e.g. \"Hotdog\" vs \"Not Hotdog\"\n- **Target Concept**: The Concept that we are trying to find - the \"actual answer\". \n- **Hypothesis** class: The set of all Concepts (aka all classification functions) you're willing to consider\n- **Sample** (or **Training Set**): The set of all Inputs **paired with** a label that is a correct output.\n- **Candidate**: Concept that you *think* might be the Target Concept\n- **Testing Set**: Looks like the Training Set, but is what you test the Candidate Concept on. They should not be the same as the Training Set, because then you will not have shown the ability to **generalize**.\n\nJust like in school, we test the student's understanding of the concept by giving them new questions they haven't seen before. If they can solve those too, then they are deemed to have learned the Target Concept.\n\n## Decision Trees: Representation\n\nA decision tree looks like any tree you see in computer science:\n\n![https://cdn-images-1.medium.com/max/1200/0*Yclq0kqMAwCQcIV_.jpg](https://cdn-images-1.medium.com/max/1200/0*Yclq0kqMAwCQcIV_.jpg)\n\nEach **decision node** represents a question (\"Am I hungry? Have I $25?\") about an **attribute**, and the **edges** that result are complete answers to the question (here we show a binary Yes/No, but you can generalize to more discrete values). The final result is an **output**, where you actually decide to take an action.\n\n## Decision Trees: Learning\n\nDecision trees are very expressive because of their [branching factor](https://en.wikipedia.org/wiki/Branching_factor). This is illustrated in the famous game [Twenty Questions](https://en.wikipedia.org/wiki/Twenty_Questions). Your goal in asking each question is to narrow down the space of possibilities. So asking **more general questions first** is the optimal algorithm here, and we will learn how to define this now.\n\nHere is the best algorithm to solve Twenty Questions:\n\n```\n1. Pick the best attribute to split the tree on \n    - (\"Best\" = splits the possible decision space exactly in half)\n2. Ask the next question\n3. Follow the answer path\n4. Go to 1. until you have an **output** (or answer, in the case of the Twenty Questions game)\n```\n\nThat is almost like what we need for generalized decision trees.\n\n## The ID3 Algorithm\n\n[The Iterative Dichotomiser 3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm) is a top down learning algorithm invented to programmatically **generate a decision tree *from a dataset***. The rough pseudocode goes:\n\n```\nLoop:\n    Select best attribute and assign to A\n    Assign A as decision attribute for Node\n    Foreach value of A\n        create a descendant of Node (aka Leaf)\n    Sort training examples to Leaves\n    If examples are perfectly classified: break;\n    Else Iterate over Leaves\n```\n\nIf you're paying attention you'll see that some of these steps are a lot more complicated than the others. The biggest one is \"selecting the best attribute\". That is done by selecting the attribute with the highest \"Gain\".\n\n**Entropy**\n\nThis term is usually used in thermodynamics, but is ([confusingly](https://en.wikipedia.org/wiki/Entropy_(disambiguation)#Information_theory_and_mathematics)) also used in Information Theory. We refer here to Shannon entropy, \"a measure of the unpredictability or information content of a message source\".\n\n![https://wikimedia.org/api/rest_v1/media/math/render/svg/f96cf5194b9102f383a05c04c8994e7af8b161fb](https://wikimedia.org/api/rest_v1/media/math/render/svg/f96cf5194b9102f383a05c04c8994e7af8b161fb)\n\nThe intuition is more important than the exact formula, and you can refer to the chart form in a binary situation:\n\n![https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/400px-Binary_entropy_plot.svg.png](https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/400px-Binary_entropy_plot.svg.png)\n\nThis is to say, if we were given a data set of all hotdogs, it would have zero entropy since it is already fully sorted with respect to our caring about hotdogs. Likewise, if it were all not hotdogs, it would also have zero entropy. But if it were equally mixed, we would have to start asking questions about the essential nature of a hotdog in order to try to subdivide our dataset in useful ways.\n\n**Information Gain**\n\nInformation Gain is thus the decrease in total Entropy before-and-after applying a particular classification attribute:\n\n![https://pbs.twimg.com/media/DxdymmTUcAApsgY.jpg:large](https://pbs.twimg.com/media/DxdymmTUcAApsgY.jpg:large)\n\nThus, ID3 relies on iterating through and calculating each attribute's Gain after each step, and picking the biggest Gain as the \"best\" attribute to split on.\n\n**Read More**\n\nThere is [a fuller explanation in this PDF](https://storage.googleapis.com/supplemental_media/udacityu/5414400946/ID3%20Algorithm%20for%20Decision%20Trees.pdf).\n\n## ID3 Bias\n\nThere are two forms of bias (not in the negative sense!) inherent in ID3.\n\n- Restriction Bias - the set of all possible Hypotheses we will consider. In the case of decision trees, we've already decided to rule out all continuous functions.\n- Preference Bias - what kinds of hypotheses from the Hypothesis set we prefer. This is at the heart of the Problem of Induction we discussed in the previous article.\n\nID3 biases towards \"good splits at the top\" and \"shorter trees rather than longer trees\".\n\n## Decision Trees: Challenges\n\n- **Continuous Attributes**: How do you expand your model to handle these if they do come up? You can deal with **continuous attributes** by establishing cutoffs, for example Group A being Age <10, Group B being 10 <= age < 20, etc, but these categories are arbitrary and may not be useful without cheating and looking at your Testing Set.\n- **Scaling**: You can represent all boolean combinations of attributes with decision trees: AND, OR, XOR, and so on. However the number of nodes scales exponentially - `O(2^N)` to be exact - so to make any decision at all you have to ask and answer a lot of questions. This is a problem for decision tree models.\n- **Stopping/Noise**: Since you can repeat attributes, do you get stuck in an infinite loop if your data has some noise? (some invalid or inconsistent labelling) \n- **Overfitting**: too large of a tree for the problem. You can mitigate by checking on a [cross validation set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Cross-validation). If an error gets low enough, you stop expanding the tree (expanding breadth first is important here). You can also try to [**prune** trees](https://en.wikipedia.org/wiki/Decision_tree_pruning) after arriving at your final tree.\n\n## Lazy Decision Trees\n\nNormal Decision Trees are *eager*, meaning all the calculation is done upfront. A better way to go about this may be to use [Lazy Decision Trees](http://robotics.stanford.edu/~ronnyk/lazyDT.ps), which construct hypotheses on the fly tailored to a particular given example. You are trading off prediction time for training time here, but the gains may be disproportionate because you may pick up on definitive, fragmented features earlier (see the paper for details).\n\n## Next in our series\n\nHopefully this has been a good high level overview of Decision Trees. I am planning more primers and would love your feedback and questions on:\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - Clustering - week of Feb 25\n    - Feature Selection - week of Mar 4\n    - Feature Transformation - week of Mar 11\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "machine-learning-an-overview-216n",
    "data": {
      "title": "Machine Learning: An Overview",
      "description": "The three major branches of ML",
      "tag_list": [
        "machinelearning"
      ]
    },
    "content": "\n*This is the first in a series of class notes as I go through the [Georgia Tech/Udacity Machine Learning course](https://www.udacity.com/course/machine-learning--ud262). The class textbook is [Machine Learning by Tom Mitchell](https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf).*\n\n![http://i.imgur.com/a69NtFM.jpg](http://i.imgur.com/a69NtFM.jpg)\n\n## Supervised Learning: Learning by Approximation\n\nSupervised Learning takes **labeled** data sets and gleaning information **so that you can label new data sets**. An example is if you look at this table:\n\n| x | y |\n|---|---|\n| Chihuahua | A |\n| German Shepherd | A |\n| Siamese | B |\n| Persian | B |\n| Poodle | ? |\n\nYou might fill in `?` with `A`. You do this despite not being told what `x`, `y`, `A`, or `B` are, but because you had a labeled data set with 4 examples, you were able to infer the right answer (together with some contextual knowledge about dog and cat breeds, which in reality is usually not present in machines but is more interesting to look at than a bunch of numbers).\n\nMore generally, you can view this as **function approximation** where you suppose that your labeled data is the outcome of an unknown \"real\" function, and you are trying to get as close to it as possible with your machine learning algorithm. **Approximation** is a key word here - the unknown function can be marvelously complex, but as long as our approximation walks, talks, and quacks close enough to the real thing (and for [noisy domains this is a very useful assumption!](http://i.stanford.edu/hazy/papers/hogwild-nips.pdf)) then that works for us. Fortunately we have proof that [neural networks](https://en.wikipedia.org/wiki/Universal_approximation_theorem) and [functions](https://en.wikipedia.org/wiki/Approximation_theory) in general can be approximated with deliberate control over accuracy.\n\n**The problem of Induction**\n\nNotice that if in the above example the real function were simply \"is x longer than 7 characters?\" then the correct answer would be `B`, not `A`. \n\nThis presents the problem with generalizing from an incomplete set of data points. Philosophers have long been [familiar with the Problem of Induction](https://en.wikipedia.org/wiki/Problem_of_induction#Ancient_and_early_modern_origins). Supervised learning leans on Induction to go from specific data points to general rules, as opposed to Deduction, where you have axioms/rules and you deduce only what can be concluded from those rules. \n\n**Classification vs Regression**\n\n- **Classification** is the process of taking some kind of input `x` and mapping to a **discrete** label, for example True or False. So for example, given a picture, asking if [it is or is not a Hotdog](https://www.youtube.com/watch?v=ACmydtFDTGs).\n- **Regression** is more about **continuous** value functions, like plotting a line chart through a scatter plot.\n\n## Unsupervised Learning: Learning by Description\n\nWith Unsupervised Learning, we don't get the benefit of labels. We just get a bunch of data and have to figure out some internal structure within what we're given:\n\n\n| x | y |\n|---|---|\n| Chihuahua | ? |\n| German Shepherd | ? |\n| Siamese | ? |\n| Persian | ? |\n| Poodle | ? |\n\n\nHere our goal is to find interesting and useful ways to **describe** our dataset. These descriptions are useful and concise ways to \"cluster\", \"compress\" or \"summarize\" the dataset. This reductionism necessarily loses some details in favor of a more simple model, so we must try to come up with algorithms to decide which are the more important details worth keeping.\n\nUnsupervised learning can be used as an input to Supervised learning, where the features it identifies can form useful labels for faster Supervised learning.\n\n## Reinforcement Learning: Learning by Delayed Reward\n\nIn RL we try to learn like how people learn, as individual smart agents interacting with other smart agents, and trying to figure out what to do over time. The problem with creating an algorithm for this is that often the reward or outcome isn't determined based on a single set of decisions alone, but rather a sequence of them.\n\nFor example, placing an X or O on a blank game of Tic-Tac-Toe doesn't immediately tell you if that was a good or bad move, especially if you don't know the rules. We can only tell if it was a \"better\" or \"worse\" move by playing out all possible games after that starting point, tallying up wins and losses, and working backwards. This is modeled as a [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process).\n\nMore generally, RL describes problems where you are given a set of valid decisions or actions to make, but have no idea how they impact the eventual outcome that you want, and having to figure out how to make those decisions at each step of the way.\n\n## Comparison of SL, UL and ML\n\nThe truth is, these categories aren't all that distinct. In SL there is the problem of induction, so it might seem that UL is better due to its agnosticism. But in practice, what we choose to cluster or emphasize involves some implicit assumptions we impose anyway. In some sense you can turn any SL problem into an UL problem.\n\nYou can also view these approaches as different kinds of optimization:\n\n- in SL you want something that labels data well - so you try to approximate a function that does that\n- in RL you want behavior that scores well\n- in UL you make up some criterion and then find clusters that organize the data so that it scores well on the criterion\n\nEverything boils down to one thing: **DATA**. Data is king in all Machine Learning, and more data generally helps. However more importantly, all these approaches assume Data is clean, consistent, trustworthy, and unbiased. They all start with data (even if the data is generated), not, for example, a theoretical hypothesis like how the human scientific method might suggest.\n\n\n![https://blogs.sas.com/content/subconsciousmusings/files/2017/09/styles-of-learning.jpg](https://blogs.sas.com/content/subconsciousmusings/files/2017/09/styles-of-learning.jpg)\n\n## Further Resources\n\nClass notes from other students going thru the class:\n\n- [Flash Cards](https://www.brainscape.com/p/1VTHS-LH-75H06)\n- [Lecture Notes for the first half of the class](https://docs.google.com/document/u/1/d/1pmLYTz2_P1_Z-8nuQUyT7ucgRgn0cRLwOA2r4-P-rGI/mobilebasic#heading=h.kmosjdeu4zh) - [More notes](https://github.com/mohamedameen93/Machine-Learning-Notes)\n- [Notes for the entire class](https://github.com/mohamedameen93/Machine-Learning-Notes)\n\n## Next in our series\n\nHopefully this has been a good high level overview of Machine Learning. I am planning more primers and would love your feedback and questions on:\n\n\n\n- [Overview](https://dev.to/swyx/machine-learning-an-overview-216n)\n- Supervised Learning\n    - [Decision Trees](https://dev.to/swyx/machine-learning-classification-learning--decision-trees-1mbh)\n    - [Regression](https://dev.to/swyx/supervised-learning-regression-4d17)\n    - [Neural Networks](https://dev.to/swyx/supervised-learning-neural-networks-mpo)\n    - [Instance Based Learning (K Nearest Neighbors)](https://dev.to/swyx/supervised-learning-instance-based-learning-and-k-nearest-neighbors-kge)\n    - [Ensemble Learning (AdaBoost)](https://dev.to/swyx/supervised-learning-ensemble-learning-lim)\n    - [Kernel Methods & SVMs](https://dev.to/swyx/supervised-learning-support-vector-machines-3mgk)\n    - [Computational Learning Theory](https://dev.to/swyx/supervised-learning-computational-learning-theory-160h)\n    - [VC Dimensions](https://dev.to/swyx/supervised-learning-vc-dimensions-10b)\n    - [Bayesian Learning](https://dev.to/swyx/supervised-learning-bayesian-learning-403l)\n    - [Bayesian Inference](https://dev.to/swyx/supervised-learning-bayesian-inference-4l72)\n- Unsupervised Learning\n    - [Randomized Optimization](https://dev.to/swyx/unsupervised-learning-randomized-optimization-4c1i)\n    - [Information Theory](https://dev.to/swyx/unsupervised-learning-information-theory-recap-4iem)\n    - [Clustering](https://dev.to/swyx/unsupervised-learning-clustering-42mi)\n    - [Feature Selection](https://dev.to/swyx/unsupervised-learning-feature-selection-84f)\n    - [Feature Transformation](https://dev.to/swyx/unsupervised-learning-feature-transformation-pcf)\n- Reinforcement Learning\n    - Markov Decision Processes - week of Mar 25\n    - \"True\" RL - week of Apr 1\n    - Game Theory - week of Apr 15"
  },
  {
    "slug": "networking-essentials-network-security-1fcp",
    "data": {
      "title": "Networking Essentials: Network Security",
      "description": "What is Traffic Engineering?",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the eleventh and last in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\n---\n\nThis is the third of a 3 part miniseries on Network Operations and Management.\n\n\n- [Software Defined Networking](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)\n\n\n---\n\n## Why Network Security?\n\nThere are a wide variety of attacks on various parts of the infrastructure we have covered in this series (*I will not link them here for brevity but you can scroll to the bottom for the relevant topics we covered*):\n\n- **Routing (BGP)**: In 2010, [China accidentally hijacked 50,000 IP prefixes from 170 countries for 18 minutes](https://arstechnica.com/information-technology/2010/11/how-china-swallowed-15-of-net-traffic-for-18-minutes/). This highlights the BGP's vulnerability where any AS can advertise an IP prefix to a neighboring AS and they will take it at face value and rebroadcast it to the rest of the Internet.\n- **Naming (DNS)**: A **Reflection Attack** is a type of Distributed Denial of Service - generating very large amounts of traffic targeted at a victim. **Phishing** is also a common attack attempting to get the user's personal information on a rogue website.\n\nThe Internet's design is **fundamentally insecure**. It was design for simplicity over security, and a host is \"on by default\" or reachable by any other host, so if you have an insecure host, it is wide open to attack. This was fine when the Internet was a small network of trusted networks, but is no longer fine now. Its decentralized nature also makes it hard to coordinate defense.\n\n## Resource Exhaustion Attacks\n\nAs we learned in the first post, **packet switching** allows multiple hosts to share the same link using **statistical multiplexing**, making it easy to achieve high utilization, but also making it easy to overload the link. These attacks target the **Availability** of a system.\n\n## Confidentiality and Authenticity Attacks\n\nWe also want our systems to provide **Confidentiality**, **Authenticity**, and **Integrity**, for example when we execute a banking transaction we want it to be private and to make sure the origin of the information really is our bank and that it hasn't been modified in-flight ([a Man In The Middle attack](https://en.wikipedia.org/wiki/Man-in-the-middle_attack)).\n\n## The Negative Impacts of Attacks\n\nCompromising **Availability**, **Confidentiality**, **Authenticity**, and **Integrity** can lead to:\n\n- Theft of confidential info\n- Unauthorized use of resources\n- False information\n- Disruption of legitimate services\n\n## Routing Security\n\nFocusing on BGP and control plane security, this involves authentication of messages being advertised by the routing protocol:\n\n- **Session authentication**: protects point-to-point communication between routers\n- **Path authentication**: protects the AS path\n- **Origin authentication**: protects the *origin* AS, guaranteeing that the origin AS that advertises a prefix is actually the owner of that prefix. (e.g. preventing route hijacking)\n\n## Origin Authentication: Route Hijacking\n\nFor a successful MITM attack, all traffic headed for the legitimate destination needs to be routed to the attacker, while the attacker's original connection with the legitimate location remains intact.\n\nThis is done by **AS path poisoning** - for the attacker's route to the legitimate location to be preserved, it prepends the path hosts onto its own AS advertisements so that they ignore the attacker's messages in an effort to avoid a loop.\n\nThis makes the MITM successful, however the circuitous route might be spotted when running a `traceroute`. The MITM can be hidden even from that, since a `traceroute` just consists of \"ICMP time exceeded messages\" from when a packet reaches a TTL of 0 - which normally would be decremented by each router along the way. If the routers in the attacker's network *never decrement the TTL*... then no messages would be generated, and the attacker AS would not be visible in a `traceroute`.\n\n## Session Authentication\n\nThe session between two ASes is a TCP session, so we just use [TCP's MD5 authentication option](https://tools.ietf.org/html/rfc2385). Every message exchanged in the connection not only contains the message, but also the [MD5 hash](https://en.wikipedia.org/wiki/MD5) of the message with a shared secret key (manually distributed \"out of band\" (aka offline) between the AS1 and AS2 operator).\n\nAnother way to guard connections is to send packets with a TTL < 254. Because most eBGP sessions are only a single hop, and an attacker would be remote, it is not possible for the recipient AS to accept packets from a remote attacker. This is called [the TTL Hack Defense](https://bird.network.cz/pipermail/bird-users/2014-April/004276.html).\n\n## Origin and Path Authentication: BGPsec\n\nTo guarantee Origin and Path authentication, [the BGPsec proposal](https://en.wikipedia.org/wiki/BGPsec) was introduced. It is made up of two parts:\n\n1. **Origin Attestation** (aka Address Attestation): A signed certificate binding an IP prefix to its owner. It must be signed by a \"trusted party\" like a routing registry or prefix allocating organization\n2. **Path Attestation**: A set of signatures that accompany the AS path as it is advertised from one AS to the next. So say along a path AS1-AS2-AS3, each AS has a public-private key pair. Each AS signs a path with its own private key and every other AS can check it with the public key. When an AS is rebroadcasting a path from another AS, it forwards on that original path's signature, as well as a new signature adding itself to the path. Thus each AS can independently verify every step of a path is untampered because it has cumulative information signed by every private key along the path.\n\nBGPsec would prevent against a variety of possible related attacks:\n\n- Path hijacks\n- Path shortening\n- Path modification\n\nHowever there are others it doesn't account for:\n\n- Path Suppression (if an AS -fails- to advertise a new route, or a route withdrawal)\n- Replay attacks (readvertisement of a withdrawn route)\n- There is no way to guarantee that the data traffic *actually travels along* the advertised path (!!! a significant weakness yet to be solved by any routing protocol)\n\n## DNS Security\n\nA small refresher on how DNS works:\n\n- Your browser requests a new, unknown domain name\n- Your ISP's stub resolver issues a query to a caching resolver\n- Caching resolver has nothing in cache so it forwards the query to an [Authoritative Name Server](https://www.dnsknowledge.com/whatis/authoritative-name-server/) (ANS)\n- the ANS often has Master and Slave replication for resilience\n- the ANS also contains [Zone files](https://en.wikipedia.org/wiki/Zone_file) and a [Dynamic Update System](https://en.wikipedia.org/wiki/Domain_Name_System#Authoritative_name_server)\n\nHere are some vulnerabilities in that system:\n\n- MITM attack between stub resolver and caching resolver\n    - Defense: DNSSEC\n- **Cache Poisoning** attack on the caching resolver to send a reply back to it before the real reply comes back from the ANS. *This is a big one so we will spend more time on it below.*\n    - Defense: 0x20\n- Spoofing of the master or slave ANSes\n    - Defense: DNSSEC\n- \"DNS Reflection\": using DNS to mount a large DDoS attack\n\nDNS resolvers are fundamentally vulnerable because their queries create a race condition - they trust the first respondent whoever it is. The basic DNS protocols have no means for authenticating responses. DNS also uses UDP, which is \"connectionless\" (no session like TCP has) so the resolver has no way of mapping the response it receives for a query other than a query ID.\n\n## DNS Cache Poisoning and how to Defend from them\n\nIn an ideal world, the stub resolver sends its `A` query to the caching (or recursive) resolver, which sends it on to the ANS, which replies with the IP all the way back. \n\n\n**Attacks**\n\nTo attack it, the attacker just needs to flood multiple replies guessing different query IDs and let one of them match. As long as the bogus message is received before the legitimate one, it will be accepted, and cached! DNS has no way to expunge a message once it has been cached, so it will continue to respond with this bogus data until the entry expires from the cache. [Here is a related paper on Prefix Hijack Attacks.](https://www2.cs.arizona.edu/~bzhang/paper/07-dsn-hijack.pdf)\n\nA further vulnerability lies in the `A` vs `NS` query system. `A` records request specific subdomains, whereas `NS` records request entire zones. An attacker can generate a stream of bogus A record queries that clearly don't exist like `fakesubdomain.google.com` and then respond with an `NS` record for the entire zone, and it would still own `google.com`. This is known as [the Kaminsky attack](https://duo.com/blog/the-great-dns-vulnerability-of-2008-by-dan-kaminsky).\n\n**Defenses**\n\nThe query ID is a first-level defense, but of course, it can be guessed. It can be made harder to guess by randomizing it, but it is only 16 bits (32,000) so the entire space is guessable (and by probability will match within a few hundred replies). We can add an additional 16 bits of entropy to the ID randomization by randomizing the source port of the query, however this may be resource intensive and counteracted by NATs. \n\nThe **0x20** defense refers to the fact that DNS is case insensitive, so the **0x20** character (which controls capitalization) isn't used, so we can use it to introduce additional entropy. So we can capitalize a request like `www.GoOgLE.com` and the pattern of upper-and-lower case would form an encoded \"channel\". Agreeing on a shared key between the resolver and the ANS would help generate the correct sequence of upper and lower case. This makes it even more difficult to inject a bogus reply.\n\nThe **DNSSEC protocol** adds authentication on top of DNS by adding signatures to responses returned for each DNS reply. So when a resolver sends a `A google.com` query to a root server, it responds with the referral to the `.com` server but also a signature of the root of the IP address and the public key of the `.com` server. So if the resolver has the public key of the root server, it can now transfer its trust over to the `.com` server as well, which sends a signed referral to the `google.com` server. In other words, each level in the hierarchy now not only responds with the next level's data, but also a signature containing the IP address of that referral as well as the public key for that referral.\n\n## DNS Amplification Attack\n\nThere is an asymmetry in size between queries and responses. An attacker's query can spoof a victim as the query source, and it is only 60 bytes. However, the DNS resolver's response will be about 3000 bytes! This is the \"amplification\" in this attack. Using multiple attackers to send spoofed requests will generate multiple large replies all heading to the victim, creating a DDoS attack.\n\nTo defend against this we must prevent IP address spoofing by disabling the ability for a DNS resolver to resolve queries from arbitrary locations on the Internet, which limits its usefulness.\n\n\n## The end of our series\n\nHopefully this has been a good high level overview of how Network Security works, and this entire series has been a useful introduction to you in some way. As a reminder, here are all the topics we have covered:\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)\n\nWhat do you think? What other topics should I cover? [Let me know @swyx on Twitter! ](https://twitter.com/swyx)"
  },
  {
    "slug": "networking-essentials-traffic-engineering-13c4",
    "data": {
      "title": "Networking Essentials: Traffic Engineering",
      "description": "What is Traffic Engineering?",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the tenth in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\n---\n\nThis is the second of a 3 part miniseries on Network Operations and Management.\n\n\n\n- [Software Defined Networking](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)\n\n\n---\n\n## What is Traffic Engineering?\n\nTraffic Engineering is how network operators deal with large amounts of data flowing through their networks. They reconfigure the network in response to changing traffic loads to achieve some operational goals, like:\n\n- Traffic ratios in a peering relationship (aka \"peering ratios\")\n- Relieve congestion\n- Balance load more evenly\n\n[Software Defined Networking](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9) is used to make Traffic engineering easier in both **data center networks** and **transit networks**.\n\n## Doesn't the Network manage itself?\n\nAlthough we have covered how [TCP](https://dev.to/swyx/networking-essentials-congestion-control-26n2) and [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/) both manage themselves (adapting to congestion, or to topology changes), network still may not run **efficiently**. There may be needless congestion with unused idle paths. So the key question a traffic engineer address is: \"How should Routing adapt to Traffic?\"\n\n## Intra-domain Traffic Engineering\n\nIn a standard network topology, every link has a weight associated with it. A very simple configuration is to tweak the weight according to your priorities. For example:\n\n- Link weight inversely proportional to capacity\n- Link weight proportional to propagation delay\n- Some other Network-wide optimization based on traffic\n\n## The 3 steps of Traffic Engineering\n\n- **Measure**: figure out the current traffic loads\n- **Model**: how configuration affects the paths in the network\n- **Control**: reconfiguring the network to assert control over how traffic flows\n\nAs an example, you can measure **topology** and **traffic**, feed them into a predictive \"What-If\" model, **optimizing for an objective function**, generating the changes you want to make and then feed that back into the network by readjusting link weights.\n\nThe objective function is an important decision in this process. We can choose to minimize the maximum congested link in the network, or just evenly splitting traffic loads across links, or something else.\n\n## Optimizing for the Link Utilization Objective\n\nEven a simple model of the \"cost of congestion\" as increasingly quadratically (as a square) with congestion is [an NP-complete problem](https://en.wikipedia.org/wiki/NP-completeness) - so it is not mathematically solvable. Instead we have to search through a large set of combinations of link weight settings to find a good setting. In practice, this is fine.\n\nWe also have other constraints to our search, which reduce the number of things we try. For example we want to minimize changes to the network. Often just 1 or 2 link weight changes is enough. Our solution must also be resistant to failure and robust to measurement noise.\n\n## Interdomain Traffic Engineering\n\nRecall that Interdomain routing concerns routing that occurs between domains or ASes. (See [our discussion of the Border Gateway Protocol](https://dev.to/swyx/networking-essentials-routing-5gb7/)). Interdomain Traffic Engineering thus involves reconfiguring the BGP policies or configurations that are running on individual edge routers in the network. \n\nNote:\n\n- Changing these policies on the edge can cause routers inside the network to direct traffic to or away from certain edge links.\n- We can also change the set of egress links for a particular destination, based on congestion, or change in quality of link, or some violation of a peering agreement (like exceeding an agreed load over a certain time window)\n\nOur actions derive from our goals for Interdomain TE:\n\n- **Predictability** (predict how traffic flows will change in response to changes in the network configuration)\n    - Downstream neighbors may make changes in response to our changes, and this is a problem for us again\n    - So we should not make any **globally visible changes**\n- **Limit influence of Neighboring domains** \n    - So we should make **consistent route advertisements** and limit the influence of AS path length\n- **Reduce overload of routing changes** (i.e change as few IP Prefixes as possible)\n    - So we group prefixes according to those that have common AS paths and move traffic by grouped prefixes\n\n## Multipath Routing\n\nOne technique applicable in Inter- and Intra-domain routing is Multipath routing - routing traffic across multiple paths. The simplest example of this is setting an equal weight on multiple paths, or Equal Cost Multi Path (ECMP). This would send traffic down those paths in equal amount.\n\nA source router can also set percentage weights on paths, for example 35% on one and 65% on another, and it might do this based on observed congestion!\n\n## Data Center Networking\n\nData Center Networks have three characteristics:\n\n- [Multi-tenancy](https://www.datacenterknowledge.com/archives/2014/08/07/four-big-advantages-multi-tenant-data-centers) - allows cost sharing, but also must provide security and resource isolation\n- Elastic resources - allocating up and down based on demand. Allowing pay per use business model.\n- Flexible service management - ability to move workloads to other locations inside the datacenter with virtual machine migration.\n\nSo our requirements develop accordingly. We need to:\n\n- load balance traffic\n- support VM migration\n- saving power\n- Provisioning (when demand fluctuates)\n- providing security guarantees\n\nA typical Data center topology has 3 layers:\n\n![https://tse4.mm.bing.net/th?id=OIP.7ZfRQrajOY55baa9vvqPZwHaFj&pid=Api](https://tse4.mm.bing.net/th?id=OIP.7ZfRQrajOY55baa9vvqPZwHaFj&pid=Api)\n\n- The **Access** layer connects to the servers themselves\n- The **Aggregation** layer\n- The **Core** layer \n\nThe Core layer is now commonly done with [a layer 2 topology](https://www.netbraintech.com/ftp/CE60/OnlineHelp/files_a_8_3.htm) which makes it easier to migrate and load balance traffic, but is harder to scale because we now have x0,000s of servers on a single, flat topology.\n\nThis hierarchy can also create single points of failure and links in the Core can become oversubscribed. In real life datacenters the links at the top can carry up to 200x the traffic of links at the bottom, so there is a capacity mismatch.\n\n## Scaling in the Access Layer\n\nOne interesting way to deal with the Scaling issue is \"Pods\".\n\nIn the Access layer, every machine has an independent MAC address. This means every switch in the layer above needs to store a forwarding table entry for every single MAC address. The solution is to assign groups of servers by switch as \"Pods\", and assign them \"pseudo-MAC addresses\". Thus servers only need to maintain entries for reaching other Pods in the topology.\n\n![pods](https://www.cisco.com/c/dam/en/us/products/collateral/switches/nexus-7000-series-switches/white-paper-c11-737022.docx/_jcr_content/renditions/white-paper-c11-737022_1.jpg)\n\n## Load Balancing across the Data Center\n\nTo spread traffic evenly across the servers in a Data Center, Microsoft invented [Valiant Load Balancing](https://www.microsoft.com/en-us/research/publication/vl2-a-scalable-and-flexible-data-center-network/?from=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2Fdefault.aspx%3Fid%3D80693) in 2009. It achieves balance by inserting an \"indirection level\" into the switching hierarchy. The switch is selected *at random* - and once it is selected it finishes the job of sending the traffic to its destination. Picking random interaction points to balance traffic across a topology actually comes from multiprocessor architectures and has been rediscovered for data centers.\n\n## Jellyfish Technique\n\n*Read the [Jellyfish paper here](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final82.pdf)*\n\nSimilarly to Valiant, Jellyfish networks Data Centers randomly, to support high throughput (eg for big data or agile placement of VMs) and incremental expandability (so you can easily add or replace servers and switches).\n\nThe structures of Data Centers tend to constrain expansion. For example, the [Hypercube](https://hellosemi.com/hypercube/pmwiki.php?n=Main.HypercubeDataCentre) requires `2^k` switches (`k` = number of servers). [FAT trees](https://en.wikipedia.org/wiki/Fat_tree) are more efficient (O(`k^2`)) but still quadratic.\n\nHere is a FAT tree - you can see the congestion at the top (Access) level:\n\n![https://2.bp.blogspot.com/-8uKSOcP-FDQ/T5ZhZpef-wI/AAAAAAAAAOE/lw-Pw0aMed4/s400/FatTree.png](https://2.bp.blogspot.com/-8uKSOcP-FDQ/T5ZhZpef-wI/AAAAAAAAAOE/lw-Pw0aMed4/s400/FatTree.png)\n\nJellyfish's topology is a \"Random Regular Graph\" - each graph is uniformly selected at random from the set of all \"regular\" graphs. A \"regular\" graph is one where each node (a switch, in this context) has the same degree.\n\nHere is a Jellyfish - having *no* structure is great for robustness!\n\n![https://2.bp.blogspot.com/-fES5sXO_rF4/T5Zhy7AeinI/AAAAAAAAAOQ/KMZ0g6eMv8s/s1600/Jellyfish.png](https://2.bp.blogspot.com/-fES5sXO_rF4/T5Zhy7AeinI/AAAAAAAAAOQ/KMZ0g6eMv8s/s1600/Jellyfish.png)\n\n## Next in our series\n\nHopefully this has been a good high level overview of how Traffic Engineering works. I am planning more primers and would love your feedback and questions on:\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "networking-essentials-software-defined-networking-35n9",
    "data": {
      "title": "Networking Essentials: Software Defined Networking",
      "description": "Why Software Defined Networking is taking the networking world by storm",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the ninth in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\n---\n\nThis is the first of a 3 part miniseries on Network Operations and Management.\n\n- [Software Defined Networking](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)\n\n---\n\nBefore we get to SDNs, a precursor on Network Management:\n\n## Network Management\n\nNetwork Management is the process of configuring the network to achieve a variety of tasks:\n\n- Load balancing\n- Security\n- Business Relationships\n\nIf the network is not configured correctly, many things can go wrong. Configuration mistakes can lead to problems like:\n\n- Persistent Oscillation (routers dont agree how to route to a destination)\n- Loops (packets get stuck between routers and don't make it to a destination)\n- Partitions (network split into two unconnected segments)\n- Black Holes (packets reach a router that just drops it)\n\nConfiguration is hard because:\n\n- Defining \"correctness\" is hard\n- Interactions between protocols lead to unpredictability\n- Humans make mistakes across hundreds of vendor specific low level configuration\n\n**Software Defined Networking** changes this and centralizes logic in a single controller by providing:\n\n- Network-wide views of Topology and Traffic\n- Network-level objectives (eg Load balance and security)\n- Direct Control\n\nThis means that the totally distributed model of routing we had before must end, routers no longer need to compute their own routes, aka **Remove Routing from Routers**.\n\n## Software Defined networking (SDN)\n\n**What is an SDN?**\n\nIt helps to add a third dimension to the normally 2D conception of networks. We can define the basic job of the **Data Plane** of the network as **forwarding traffic**. But we can also define a **Control Plane** of the network which computes routing tables, which take into account the state of each router.\n\nThe Control Plane is the logic that controls forwarding behavior eg for routing protocols and network middlebox configuration.\n\nThe Data Plan simply forwards and switches traffic according to control plane logic.\n\nIn a traditional network, data and control planes are put on the routers. But in an SDN, control is centralized while data is still decentralized.\n\nThis idea started in 2004 as the [Routing Control Platform](https://queue.acm.org/detail.cfm?id=2560327) for the BGP protocol and was generalized to 4 different planes soon after (\"**4D**\"):\n\n- Decision Plane \n- Dissemination/Discovery Planes\n- Data Plane \n\nThis was made practical in 2008 with by switch vendors opening their API and adopting the [Openflow](https://en.wikipedia.org/wiki/OpenFlow) protocol.\n\n**What are the advantages of SDN?**\n\nThey are easier to:\n\n1. Coordinate\n2. Evolve (eg Virtual Machine migration in Data centers)\n3. Reason about\n\nIt is also easier to experiment by deploying experiments for research alongside existing networks.\n\nHaving a separate **control plane** allows us to apply software techniques to old networking problems. We write the Control Plane in a high level programming language like Python or C, and the Data Plane is programmable hardware that obeys OpenFlow control commands to operate the switch.\n\nWe can apply SDNs to Data centers, Backbone networks, Enterprise networks (where it is easier to write Security protocols), Internet Exchange Points, and Home Networks. We will focus on the first 3.\n\n## SDNs in Data Centers\n\nData centers consist of racks of servers (as many as 20k servers/cluster). If each server can run 400 VMs, thats 400k VMs in a cluster. It is a problem provisioning/migrating these machines in response to load. So the solution is to program switch state from a central database.\n\nWe also make the servers addressed with Layer 2 \"flat\" addressing - so a server can be migrated to another section of the data center without requiring the VM to obtain new addresses, and just updating the switch state, which is much easier to do.\n\n## SDNs and Internet Backbone Security\n\nThe goal is to filter attack traffic from DDoS attackers. If high traffic is detected, the controller (like RCP) might install a \"null route\" to ensure no more traffic reaches the victim from the attacker.\n\n## Challenges with SDNs\n\n1. Scalability: How do you scale > 1000's of switches?\n2. Consistency: Ensuring different replicas see the same view\n3. Security/Robustness: how to recovery from failure/compromise?\n\n[More on OpenFlow, Nicira and its founder Martin Casado.](https://www.infoworld.com/article/2618042/virtualization/why-is-nicira-worth--1-2-billion-.html)\n\n## Different SDN Controllers\n\nSome options\n\n- [Nox](https://searchsdn.techtarget.com/definition/NOX)\n- [Ryu](https://osrg.github.io/ryu/) - a Python controller supporting OpenStack and 1.2+ versions of OpenFlow\n- [Floodlight](http://www.projectfloodlight.org/floodlight/) - a Java controller\n\nHigher level controllers\n\n- [Pyretic/Frenetic](http://frenetic-lang.org/pyretic/)\n- [Procera](https://www.sdxcentral.com/listings/procera-networks/)\n- [RouteFlow](https://www.sdxcentral.com/listings/routeflow-project/)\n- [Trema](https://trema.github.io/trema/)\n- etc.\n\n## Nox\n\nThis is a first-generation OpenFlow controller. It is [open-source](https://github.com/noxrepo?tab=overview&from=2018-03-01&to=2018-03-31), stable, and widely used. The NOX Architecture has a set of switches and a set of network-attached servers.\n\nThe basic abstraction that Nox supports is a switch control abstraction where OpenFlow is the prevailing protocol. Control is defined by the header of the packet as defined in the OpenFlow spec, which has:\n\n- a header ([10-tuple](https://www.sdxcentral.com/articles/contributed/sdn-openflow-tcam-need-to-know/2012/07/))\n- counter: statistics on that flow\n- actions: e.g. forward, drop, send to controller\n\nWhen a switch receives a packet, it:\n\n- updates its counter for that flow\n- applies corresponding actions\n\nNOX is event-based. The basic programmatic interface for the NOX controller which knows how to process different types of Events such as (Switch Join/Leave, or Packet In/Receive events). The controller also keeps a network view, which includes a view of the underlying topology, and also speaks a control protocol to the switches in the network, allowing it to update their state.\n\nNOX provides good performance, but requires using low-level OpenFlow commands and C++. POX is a fork that uses Python and is higher level, for less good performance.\n\n\n## Next in our series\n\nHopefully this has been a good high level overview of how Software Defined Networking works. I am planning more primers and would love your feedback and questions on:\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "my-new-mac-setup-4ibi",
    "data": {
      "title": "My New Mac Setup",
      "description": "quick checklist for setting up a new macbook",
      "tag_list": [
        "mac"
      ]
    },
    "content": "\nI set up a new Mac for work today. Here's what I did immediately:\n\n- Settings:\n    - Disable Ask Siri\n    - [Big cursor](https://www.lifewire.com/make-mac-mouse-pointer-bigger-2260808)\n    - Trackpad -> Scroll & Zoom - Natural off\n    - Trackpad -> Point & Click -> Look up & data detectors off\n    - turn on Airdrop\n- Finder:\n    - show filename extensions\n    - show dotfiles (just hold Cmd + Shift + . (dot) in a Finder window)\n    - [show path bar](https://www.tekrevue.com/tip/show-path-finder-title-bar/)\n- Keyboard:\n    - [remap command+Q to literally anything else](https://apple.stackexchange.com/questions/78948/how-to-disable-command-q-for-quit)\n    - copy picture of selected area to clipboard -> Cmd+E\n- Dock:\n    - Remove everything from the Dock except: Finder, System Preferences and Trash\n    - Turn Hiding on\n- Browser: Download Chrome, set to default.\n- Chrome extensions:\n    - Morpheus Dark theme\n    - 1password\n    - [Display Anchors](https://chrome.google.com/webstore/detail/display-anchors/poahndpaaanbpbeafbkploiobpiiieko/related?hl=en)\n    - [React Devtools](https://chrome.google.com/webstore/detail/react-developer-tools/fmkadmapgofadopljbjfkapdkoienihi?hl=en)\n    - [Refined Github](https://github.com/sindresorhus/refined-github)\n    - [Code Copy](https://chrome.google.com/webstore/detail/codecopy/fkbfebkcoelajmhanocgppanfoojcdmg?hl=en)\n    - [Video Speed Controller](https://chrome.google.com/webstore/detail/video-speed-controller/nffaoalbilbmmfgbnbgppjihopabppdk?hl=en)\n    - [Palettab](https://palettab.com/)\n    - [Privacy Badger](https://chrome.google.com/webstore/detail/privacy-badger/pkehgijcmpdhfbdbbnkijodmdjhbjlgp?hl=en-US)\n    - [RescueTime](https://chrome.google.com/webstore/detail/privacy-badger/pkehgijcmpdhfbdbbnkijodmdjhbjlgp?hl=en-US)\n    - [uBlock Origin](https://chrome.google.com/webstore/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm?hl=en)\n    - [Octolinker](https://chrome.google.com/webstore/detail/octolinker/jlmafbaeoofdegohdhinkhilhclaklkp?hl=en)\n    - [async render toolbox](https://github.com/sw-yx/async-render-toolbox) (i made this)\n- Emojis: https://matthewpalmer.net/rocket/\n- Password Manager: https://www.1password.com/ (i have a company account)\n- Window Manager: https://www.spectacleapp.com/\n    - launch at login\n- Clipboard Manager: https://clipy-app.com/\n- Screenshots: https://zapier.com/zappy\n- Caffeine (Keep Mac awake for talks): http://lightheadsw.com/caffeine/\n- Video capture: https://getkap.co/\n- Dual Screen: https://www.duetdisplay.com/\n- Gifs: https://download.cnet.com/g00/LICEcap/3000-2094_4-75984664.html?i10c.encReferrer=&i10c.ua=1&i10c.dv=3\n- [Slack](https://slack.com/downloads/osx) or [Discord](https://discord.com/)\n- OBS: https://obsproject.com/\n- SkyFonts: https://www.fonts.com/web-fonts/google\n- Microsoft Todo: https://apps.apple.com/app/apple-store/id1274495053?mt=8\n- Stretchly: https://hovancik.net/stretchly/\n- SimpleNote: https://apps.apple.com/us/app/simplenote/id692867256?ls=1&mt=12\n- Notion: https://www.notion.so/desktop\n   - https://chrome.google.com/webstore/detail/notion-web-clipper/knheggckgoiihginacbkhaalnibhilkk?hl=en\n- App Search/Utils: https://www.alfredapp.com/\n    - set to Alfred Dark\n    - [airdrop to iphone/ipad](https://github.com/swmoon203/CrossShare/blob/master/Alfred%20Workflow/Cross%20Share%20Airdrop.alfredworkflow)\n    - [Cupcake Ipsum](http://www.packal.org/workflow/cupcake-ipsum)\n- My dotfiles: https://gist.github.com/sw-yx/7fa1009e460ecb818d5e6d9ca4616a05\n- [Github CLI](https://github.com/cli/cli): `brew install github/gh/gh`\n- Editor: Download VS Code (I used to use Insiders but the popups are super annoying). use Settings Sync to sync across machines\n  - have to set up powerline fonts \"Meslo LG M for Powerline\" ([download](https://github.com/powerline/fonts/blob/master/Meslo%20Slashed/Meslo%20LG%20M%20Regular%20for%20Powerline.ttf))\n  - auto-close-tag v0.5.6\n  - auto-rename-tag v0.0.15\n  - Bookmarks v9.1.0\n  - code-settings-sync v3.1.2\n  - debugger-for-chrome v4.10.2\n  - es7-react-js-snippets v1.8.7\n  - graphql-for-vscode v1.12.1\n  - mdx v0.1.0\n  - prettier-vscode v1.6.1\n  - python v2018.9.2\n  - python v0.2.3\n  - rainbow-brackets v0.0.6\n  - shades-of-purple v3.17.0\n  - vscode-graphql v0.1.5\n  - vscode-import-cost v2.9.0\n  - vscode-styled-components v0.0.23\n  - vscode-wakatime v1.2.3\n- Terminal environments   \n    - [ZSH](https://ohmyz.sh/) (first usage of `git` will prompt you to install git)\n        - Font - [Inconsolata for Powerline](https://github.com/powerline/fonts/blob/master/Inconsolata/Inconsolata%20for%20Powerline.otf)\n        - [autosuggestions](https://github.com/zsh-users/zsh-autosuggestions)\n        - [syntax highlighting](https://github.com/zsh-users/zsh-syntax-highlighting)\n    - [Homebrew](https://brew.sh/) - i have a bunch more stuff in `brew list` but i'm not sure what i use actively.\n        - [fzf](https://github.com/junegunn/fzf) - usage example is [here](https://twitter.com/swyx/status/1048009961723318272)\n        - [yarn](https://yarnpkg.com/en/docs/install#mac-stable) note `brew install yarn --ignore-dependencies` since i use nvm\n        - [`brew install z`](https://brewinstall.org/install-z-on-mac-with-brew/) - REALLY GOOD TRY IT\n        - `brew install python`\n        - `brew install deno`\n        - `pip3 install --user powerline-status`\n        - go to a neutral folder and `git clone https://github.com/powerline/fonts && cd fonts && ./install.sh`\n    - [Node.js/NPM](https://nodejs.org/en/download/)\n        - `npm login`\n        - `sudo npm install netlify-cli -g`\n        - `npm i -g sign-bunny fortune-node parrotsay`\n        - `sudo npm install -g @aws-amplify/cli`\n        - `amplify configure`\n        - `sudo npm install -g trash-cli`\n    - [nvm](https://github.com/creationix/nvm)\n    - [Anaconda](https://www.anaconda.com/download/#macos)\n    - [Docker Desktop](https://hub.docker.com/editions/community/docker-ce-desktop-mac/)\n    - [Java Development Kit](https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html) \t\nf*ckoracle@mailinator.com ******\n    - [GoLang](https://golang.org/dl/) dont forget `export PATH=$PATH:/usr/local/go/bin`\n    - [Hyper Terminal](https://hyper.is/) - make sure to set [shell: '/bin/zsh'](https://gist.github.com/robertcoopercode/276d7cf66e9b0eea48c117fff1762a17#file-hyper-js-L60)\n    - `fontFamily: '\"Inconsolata for Powerline\", Menlo, \"DejaVu Sans Mono\", Consolas, \"Lucida Console\", monospace',`\n    - [diff-so-fancy](https://www.npmjs.com/package/diff-so-fancy)\n\n```\nnpm i -g diff-so-fancy\ngit config --global core.pager \"diff-so-fancy | less --tabs=4 -RFX\"\n```\n\nYou can also diff with this bash function `dif() { git diff --color --no-index \"$1\" \"$2\" | diff-so-fancy; }` or with VSCode `code --diff file1.js file2.js`\n\n\n- Log in to:\n    - Twitter\n    - Github\n        - [add ssh keys](https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/)\n    - Gmail\n\n\n---\n\nOther good \"new laptop setup\" lists:\n\n- https://github.com/minamarkham/formation"
  },
  {
    "slug": "networking-essentials-content-distribution-jag",
    "data": {
      "title": "Networking Essentials: Content Distribution",
      "description": "How CDNs, BitTorrent and Distributed Hash tables work",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the eighth in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\nIn [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol), your browser makes a request to a server over a network, and receives a response with content. The request is layered over a byte stream protocol, usually [TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol). The server doesn't retain any information about the client, so it is \"stateless\" (there are common ways to get around this).\n\n## HTTP Requests\n\n*Note that this is true for HTTP/1; HTTP/2 is on the rise and is not covered here*\n\nThe content of a HTTP Request includes\n\n- Request line\n    - the **method** of a request: GET, HEAD, POST, PUT, DELETE\n    - the requested URL endpoint e.g. \"/index.html\"\n    - the HTTP version we are using e.g. 1.1\n- Headers\n    - the **referrer** - what caused the page to be requested\n    - the **User-Agent** - the client software used eg Chrome or Firefox\n\n\n## HTTP Response\n\n*Note that this is true for HTTP/1; HTTP/2 is on the rise and is not covered here*\n\nThe content of a HTTP Response includes\n\n- Status line\n    - the HTTP version we are using e.g. 1.1\n    - the response code eg `100, 200 OK, 300, 301, 404, 500`\n- Other Headers\n    - Location (for redirecting)\n    - Server (for server software)\n    - Allow (for HTTP methods allowed)\n    - Content-Encoding (eg is it compressed)\n    - Content-Length (in bytes)\n    - Expires (for caching)\n    - Last-modified\n\n## HTTP/2\n\nHTTP/2 doesn't use newline separated requests and responses, and instead splits data into smaller messages and frames. It also allows new features like pseudo-header fields (`:path`, `:authority`, etc) and ALPN protocol identifiers. It is out of scope for this article but [Google Web Fundamentals](https://developers.google.com/web/fundamentals/performance/http2/) and [HPBN](https://hpbn.co/http2/) have good resources alongside [Wikipedia](https://en.wikipedia.org/wiki/HTTP/2).\n\n## Caching\n\nTo improve web performance, we cache things so that we don't have to make so many hops through to the origin server every time we need something we fetch very often. We can cache things in various places:\n\n- in the browser (aka \"locally\")\n- in the network (your ISP, or CDNs)\n\nCaching in the network (say by your ISP) can in particular benefit if multiple clients (aka users) all request the same thing, which makes loading faster and also saves money for the ISP because it doesnt have to route through other expensive links to reach the origin. \n\nCaches periodically expire content based on the expire setter and check back with the origin. If nothing has been modified, the origin server sends back a `304 Not Modified` response.\n\nThe decision of where to cache is done in two ways:\n\n1. Explicit browser configuration to point to a local cache\n2. Server directed - the origin server might point direct you to a cache. This is done with a special reply to a DNS request\n\n## (Web) CDNs\n\n**What is A CDN?\n\n- CDNs are an overlay network of web caches designed to deliver content to the client from the optimal (usually geographically closest) location.\n- Distinct, geographically disparate servers close to users.\n\n![https://mjau-mjau.com/content/2.blog/6.cloudflare-page-caching/network-map.png](https://mjau-mjau.com/content/2.blog/6.cloudflare-page-caching/network-map.png)\n\nCDNs can be owned by large content providers like Google, or independent Networks (like Akamai and Netlify) and ISPs (like AT&T or Level 3). To give idea of the scale of these networks, Google has about 30,000 frontend cache nodes, while Akamai has 85,000 unique caching servers in 1000 unique networks around the world in 72 countries.\n\n**Challenges in running a CDN**\n\nThe goal is to replicate content on many servers, but unaddressed are:\n\n- How to replicate the content?\n- Where to replicate it?\n- How clients should find the replicated content?\n- How clients should choose server replica? (aka the \"server selection problem\")\n- How to direct clients once the replica has been selected? (aka the \"Content Routing\" problem)\n\n**How server selection works in a CDN**\n\nWhich server to direct client to?\n\n- Lowest load?\n- Lowest latency? <- this is often the winner, since latency is important to web performance\n- any random \"alive\" server?\n\n**Content Routing aka How clients get redirected to a server**\n\n1. Routing-based redirection (eg [anycast](https://en.wikipedia.org/wiki/Anycast)). This is the simplest but it gives service providers very little control.\n2. Application-based routing (eg HTTP redirect). This is effective, but requires that the first request go to the origin server so it doesnt really help reduce first load latency\n3. **Naming-based routing** (eg DNS). This is the most common approach as the client looks up a domain name and the response is the address of a nearby cache. This provides significant flexibility in directing different clients to different server replicas. It is fast and provides fine-grained control.\n\n## Naming-based redirection\n\nWhen you look up `symantec.com` from New York, you might get a `CNAME` (Canonical Name) directing you to a nameserver like `a568.d.akamai.net`. The `A` response from `a568.d.akamai.net` will then direct you to an IP address like `207.40.194.46`.\n\nWhen you do the same from a different city, the nameserver will respond with a different IP address like `81.23.243.152`, which is closer to you in that city. This is how naming based redirection works.\n\n## CDN and ISP peering\n\nCDNs and ISPs are very symbiotic. CDNs like ISPs because: \n\n- CDNs peering with ISPs provides better throughput since there are no intermediate AS hops.\n- Having more vectors to route content (redundancy) increases reliability\n- Having direct connectivity to multiple networks where the content is hosted allows the ISP to spread the content across multiple transit links, which helps deal with bursty traffic, while reducing the 95th percentile load and thus its traffic costs.\n\nISPs like CDNs because:\n\n- they improve performance for customers\n- they lower transit costs\n\n## Bit Torrent\n\nBitTorrent is a peer-to-peer CDN for file sharing and specifically large files.\n\nOrigin servers have problems with large files because everyone requesting from that file means congestion or overload for the origin server. So the solution is to chop up the file and replicate it at all peers, so you fetch the content from other peers instead of a single server.\n\nThe publishing process is:\n\n- A peer creates a \"torrent\", which has an assigned tracker and lists all the chunks of a file (along with their checksum)\n- \"Seeders\" create the initial copy of the file\n- To download, a client contacts the tracker which has a list of all the \"seeders\". It also returns a random list of \"leechers\", which are clients that contain incomplete copies of the file\n- The client starts to download chunks of the file from the seeder\n- These parts may be different from those downloaded by other clients.\n- The clients can now begin to swap chunks with one another.\n\nThe problem with this model is **freeloading**, where a client leaves the network as soon as it finishes downloading a file, not providing a benefit to others who also want the file.\n\nThe solution to freeloading is **choking** - the temporary refusal to upload chunks to another peer that is requesting them. This seems a counterintuitive restriction, but combined with the rule that if a peer can't download from a client then it won't upload. This eliminates the incentives to freeload.\n\n## Chunk Swapping\n\nThe algorithm for swapping chunks is important for Bit Torrent. It uses a \"rarest piece first\" strategy, leaving the most common pieces to download at the end. However, by definition, rare pieces may be available at the fewest clients, so that may slow download times. Clients may opt to start off with \"random piece first\" strategy from seeders.\n\nBy the end, you just want to request missing pieces from all peers and cancel redundant requests when the piece arrives.\n\n## Distributed Hash Tables\n\nDHT's enable **structured content overlay**. A common type of DHT is [Chord](https://en.wikipedia.org/wiki/Chord_(peer-to-peer)), which uses **consistent hashing** to establish an intuitive, scalable, distributed key-value \"lookup service\" (e.g. DNS or directories) with provable correctness and good performance.\n\n**Consistent Hashing**\n\nThe main idea here is that **Keys** and **Nodes** map to the same **ID Space**.\n\n![http://blog.plasmaconduit.com/content/images/2014/Sep/consistent_hashing_003.jpg](http://blog.plasmaconduit.com/content/images/2014/Sep/consistent_hashing_003.jpg)\n\nA hash function like SHA1 is used to assign identifiers to both Keys (eg hashing the key) and Nodes (eg hashing the IP address). Once this is done we map the key IDs to the Node IDs so we know which is responsible for the lookups for a particular key. **Chord**'s idea is that **a key is stored at its successor**, aka the node with the next highest ID. (if the ID is at the highest number, it wraps around to the first node, hence the ring format)\n\nThis system offers load balancing because nodes receive roughly the same number of keys (given a uniformly distributed hash algorithm like SHA1).\n\nThis system also offers flexibility because when a node enters or leaves a network, only some keys need be remapped.\n\nThe final part of the system is that nodes need to be able to find other nodes. You could make every node know every other node, but that would require `O(N)` storage. You could make every node only know their success, which makes for `O(N)` lookup. The happy midddle is **Finger Tables**, where every node knows `m` other nodes, with exponentially increasing distance. For example, Node 10 will maintain address for:\n\n- 10 + 2^0: Node 11\n- 10 + 2^1: Node 12\n- 10 + 2^2: Node 14\n- etc\n\nThis reduces both storage and lookup to scale at `O(log N)`.\n\n## Next in our series\n\nHopefully this has been a good high level overview of how CDNs, BitTorrent and Distributed Hash tables work. I am planning more primers and would love your feedback and questions on:\n\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "networking-essentials-rate-limiting-and-traffic-shaping-43ii",
    "data": {
      "title": "Networking Essentials: Rate Limiting and Traffic Shaping",
      "description": "How Computer Networks shape, police, measure, and limit the traffic that flow across them.",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the seventh in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\nNow that we have explored how [TCP Congestion Control](https://dev.to/swyx/networking-essentials-congestion-control-26n2) works, we might think we understand how Internet traffic is formed. But Congestion Control is very low level - there are still high level limits we put on Internet traffic to address resource or business constraints. We call this Rate Limiting and Traffic Shaping.\n\n## Ways to classify Traffic\n\nAll traffic is not created equal. Transferring Data is often bursty in terms of traffic needs. Transferring Audio is usually continuous as you are streaming down a fairly constant set of data. Transferring Video be can continuous or bursty due to compression.\n\nThe high-level way to classify traffic comes down to:\n\n- **Constant Bit Rate (CBR)**: traffic arrives at regular intervals, and packets are roughly the same size, which leads to a constant bitrate (e.g. audio). We shape CBR traffic according to their peak load.\n- **Variable Bit Rate (VBR)**: all variable (e.g. video and data). We shape VBR traffic according to both their peak and their average load.\n\n![https://www.securitycameraking.com/securityinfo/wp-content/uploads/2015/08/Bit-Rates-e1438874598220.jpg](https://www.securitycameraking.com/securityinfo/wp-content/uploads/2015/08/Bit-Rates-e1438874598220.jpg)\n\n## Traffic Shaping Approaches\n\nThere are three we will cover:\n\n- Leaky Bucket\n- (r, t) shaping\n- Token Bucket\n\n**Leaky Bucket Traffic Shaping**\n\n![https://cdncontribute.geeksforgeeks.org/wp-content/uploads/leakyTap-1.png](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/leakyTap-1.png)\n\nA Leaky Bucket turns a bursty flow in to a regular flow. The two relevant parameters are **bucket size** and **drain rate**. The drain or \"leak\" acts as the \"regulator\" on the system. You want to set the **drain rate** at the **average** rate of the incoming flow, and the **bucket size** according to the **max** size of burst you expect. Overflows of the bucket are discarded or placed at a lower priority.\n\nLeaky Bucket was developed in 1986 and you can [read further here](https://en.wikipedia.org/wiki/Leaky_bucket).\n\n**(R, T) Traffic Shaping**\n\nThis variant deals with **constant flows** best. It works like:\n\n1. Divide traffic into `T`-bit frames\n2. Inject up to `R` bits in any `T`-bit frame \n\nIf a sender wants to send more than `R` bits, it has to wait until the next `T` bit frame. A flow that follows this rule is called an \"(R,T) smooth traffic shape\".\n\nSo the max packet size is `R * T` which usually isn't very large, and the range of behaviors is limited to fixed rate flows (CBR, above). Variable flows (VBR) have to request data rates (`R*T`) that are equal to the peak rate which is very wasteful.\n\nIf a flow exceeds the R,T rate, the excess packets are assigned a lower priority, or dropped. Priorities are assigned by either the sender (preferred, since it knows its own priorities best) or the network (this is known as [policing](https://en.wikipedia.org/wiki/Traffic_policing_(communications)).\n\n**Token Bucket Traffic Shaping**\n\nIn a Token Bucket, **tokens** arrive at a rate `R`, and the bucket can take `B` tokens. These two parameters determine the Token Bucket regulator.\n\nThe bursty flow has two parameters `Lpeak` and `Lavg` to describe it. It flows into the regulator similar to how it flows into the Leaky Bucket regulator.\n\nThe difference here is, traffic in a Token Bucket can be sent by the regulator as long as there is a token for it. So the Token Bucket kind of works like the opposite of the Leaky Bucket. If a packet of size `b` comes along:\n\n- If the bucket is **full**, the packet is sent and `b` tokens are removed.\n- If the bucket is **empty**, the packet must *wait* until `b` tokens drip into the bucket.\n- If it is partially full, it also waits until the number of tokens in the bucket rises to exceed `b`.\n\n![https://gateoverflow.in/?qa=blob&qa_blobid=14382465908978628560](https://gateoverflow.in/?qa=blob&qa_blobid=14382465908978628560)\n\nToken Buckets permit **variable/bursty flows** by bounding it rather than always smoothing it like with the Leaky Bucket. They have no discard or priority policies whereas Leaky Buckets do. Because they can end up \"monopolizing\" a network, there is a need to police Token Buckets by combining a Token Bucket and a Leaky Bucket into a **Composite Shaper**.\n\n**An interesting technique: Power Boost**\n\nFirst implemented by [Comcast in 2006](http://www.dslreports.com/faq/14520), Power Boost allowed users to send at a higher rate for a brief time. This takes advantaged of spare capacity for users who do not put sustained load on a network. It can be \"capped\" or \"uncapped\". The amount of the boost is measurable by the excess speed over the rate that the user has subscribed for, and can be regulated by a Token Bucket. A cap is implemented by a second Token Bucket on the amount of the boost.\n\nPower Boosting can have an impact on latency if inbound traffic exceeds router capabilities and buffers start filling up, so implementing a traffic shaper in front of a power boost may help ameliorate this issue. [More in this SIGCOMM11 paper](https://conferences.sigcomm.org/sigcomm/2011/papers/sigcomm/p134.pdf).\n\nThe initiative was [ended in 2013](https://www.lightreading.com/broadband/fttx/comcast-to-shelve-powerboost/d/d-id/700963).\n\n**An interesting problem: Buffer Bloat**\n\nLarge buffers are actually a problem in home routers, hosts, switches and access points, especially when combined with Power Boosting. A period of high inflow (potentially mixing data with other users) exceeding the drain rate of the buffer can cause data to be stalled in the buffer, introducing unnecessary latency that can be a problem for time sensitive applications like voice and video.\n\n![https://wiki.untangle.com/images/b/b3/Bufferbloat_diagram_2.png](https://wiki.untangle.com/images/b/b3/Bufferbloat_diagram_2.png)\n\nIt is impractical to try to reduce buffer size everywhere, so the solution to buffer bloat is to use Traffic Shaping methods like the ones we learned above to ensure that the buffer never fills.\n\n## Network Measurement\n\nHow do you \"see\" what traffic is being sent on the network?\n\n- **Passive**: collection of packets and flow statistics that are already on the network e.g. packet traces\n- **Active**: inject additional traffic to measure various characteristics e.g. `ping` and `traceroute`\n\nMeasuring traffic is important for billing your users via traffic. In practice, companies charge based on the 95th percentile rate (measured every 5 minutes) to allow for some burstiness, also known as the [Committed Information Rate](https://en.wikipedia.org/wiki/Committed_information_rate).\n\nMeasuring traffic is also helpful for security: detecting compromised hosts, botnets, and DDoS attacks.\n\n## Passive Measurement\n\n**SNMP**\n\nMost networks have the [Simple Network Management Protocol](https://en.wikipedia.org/wiki/Simple_Network_Management_Protocol) built in which provides a [Management Info Base](https://en.wikipedia.org/wiki/Management_information_base) of data you can analyze. By periodically polling the MIB from a device for the byte and packet counts it sends, it can determine the rate it is sending data. But because the data is so coarse, you can't do more sophisticated queries than that.\n\n**Packet Monitoring**\n\nThis involves inspecting packet headers and contents that pass through your machine with tools like `tcpdump`, `ethereal`, and `wireshark`, or hardware devices mounted alongside servers in the network. This provides a LOT of detail including timing and destination information, however that much detail is also high overhead, sometimes requiring a dedicated monitoring card.\n\n**Flow Monitoring**\n\nA monitor (possibly running on the router itself) records statistics per flow. A flow is defined by:\n\n- Common source and destination IP\n- Common source and destination port\n- Common protocol type\n- Common [TOS byte](https://en.wikipedia.org/wiki/Type_of_service)\n- Common interface\n\nThe flow may also have other information like:\n\n- Next-hop IP address\n- source/destination AS or prefix\n\nGrouping of packets into flows based on these identifiers as well as based on time proximity is common.\n\nTo reduce monitoring overhead, **sampling** may also be done, by creating flows only on 1 out of every 10 or 100 packets (possibly randomly sampling).\n\n\n\n## Next in our series\n\nHopefully this has been a good high level overview of how Networks shape, police, measure, and limit the traffic that flow across them. I am planning more primers and would love your feedback and questions on:\n\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "3-tips-from-kent-c-dodds-for-people-just-getting-started-ik8",
    "data": {
      "title": "3 Tips from Kent C Dodds for People Just Getting Started",
      "description": "advice for beginners from a podcast",
      "tag_list": [
        "advice"
      ]
    },
    "content": "\r\nI heard this recently on the [6 Figure Dev podcast](https://6figuredev.com/podcast/episode-061-react-with-kent-c-dodds/) and just had to share notes from the great KCD:\r\n\r\n- **Software is very hard**. If you're struggling, its because it's not easy. Ignore the people who tell you it's easy. It's constantly changing, and it is normal to struggle to ramp up and keep up.\r\n- **Do motivation-based learning**. You don't have to learn ALL the fundamentals first. Learn what you need to get the thing you want to build built. It's the motivation that will push you through when things get hard. Dive in later when you are curious about framework source code and so on.\r\n- **You will learn new things through building - teach them.** Don't worry if something's been said before. The stuff you learn won't be new to the world, but it will be **new to you** and I guarantee you there will be others in the world to whom it will be new too. **Learn by teaching** through a blogpost or a meetup talk. **YOU** can reach people who the current content isn't reaching. For example, non-English-speaking audiences, or more recent beginners (senior engineers are often afflicted by the \"curse of knowledge\", and can't effectively explain things to beginners anymore).\r\n\r\nIf you want more of the story of how he supercharged his own career, I recommend this talk: [Zero to 60 in Software Development: How to Jumpstart Your Career - Forward 4 Web Summit](https://www.youtube.com/watch?v=-qPh6I2hfjw&list=PLV5CVI1eNcJgNqzNwcs4UKrlJdhfDjshf)\r\n\r\nThat's it. Just had to share. Go do it."
  },
  {
    "slug": "networking-essentials-congestion-control-26n2",
    "data": {
      "title": "Networking Essentials: Congestion Control",
      "description": "Bottlenecks inevitably arise in networks. How do we deal with them in TCP? How about in practical streaming applications like Youtube and Skype?",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the sixth in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\n## Congestion Collapse 💩\n\nSay we have a central switch S that is linked to a destination D with a connection capacity of 1 Mbps. However, we have two source hosts H1 and H2 connecting through S trying to send data to D at rates of 10 Mbps and 100 Mbps respectively. \n\nThe exact numbers don't matter, but the point is H1 and H2 are trying to send data at a much higher rate than the S-D link can take it. The S-D link is a bottleneck!\n\nBy [Internet architecture design](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e), the source hosts H1 and H2 are unaware of each other, and of the current state of the S-D link. As a result, they very inefficiently compete for resources on the bottleneck and this can be lead to lost packets and long delays, to the point where the S-D link isn't even used to its full capacity because of all the failures. This is called **congestion collapse**.\n\n![https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMCpIHEbLysGQSMdsWp9qRPhq45l8l2QPu_KkI_Zzyk-o6jD5f](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSMCpIHEbLysGQSMdsWp9qRPhq45l8l2QPu_KkI_Zzyk-o6jD5f)\n\n**Congestion Collapse** is defined where **an increase in load leads to a decrease in useful work**. It can be caused by:\n\n- spurious retransmission (of packets, due to how TCP works when packets are dropped and ACKs aren't received)\n    - solution: better timers and TCP congestion control\n- undelivered packets (packets consume resources but are dropped elsewhere in the network)\n    - apply congestion control to ALL traffic\n\n## Goals of Congestion Control 👼\n\nSo given the realities of different bandwidths in our network, we want to:\n\n- Use network resources **efficiently**\n- Preserve **fair** allocation of resources\n- avoid congestion collapse\n\nYou can chart Fairness and Efficiency between two hosts with a phase plot:\n\n![https://i.ytimg.com/vi/EfeAglStSd4/maxresdefault.jpg](https://i.ytimg.com/vi/EfeAglStSd4/maxresdefault.jpg)\n\nYou can accomplish these goals with **network-assisted congestion control** where routers provide feedback with a bit to indicate congestion (eg the [Explicit Congestion Notification](https://en.wikipedia.org/wiki/Explicit_Congestion_Notification) extension in TCP) but more commonly the congestion control is implicit, or **end-to-end**, where the network does not give feedback and congestion is inferred by looking at packet loss and delays.\n\n## TCP Congestion Control\n\nIn TCP Congestion Control, senders slowly increase their rate until packets are dropped. Packet loss is interpreted as congestion and the rate is slowed down. The rate is increased again periodically to retest the network in case the congestion was temporary (eg due to a maxed out buffer in a router).\n\nThe increase/decrease algorithm can work in two ways.\n\n- **Window-based** algorithms have a set number of packets out at any given time. New packets are sent when ACKs from old packets are received. So if extant packets drop, new packets don't get sent, and the effective send rate decreases, and vice versa. So the **window size** is a natural control. The *increase and decrease* of window size is a different matter: when a packet is successfully sent, the window size \"additively\" increases by one. When a packet fails, the window size is cut \"multiplicatively\" in half. This asymmetric increase/decrease algorithm is known as [Additive Increase, Multiplicative Decrease (AIMD)](https://en.wikipedia.org/wiki/Additive_increase/multiplicative_decrease).\n- **Rate-based** alogithms monitor their loss rate instead, and use a timer to modulate their rate. This is less common.\n\nOn a phase plot, AIMD moves rates up parallel to the X1 = X2 \"fair\" line, and down according to its angle to the origin:\n\n![http://omsnotes.com/6250/images/fig8.14.png](http://omsnotes.com/6250/images/fig8.14.png)\n\nIn terms of individual rate over time, AIMD also results in a \"sawtooth\" plot:\n\n![http://omsnotes.com/6250/images/fig8.15.png](http://omsnotes.com/6250/images/fig8.15.png)\n\nA TCP sender sends at an average rate of 3/4 of it's window size due to AIMD. \n\nTo calculate **throughput**, you take the average rate divided by the RTT, or `3/4 * Wm/RTT`.\n\nAlso because of AIMD, the time between the troughs is given by `Wm/2 + 1` (the `Wm/2` being the time taken to increase the height of half the window 1 at a time, and the `1` being the time it takes to decrease it in half). \n\nTo calculate **the loss rate**, Compare the area under the sawtooth (successful sends) to the total packets sent, i.e. `1/2 * Wm/2 * (Wm/2 + 1)` or basically `Wm^2 / 8`. \n\n**Throughput and LossRate** are related by `Throughput ~ k/(RTT * sqrt(Lossrate))`.\n\n## The TCP Incast Problem\n\nThere is a special congestion problem that specifically happens in the conditions of data centers. **Incast** is a drastic reduction in application throughput that results when servers using TCP all simultaneously request data (sometimes with [Barrier Synchronization](https://docs.oracle.com/cd/E19120-01/open.solaris/816-5137/gfwek/index.html), leading to a gross underutilization of network capacity in many-to-one communication networks like a datacenter. This is due to:\n    - Requirement of high bandwidth and low latency\n    - Many parallel requests coming from servers\n    - High \"fan in\" in a data center\n    - Small buffers in switches\n    - Filled up switch buffers result in bursty retransmissions cause by TCP timeouts that last hundreds of milliseconds\n    - But (as above) the requirement of low latency typically on the order of <1ms.\n\nTwo solutions are:\n\n- Fine grained TCP retransmission timers on the order of microseconds instead of milliseconds\n- Clients acknowledging every other packet instead of every packet, reducing network load\n\nBottom line: Timers should operate on a granularity close to the RTT of the network, and in a Data Center that is <1ms.\n\n## Multimedia and Streaming\n\nThe next half of this piece focuses on dealing with the unique problems of sending multimedia and streaming. Some characteristics of these situations are:\n\n- Large volume of data (eg video with sound)\n- Data volume varies over time (depending on compression algorithm)\n- Low tolerance for delay variation\n- Low tolerance for delay\n\nThe most important of these is delay variation. We can establish a clientside buffer to manage some variation and variance but delay variations will threaten the smooth playback especially if the buffer is starved.\n\nTCP is not a good fit for streaming. It is meant to:\n\n- deliver reliably - but the client may not care about that\n- slow down upon loss - this leads to delay variation\n- 20 byte header for every packet - significant overhead\n\n[UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol) is better:\n\n- no retransmission\n- no sending rate adaptation\n- small header\n\n## Case studies\n\n**Youtube (streaming video)**\n\nAll videos are converted to Flash, and every browser can play it with HTTP/TCP. Even though TCP isn't a good fit for streaming, the creators of Youtube preferred to keep it simple so it can leverage existing HTTP/TCP infrastructure like CDNs.\n\n**Skype/VOIP**\n\nSkype has a central login server but uses P2P to exchange data. It compresses to a fairly low bitrate (67 bytes/pkt, 140 pkts/sec or 40kbps), encrypted both ways. But it is still very susceptible to drops in network quality.\n\n## Quality of Service\n\nWe can use explicit reservations, or marking and policing packet streams as higher priorities.\n\nIf we have a VOIP app and an FTP app both sharing the same bandwidth, we want to make sure the VOIP has a higher priority. We can mark the audio packets as they arrive at the router with a priority queue, and then use scheduling (eg [weighted fair queueing](https://en.wikipedia.org/wiki/Weighted_fair_queueing) where you simply serve the high-pri queue more often than the low-pri queue) \n\n\n## Next in our series\n\nHopefully this has been a good high level overview of the Congestion control and some of its practical implementations/applications. I am planning more primers and would love your feedback and questions on:\n\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "networking-essentials-dns-1dl7",
    "data": {
      "title": "Networking Essentials: DNS",
      "description": "Domain names, you buy them, you sell them, but how do they work?",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the fifth in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\n## The Domain Name System\n\nThe purpose of DNS is to map IP addresses to human-readable names:\n\n- The client wants to looks up a domain name\n- The client's stub resolver takes the name and issues a query. The resolver may have cached the name.\n- If not in cache, query gets sent to Local DNS Resolver (usually configured when your machine gets assigned its IP address, using the [Domain Host Control Protocol](https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol))\n- If the first Local DNS Resolver doesn't respond within a preset timeout, it will try the second.\n- The query is **recursive**, so redirects and referrals spawn further queries and the client only wants the end result.\n- But Local Resolvers work **iteratively**, only answering the specific query in front of them.\n\nSo a query/resolution sequence might go:\n\n- Request: `A \"www.gatech.edu\"` (to root server)\n- Response: `NS \"k.edu.servers.net\" (`NS` records are referrals)\n- Request: `A \"www.gatech.edu\"` (to `.edu` servers)\n- Response: `NS \"dns1.gatech.edu\"\n- Request: `A \"www.gatech.edu\"` (to `gatech.edu` servers)\n- Response: `A 130.207.160.173` (success!)\n\nThis process is rather slow due to all the round trips, so the Local Resolvers keep a cache of all the `A` and `NS` mappings for a particular TTL (Time to Live). You may also want to store more frequently accessed domains like the root or `google.com` for days and weeks, but a local name like `www.gatech.edu` might change more frequently and so deserve a shorter TTL.\n\n## Record Types\n\nWhat are those `A` and `NS` notations above? They are record types indicating level of authority in the response:\n\n- `A` records map Names to IP addresses\n- `NS` (aka referrals) records map Names to authoritative nameservers \n\nIn plain English, if you ask the root server for a specific name, it probably doesn't specifically know the IP, but it will know who knows, and tells you to go ask that \"authoritative nameserver\". And so on down the line until you find the final nameserver that knows the exact IP of the domain you are looking for. This lets the Domain Name System be implemented as a hierarchy.\n\nOther record types:\n\n- `MX` maps Names to Mail Servers\n- `CNAME` sets a Canonical name, or alias, to another domain name that needs to be looked up\n- `PT R` maps IP addresses to domain names (reverse lookup)\n- `AAAA` maps Names to IPv6 addresses\n\n## Try it yourself!\n\nYou can run your own traces in your terminal! try `dig www.gatech.edu`:\n\n```\n; <<>> DiG 9.8.3-P1 <<>> www.gatech.edu\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 40374\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;www.gatech.edu.\t\t\tIN\tA\n\n;; ANSWER SECTION:\nwww.gatech.edu.\t\t59\tIN\tCNAME\ttlweb.gtm.gatech.edu.\ntlweb.gtm.gatech.edu.\t29\tIN\tA\t130.207.160.173\n\n;; Query time: 267 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Wed Sep 26 00:01:01 2018\n;; MSG SIZE  rcvd: 72\n```\n\nThe QUESTION SECTION shows our A record query for `www.gatech.edu`.\n\nThe ANSWER SECTION shows the answer with a CNAME swapping `www.gatech.edu` for `tlweb.gtm.gatech.edu` with a 59 second TTL.\n\nSo we issue another A request for `tlweb.gtm.gatech.edu`, and this time get back `130.207.160.173`.\n\n## Load Balancing example\n\nTry `dig nytimes.com`:\n\n```\n; <<>> DiG 9.8.3-P1 <<>> nytimes.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 23334\n;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;nytimes.com.\t\t\tIN\tA\n\n;; ANSWER SECTION:\nnytimes.com.\t\t319\tIN\tA\t151.101.193.164\nnytimes.com.\t\t319\tIN\tA\t151.101.1.164\nnytimes.com.\t\t319\tIN\tA\t151.101.129.164\nnytimes.com.\t\t319\tIN\tA\t151.101.65.164\n\n;; Query time: 128 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Wed Sep 26 00:03:49 2018\n;; MSG SIZE  rcvd: 93\n```\n\nThe 4 parallel addresses in the ANSWER SECTION are all the same, but if for example `151.101.193.164` gets overloaded the next response will swap that out for one of its other siblings.\n\n## Reverse lookup example\n\nWhat if you tried to look up an IP address?\n\n`dig -x 130.207.7.36`:\n\n```\n; <<>> DiG 9.8.3-P1 <<>> -x 130.207.7.36\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 3657\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n\n;; QUESTION SECTION:\n;36.7.207.130.in-addr.arpa.\tIN\tPTR\n\n;; ANSWER SECTION:\n36.7.207.130.in-addr.arpa. 299\tIN\tPTR\tgranite.cc.gatech.edu.\n\n;; Query time: 449 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Wed Sep 26 00:09:13 2018\n;; MSG SIZE  rcvd: 78\n```\n\nYou get the `PTR` record type pointing you back to the human readable domain. Note the reversed IP octets as the IP address moves from higher to lower parts in the domain name hierarchy.\n\n## Next in our series\n\nHopefully this has been a good high level overview of the Domain Name System and you can ping your own domains to see where records are held. I am planning more primers and would love your feedback and questions on:\n\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "networking-essentials-router-design-ahc",
    "data": {
      "title": "Networking Essentials: Router Design",
      "description": "A small detour into Router Hardware!",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the fifth in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\nLet's make a router!\n\n## Basic Router Architecture\n\nHere is the basic job of a Router:\n\n1. Receive packet\n2. Look at header to determine destination\n3. **Look up** forwarding table to determine output interface\n4. Modify header (e.g. Decrementing TTL - Time to Live field and updating header checksum)\n5. Send packet to output interface\n\nAll this is done through a single **Line Card**, each of which has: a **lookup table**, ability to modify headers, and a buffer for packets coming in and out of the card. A router connects a bunch of **Line Cards** via an **Interconnection Fabric**. \n\nTwo things of note:\n\n- **Why One Table Per Card?** Each card has its own copy of the lookup table. We used to do the alternative which is a central table for the whole router, but it was a bottleneck as each card would have to communicate with the central table across a shared bus.\n- **On the Interconnection Fabric**: We use a crossbar switch (aka switched backplane) to let nonconflicting input-output pairs to send data at the same time instead of a shared bus (which is limited to one input/output at a time). Every input port has a connection to every output port, which enables parallelism but requires some scheduling algorithm to ensure fair use.\n\n## Maximal Matching in Crossbar Switching Algorithm\n\nMaximal matching between N inputs and N outputs means:\n\n- try to allocate a one-to-one input to output as far as possible\n- remaining demands get put in a queue\n- Many crossbars have a \"speedup\" where the interconnect is run 2x as fast as the line cards to allow 2x faster matching. This is common practice, however head of line blocking still can't be solved by speedups.\n\n**Head of line blocking** is where there are a lot of packets destined for the same output ahead of another packet that ought to be sent to a different output (and therefore shouldn't wait for the others to clear). The solution is **Virtual Output Queues** where we establish one queue per output port.\n\n## Scheduling and Fairness\n\nCrossbar switching involves **scheduling** - the two goals here are:\n\n- **Efficiency**: if there is available capacity to send traffic from input to output, it should be used up.\n- **Fairness**: Each queue at each input should be scheduled \"fairly\" - typically an idea called **max-min fairness** which is like Pareto optimality where you can't make something better off without making something else worse off.\n\nMax-min fairness can be achieved by:\n\n- Round Robin Scheduling (although packets may have different sizes, and thus be unfair)\n- Bit by bit scheduling (which can be impractical, how do you break packets up into bits)\n- \"Fair queueing\" (compute \"virtual finishing time\" of each packet and serves the one with the minimal finishing time)\n\n## Next in our series\n\nHopefully this has been a good high level overview of Routing protocols and BGP. I am planning more primers and would love your feedback and questions on:\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- Router Design\n- DNS\n- Congestion Control and Streaming\n- Rate Limiting and Traffic Shaping\n- Content Distribution\n- Software Defined Networks\n- Traffic Engineering\n- Network Security\n"
  },
  {
    "slug": "networking-essentials-naming-addressing-and-forwarding-13kk",
    "data": {
      "title": "Networking Essentials: Naming, Addressing, and Forwarding",
      "description": "How IP Addressing evolved over time, how it works today, and how we can transition to IPv6 in future",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the fourth in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\n## What is IP Addressing?\n\nThe \"Internet Protocol\" v4 address is a 32-bit number in \"dotted quad\" notation, for example `130.207.7.36`. Each of the 4 numbers represent 8 bits, for a total of 32 bits. 8 bits of a binary number goes from 0 to 255 (remember that 2 ^ 8 = 256). \n\nTherefore the total number of IPv4 addresses is 2^32, about 4 billion. Although [we are running out](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e) of them, for our purposes here we will learn how to deal with all these addresses. If we were to store and look up each address individually, this would be expensive and inefficient. \n\nSo we group IP addresses up.\n\n## Pre 1994 Classful Addressing\n\nBefore 1994 we used \"Classful\" addressing, splitting IP addresses into \"routing prefixes\" (indicating class A B or C), \"network ID\" section and a \"host ID\" section:\n\n![https://knowledgeofthings.com/wp-content/uploads/2017/10/ipclassful.png](https://knowledgeofthings.com/wp-content/uploads/2017/10/ipclassful.png)\n\nThere were only about 5000 Routing prefixes in Routing Tables back in 1992, but that accelerated in 1994 and it was clearly going to be unsustainable, and we began to run out of class C addresses, while still reserving entire chunks of IPv4 address space for unused A and B classes. \n![http://media.packetlife.net/media/blog/attachments/702/plot.png](http://media.packetlife.net/media/blog/attachments/702/plot.png)\n\n## Classless Interdomain Routing (CIDR)\n\nThe classful strategy had failed to anticipate that demand would be huge for class C addresses and its inflexibility led us toward a new addressing protocol, CIDR. Instead of having fixed \"network ID\" and \"host ID\" portions of the 32 bits in IPv4, we would have an IP address with a \"mask\", indicating the length of the network ID.\n\nSo `65.14.248.0/22` translates to a binary like this:\n\n```\n---65--- ---14--- ---248-- ----0--\n01000001 00001110 11111000 0000000\n^^^^^^^^ ^^^^^^^^ ^^^^^^\n\n^ first 22 bits are network ID\n```\n\nSo the \"mask\" is a variable length address prefix and independent of the range of IP addresses being used (whereas \"classful addressing\" basically fixed the address prefix lengths). This allows RIRs to allocate more flexibly according to size of the network.\n\nHowever it is possible to have contiguous or overlapping address prefixes, e.g. both a `65.14.248.0/22` and a `65.14.248.0/24`. In this case, `65.14.248.0/24` would be a subset of `65.14.248.0/22` (and more specific). If overlaps like these are found on a routing table, the solution is to forward based on the longest mask (aka address prefix) length. This \"masking\" of more specific addresses allow upstream routers (eg `65.14.248.0/8`) to aggregate (and not announce) more specific prefixes.\n\nCIDR was very successful in slowing the growth of routing tables to a linear rate from 1994 til the early 2000s. But from 2000 onward a new practice arose that made it difficult for upstream providers to aggregate IP prefixes together.\n\n## Multihoming vs IP Prefix Aggregation\n\nMultihoming is when an AS (typically with a `/24` mask) wants to be reachable via two upstream ISPs.\n\nLet's take an example with two ISPs, ISP1 and and ISP2. If ISP1 owns `12.0.0.0/8` and assigns `12.20.249.0/24` to our AS, our AS \"multihomes\" and advertises `12.20.249.0/24` to ISP2. Now both ISP1 and ISP2 want to advertise this prefix to the rest of the Internet. Although ISP1 might want to aggregate this prefix into `12.0.0.0/8`, it can't, because ISP2 is still advertising `12.20.249.0/24`, and being a longer prefix, all the traffic would flow to ISP2. ISP1 is forced *not* to aggregate the AS' prefix into the one it already owns.\n\nSo although CIDR aggregation was solving the growth in routing table size for a while, multihoming caused an explosion again because aggregation became less useful.\n\n## Address Lookups with Longest Prefix Match Algorithm\n\nSo CIDR requires LPM to look up the right prefix in a routing table efficiently, and LPM needs to search the space of all prefix lengths AND all prefixes of a given length. The other constraint is speed - an [OC48](https://en.wikipedia.org/wiki/Optical_Carrier_transmission_rates#OC-48_/_STM-16_/_2.5G_SONET) requires a 160 *nanosecond* lookup, or a max of 4 memory accesses.\n\nBinary Tries are very speed inefficient, while Direct Tries or Exact Match strategies are space inefficient. We compromise by using \"Multi-bit\" or \"Multi-ary\" Tries. A k-ary Trie would then have Depth of `W/K`, Degree of `2^K`, and Stride of `K` bits. Binary Tries are special cases of Multi-ary Tries where `k=1`, i.e. Depth is `W`, Degree is `2`, and Stride is `1` bit. So we can control memory access by moving `k` to solve for the Depth that we want.\n\nFurther optimizations exist, like [leaf-pushing](https://ieeexplore.ieee.org/iel7/7106213/7110105/07110131.pdf), [Luleå](https://en.wikipedia.org/wiki/Lule%C3%A5_algorithm), and [Patricia tries](http://www.mathcs.emory.edu/~cheung/Courses/323/Syllabus/Text/trie02.html). Alternatives to Tries altogether also exist, for example [Content-Addressable Memory](https://en.wikipedia.org/wiki/Content-addressable_memory) lookups, which are basically exact-match.\n\n## Solutions to IPv4 exhaustion: Network Address Translation\n\n**NAT allows multiple networks to reuse the same private IP address space.** Private IP addresses were reserved in 1996 ([RFC 1918](https://tools.ietf.org/html/rfc1918) where you can see the origin of familiar addresses like `192.168.0.0` and `172.16.0.0`). NAT allows networks to reuse portions of internet address space, taking private IP addresses and translating to a single globally visible IP address. The NAT maintains a table mapping its public address and port to the private one that it rewrote.\n\nThis is popular for small/home office networks and VPNs. However, the End to End principle of the [internet architecture](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e) is broken to do this.\n\n## Solutions to IPv4 exhaustion: IPv6 (128-bit addresses)\n\nThe other solution is IPv6. IPv6 not only adds a lot more bits for addresses, but clears up a lot of gunk in the header. Here's a comparison of the required fields:\n\n![https://www.researchgate.net/profile/Muzhir_Al-Ani/publication/269810379/figure/fig1/AS:295073662160901@1447362451826/Comparison-of-IPv4-and-IPv6-headers-structures-15.png](https://www.researchgate.net/profile/Muzhir_Al-Ani/publication/269810379/figure/fig1/AS:295073662160901@1447362451826/Comparison-of-IPv4-and-IPv6-headers-structures-15.png)\n\nThe benefits of IPv6:\n\n- more addresses, of course\n- simpler header\n- easier multihoming\n- built in security with [IPSec/Extension Headers](http://www.ipv6now.com.au/primers/IPv6PacketSecurity.php) - ([some caveats of course](https://www.internetsociety.org/blog/2015/01/ipv6-security-myth-2-ipv6-has-security-designed-in/))\n\nHowever, we have only seen very slow adoption of IPv6 so far, because IPv6 is hard to deploy incrementally. Everything runs on IPv4, with a massive network effect both above and below the narrow waist. Adopters face significant incompatibility issues. So far, Incremental Deployment involves either [Dual Stacking IPv4 and IPv6](https://www.juniper.net/documentation/en_US/junos/topics/concept/ipv6-dual-stack-understanding.html) or [Translation/Tunneling](http://www.ciscopress.com/articles/article.asp?p=2104947) (kind of the same thing but with a separate layer just for [6 to 4 tunneling](https://en.wikipedia.org/wiki/6to4)).\n\n\n## Aside: How IP Address Allocation works\n\nIP addresses are allocated to ISPs by a hierarchy:\n\n- At the top, the global [Internet Assigned Numbers Authority (IANA)](https://www.iana.org/) allocates to: \n- one of five Regional Internet Registries: AfriNIC, APNic, ARIN, LACNIC, or RIPE. Which in turn allocates to:\n- Individual ASes, like your university network\n\nThis worked until 2011 when [IANA gave out the last \"/8\" block](https://www.nro.net/ipv4-free-pool-depleted/). Also fun knowledge, the allocations across registries aren't even:\n\n![https://www.cisco.com/c/dam/en_us/about/ac123/ac147/images/ipj/ipj_8-3/83_ipv4_figure_01_lg.jpg](https://www.cisco.com/c/dam/en_us/about/ac123/ac147/images/ipj/ipj_8-3/83_ipv4_figure_01_lg.jpg)\n\nThis is also a nice CIDR cheatsheet showing classful to classless translations as well as allocations in each address range and exceptions therein: <https://oav.net/mirrors/cidr.html>.\n\n## Next in our series\n\nHopefully this has been a good high level overview of IP addressing works and the problems we face today with IPv4. I am planning more primers and would love your feedback and questions on:\n\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "networking-essentials-routing-5gb7",
    "data": {
      "title": "Networking Essentials: Routing",
      "description": "How the Internet cobbles together thousands of Autonomous Systems with the Border Gateway Protocol",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the third in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\n[As we established in the first article](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e), the Internet is not one thing, it is made up of a decentralized group of networks (from ISPs like Comcast to search servers like Google to  content servers like universities). These are more formally known as Autonomous Systems. When you request content over the Internet, data flows over multiple ASes to get to you. More precisely, there is Intra-domain routing (within your AS) and Inter-domain routing (between ASes). Establishing domains is the function of Switches [as we discussed in the second article](https://dev.to/swyx/networking-essentials-switching-3eba).\n\n## Intra-AS Topology\n\nNodes are known as **Points of Presence** and are near major population centers. Here's the AS known as the Abilene Network:\n\n![https://cs.stanford.edu/people/eroberts/courses/soco/projects/2003-04/internet-2/images/domesticpeer.gif](https://cs.stanford.edu/people/eroberts/courses/soco/projects/2003-04/internet-2/images/domesticpeer.gif)\n\nThere are two types of intra-domain Routing algorithm.\n\n**Distance Vector**\n\nIn a \"pure\" implementation of Distance Vector routing protocols, the cost of routing to various nodes is simply the number of hops between them. Higher level protocols like BGP (explained later) also include business relationships, and are known as **Path Vector** protocols. \n\nBased [on the Bellman-Ford algorithm](https://www.youtube.com/watch?v=iTW2yFYd1Nc0), routers will send copies of their own routing table (\"vectors\") to neighbors, and will compute costs to each destination based on shortest path. Since each shortest path of a neighbor is known, the router simply has to add its own routing table's cost to that of the neighbor through which it is trying to reach.\n\nThis is a very simple model and updates quickly when distance costs are low, but when failures occur (distance costs increase for whatever reason), bad news travels slowly. A sharp increase in costs cause neighbors to ping back and forth with each other increasing their estimate of lowest cost neighbors until they find a globally stable new equilibrium. This is the [count to infinity problem](http://www.ques10.com/p/3796/what-is-count-to-infinity-problem-in-distance-vect/).\n\nThe solution to that is [poison reverse](https://searchnetworking.techtarget.com/definition/poison-reverse). We establish asymmetry in the routing table by introducing an \"Infinity\" cost in links we definitely don't want to revisit, so that useless pingbacks are avoided.\n\nThe **[Routing Information Protocol](https://en.wikipedia.org/wiki/Routing_Information_Protocol)** is an implementation of this. The specific rules are:\n\n- We will set a link to Infinity if it takes 16 or more hops. (intentionally small value) \n- Table refreshes occur every 30 seconds\n- Every round has a timeout of 180 seconds\n- **Split Horizon rule**: When an update occurs, send to all neighbors except for the source of the update\n\nRIP is ultimately just a short circuiting of the Count to Infinity problem, which causes **slow convergence**; to really solve it, we'll have to explore a different paradigm.\n\n**Link State**\n\nThe idea for Link State routing is that each node distributes the **network map** instead of the routing table. Each node computes its own shortest path using [Dijkstra](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm).\n\nMechanically this looks like:\n\n- Add costs of immediate neighbors\n- Flood costs to neighbors\n\nThe two implementations of this are:\n\n- **Open Shortest Paths First (OSPF)**\n- **Int. System-Int. System (IS-IS)** (more popular)\n\nThis is the prevailing routing algorithm today, but one problem with it is scale. The complexity of the protocol is O(n^3).\n\nTo cope with this scale problem, we introduce **hierarchy**. OSPF uses \"areas\" while IS-IS uses \"levels\".\n\nAs an example: OSPF has a backbone area known as **Area 0**, and every other area has a designated \"Area 0 Router\". The Area 0 routers do computation between themselves, and all other routers do computation between their area and their Area 0 Router. Basically this works on like how domains work in the networking topics we've already explored.\n\n## Inter-AS topology\n\nThere are 10's of 1000's of ASes, so they need to tell each other they exist by sending **route advertisements** (or \"announcements\") using the **Border Gateway Protocol** (BGP).\n\nIn **External BGP**, a route advertisement includes a lot of data, but critically:\n\n- **Destination prefix** (eg IP prefix for the university)\n- **Next Hop IP** - the address of the next router that the origin router must send traffic to (typically the first router in the neighboring network - Doable because the border router of that network shares the same subnet as the border router of the origin network eg a [/30 subnet](https://www.petri.com/how-30-and-32-bit-ip-subnet-masks-can-help-with-cisco-networking))\n- **AS Path** - a sequence of AS numbers that describe the route to the destination. The last number on the AS Path is known as the origin AS, which is the origin of the route advertisement.\n\nOnce inside an AS, **Internal BGP (iBGP)** is used to transmit routes *inside* an AS for **external** destinations. This is not to be confused with **Intra-domain routing Protocol (IGP)** which transmit routes *inside* an AS for **internal** destinations. \n\nTo go from an origin inside AS1 to a destination in AS2:\n\n- AS1 learns the route to AS2 with **eBGP**\n- Router inside AS1 learns the route to AS2 with **iBGP**\n- Origin router needs to use **IGP** to reach the **iBGP Next Hop** address inside AS1.\n\n**BGP Route Selection**\n\nChoosing between multiple routes to the same destination:\n\n1. Prefer higher **local preference**\n2. Shortest AS path length (fewer ASes to traverse the better)\n3. Multi-Exit Discriminator (MED) - an AS tells you, of its multiple possible exits to your destination, which is preferred. MED values are not comparable between ASes.\n4. Shortest IGP path - leads to \"hot potato routing\" where you basically want to spend as little time inside your own network as possible\n5. Arbitrary Tiebreak based on stability or age or router with lowest router ID.\n\n**Local Preference** is just a number a sysadmin can assign to a particular route. It's local so it isn't transmitted anywhere, but it allows sysadmins to explicitly state one route should be preferred over others. Useful for configuring primary and backup routes. Although local preferences are local, ASes can attach a **BGP Community** to their advertisements to that other ASes can configure their local prefs to work with them, based on prior agreement.\n\n**Multi-Exit Discriminators** \n\nMED overrides \"Hot Potato Routing\" allowing an AS to explicitly specify that a neighboring AS carry the traffic *over its own network* rather than dumping the traffic at the closest egress and forcing traffic across the neighbor's backbone. This isn't a friendly move, this is a way to prevent an AS freeriding on another AS' backbone, for example when a transit provider peers with a content provider and doesn't want the content provider to get free transit.\n\n## Interdomain Routing Business Models\n\n**It's all about routing money.** There are two types of relationships:\n\n- Customer-Provider: Money flows from customer to provider regardless of traffic flow direction\n- Peering: An AS can exchange traffic with another free of charge (aka \"settlement-free peering\")\n\nSo ASes always prefer to:\n\n1. route traffic through its **customer** because it makes money\n2. route through **peers** because its free\n3. last choice, route through its **provider** because it costs money\n\n**Filtering/export**\n\nGiven that an AS learns a route from its neighbor, to whom should it re-advertise that route?\n\n- A route from a customer should be advertised to everyone, because the AS makes money that way\n- A route from a provider should only advertised to customers so you dont bleed money (eg dont have a route from a provider to another provider, you'd be a transit provider between the two providers and paying them both)\n\nAccording to [Griffin, Shepherd and Wilfong's Safety paper](https://people.eecs.berkeley.edu/~istoica/classes/cs268/05/papers/GSW02.pdf), routing stability is guaranteed from these exact rules; without them, the route can oscillate indefinitely. Imagine 3 ASes with local preferences in a rock-paper-scissors setup (Varadhan Gorindam, Estrin (1996)). Business relationships like regional peering and paid peering can violate these conditions.\n\n**More info on this topic can be found on this primer: [BGP Routing Policies in ISP networks](https://www.cs.princeton.edu/~jrex/papers/policies.pdf)**\n\n\n## Next in our series\n\nHopefully this has been a good high level overview of Routing protocols and BGP. I am planning more primers and would love your feedback and questions on:\n\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "networking-essentials-switching-3eba",
    "data": {
      "title": "Networking Essentials: Switching",
      "description": "Discussing why we need Switches in a network and how they work",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the second in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\nLet's build our own Intranet from scratch. What are the first problems we will run into?\n\nSay we have just two machines, each connected by some network interface (like an Ethernet Adapter).\n\n![https://4.bp.blogspot.com/-kaAW47FyDRU/WP8WmlbyhzI/AAAAAAAAAdQ/Ml_5Upx4diwlLmLxsmu45eDh2B4F88YCgCLcB/s1600/How%2Bto%2BConnect%2BTwo%2BComputers%2Bwithout%2Ba%2BRouter.png](https://4.bp.blogspot.com/-kaAW47FyDRU/WP8WmlbyhzI/AAAAAAAAAdQ/Ml_5Upx4diwlLmLxsmu45eDh2B4F88YCgCLcB/s1600/How%2Bto%2BConnect%2BTwo%2BComputers%2Bwithout%2Ba%2BRouter.png)\n\nManufacturers assign each machine (or **host**) a physical address (also known as a [MAC address](https://en.wikipedia.org/wiki/MAC_address)) so we can tell them apart. We also assign each machine a unique IP address - this is public information but the MAC addresses are not.\n\n[As we previously discussed with packet switching](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e), individual packets of data that we send from one host to the other, also called [datagrams](https://en.wikipedia.org/wiki/Datagram), simply have their destination addresses stamped on them. (More accurately speaking, the host **encapsulates** the datagram in an [Ethernet frame](https://en.wikipedia.org/wiki/Ethernet_frame) which has the destination MAC address on it on it.)\n\nSeems simple enough, but we've already run into our first problem.\n\nRemember we're setting up our Intranet from scratch. The machines are brand new, and have just been hooked up for the first time. **How does a host learn the MAC address of another host??**\n\n## Address Resolution Protocol\n\nThe ARP is pretty much the only way possible to solve this problem given the constraints: \n\n1. For a first time transmission to a new IP address, a host asks everyone in its network for someone who has that specific IP address\n2. The one who matches that IP address responds (in a DM 😎) with it's MAC address\n3. The host starts building a simple table mapping each IP address to a MAC address based on received responses through its lifetime\n4. For future transmissions to that IP address, the host doesn't have to ask anymore, it can just start dispatching datagrams to the stored MAC address right away *to anyone in its network*\n\nPretty neat! You may ask: why so complicated? Why must we have two sets of addresses per machine? There are many practical considerations today but at the most fundamental this is a result of the multilayered network model [we discussed in the previous post](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e) - you are translating from the Link Layer (MAC address) to the Internet Layer (IP address) and they are decoupled for better or worse. \n\nLikewise, the packets of data we send are datagrams at the low level, but what gets sent over the Internet Layer is actually Ethernet **frames** (with the destination addresses attached).\n\n## Hubs: Broadcasting\n\nOk so we have figured out how to send data between two machines. How do you scale this up to, say, 10 machines?\n\nFirst let's consider an idea called a **Hub**. **Hubs** are the simplest form of interconnection which help you broadcast messages. If one host sends a frame to a Hub, the Hub will broadcast that frame to all other Hubs and hosts it is connected to. \n\n![https://geek-university.com/wp-content/images/ccna/how_hub_works.jpg?x13092](https://geek-university.com/wp-content/images/ccna/how_hub_works.jpg?x13092)\n\nThis is known as [flooding](https://en.wikipedia.org/wiki/Flooding_(computer_networking)) and it causes latency and can lead to [collisions](https://en.wikipedia.org/wiki/Collision_domain) in any given port. \n\nIf a hacker wanted to overwhelm the network, all they would have to do is send a bunch of frames to a couple of hubs and the hubs would happily bring down the network on the hacker's behalf. That (and a bunch more good reasons) is why we don't want Hubs to broadcast -everything- and we should try to **isolate traffic** from hubs.\n\n## Switches: Traffic Isolation\n\nSwitches subdivide our network into **segments**. A frame from within a segment that is bound for a destination in the same segment as the origin never gets broadcasted to other segments. Switches can work with Hubs:\n\n![http://www2.ic.uff.br/~michael/kr1999/5-datalink/hubSwitch.jpg](http://www2.ic.uff.br/~michael/kr1999/5-datalink/hubSwitch.jpg)\n\nSo within a segment, a hub is responsible for broadcast, but the **switch**'s job is to ensure that the only data that leaves a segment is data meant for other segments. Switches are smarter than just broadcasting that data as well, so they also build up **switch tables** mapping destination MAC addresses to the switch's output ports. This is called a **[Learning Switch](https://telconotes.wordpress.com/2013/03/09/how-a-switch-works/)**, and when it encounters a new address it doesn't know it floods all its ports and records the address of whoever responds.\n\nHowever the flooding can go out of control when there are loops introduced in the network (multiple connections to a node added for resilience in case any particular connection fails) and the switch floods data it has already flooded before (creating a [cycle](https://en.wikipedia.org/wiki/Cycle_(graph_theory))).\n\n## Spanning Trees: Breaking cycles\n\nWe break cycles in Ethernet networks with the [Spanning Tree Protocol](https://en.wikipedia.org/wiki/Spanning_Tree_Protocol). The basic idea is to have a subset of the network that touches all the nodes of the network, but doesn't have any cycles. We would then make our switches only flood/broadcast only the connections which are on the spanning tree, thus guaranteeing that the switches won't flood themselves.\n\nBuilding a Spanning Tree is a messy recursive algorithm:\n\n- the Switches pick a root for a tree (eg switch with smallest ID, but the actual [election process](http://www.firewall.cx/networking-topics/protocols/spanning-tree-protocol/1054-spanning-tree-protocol-root-bridge-election.html) is wonderfully complicated -  If all switches have the same switch ID, then the switch with lowest MAC address (number will out rule letters) will become the root switch, or you can force a particular switch to be the root switch by creating loopback addresses.)\n- switches pass on their distance to root to other switches they are connected to\n- each switch excludes any link not on the shortest path to root (so every switch only has one outgoing link, but could have multiple inbound links from switches further away from the root)\n\n## Buffer Sizing\n\nBecause we've chosen to use [statistical multiplexing](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e), our Switches also need a buffer to store the multiple packets that may be coming in at the same time. How much do they need?\n\nGiven a round trip delay of 2T and the link's bottleneck capacity is C, the rule of thumb for the buffer size is 2T * C. This just represents the maximum amount of outstanding data that could be enroute between source, router, and destination at any point in time. You can derive this mathematically or just visualize it as the height of the [sawtooth flow](https://en.wikipedia.org/wiki/Sawtooth_wave) but that is beyond scope for us here.\n\n![https://i.ytimg.com/vi/vYRJs-ZRISM/mqdefault.jpg](https://i.ytimg.com/vi/vYRJs-ZRISM/mqdefault.jpg)\n\n**Aside**: The 2T * C max buffer size rule of thumb is unnecessary for real life networks. Backbone switches can have more than 20,000 connections at once, and the TCP flows that come in on them aren't going to be synchronized. This means the min and max of the sawtooth are very likely not a linear multiple of all the connections and the buffer needs will likely converge to a statistical average. So more derivation leads us to conclude that a good buffer assuming TCP flows are non-synchronized is 2T * C / sqrt(N).\n\n![https://ars.els-cdn.com/content/image/1-s2.0-S0140366415002480-gr1.jpg](https://ars.els-cdn.com/content/image/1-s2.0-S0140366415002480-gr1.jpg)\n\n*note the lower sawtooth height in the descynchronized scenario*\n\n## Switches vs Routers\n\nSwitching and Routing are somewhat competing, somewhat parallel concepts. In this post we have explored Switching as a simpler concept for efficiently funneling data through a decentralized network. But next we will explain Routing in contrast to Switching.\n\nSwitches are on Layer 2 of the OSI model, the Ethernet layer. They are **auto-configuring** (as we have seen above, they just figure out their own spanning trees and address resolution), and forwarding tends to be fast. However spanning tree and ARP queries pose a significant load on the network.\n\nRouters and VLANs are on Layer 3 of the OSI model, the IP layer. They are not restricted to spanning trees - [multipath routing](https://en.wikipedia.org/wiki/Multipath_routing) uses multiple paths for increased bandwidth/resilience, and advancements like Software Defined Networking further blur the lines between Layers 2 and 3. Their primary purpose is to break up broadcast domains. We will explore this in our next post.\n\n\n## Next in our series\n\nHopefully this has been a good high level overview of why we need Switches and how they might work if we were to make our own Internet. I am planning more primers and would love your feedback and questions on:\n\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "networking-essentials-architecture-and-principles-2g5e",
    "data": {
      "title": "Networking Essentials: Architecture and Principles",
      "description": "Discussing the architecture of the Internet and its the principles that guided its original design.",
      "tag_list": [
        "networking"
      ]
    },
    "content": "\n*This is the first in a series of class notes as I go through the [free Udacity Computer Networking Basics course](https://www.udacity.com/course/computer-networking--ud436).*\n\n## The Fundamental Goal\n\n*For much of this information our chief reference is David Clark's [The Design Philosophy of the DARPA Internet Protocols](http://ccr.sigcomm.org/archive/1995/jan95/ccr-9501-clark.pdf) paper published in 1988.*\n\nThe fundamental design goal of the Internet was to achieve \"effective multiplexed utilization of existing interconnected networks\". In English, that means we **share** usage of  **connected** networks. \n\n- The goal of **sharing** is solved by **[statistical multiplexing](https://en.wikipedia.org/wiki/Statistical_time-division_multiplexing)** (aka packet switching). In **packet switching**, every packet sent carries it's own destination information, so it can be forwarded along, similar to how a letter sent through the mail has its destination on it. This means the network can deliver the packet on a [best effort](https://en.wikipedia.org/wiki/Best-effort_delivery) basis, which enables sharing because many senders can use the same network at the same time but at the risk of some droppped packages.\nThis is in contrast to **circuit switching**, which is more like a phone network - direct connections are established, transmissions have less chance of losing/dropping packets, but if the available capacity is taken up, you're out of luck.\n\n- The goal of **connecting** networks is solved by the \"[narrow waist](https://www.youtube.com/watch?v=uXumm52oBMo)\". With reference to [the OSI 7 layer systems model](https://en.wikipedia.org/wiki/OSI_model), our **network layer** is very thin - it mostly consists of [the Internet Protocol](https://en.wikipedia.org/wiki/Internet_Protocol)! IP provides guarantees to the **transport layer** sitting above it, usually [TCP](https://en.wikipedia.org/wiki/Transmission_Control_Protocol) or [UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol), and the combination of [TCP/IP](https://en.wikipedia.org/wiki/Internet_protocol_suite) is the most common end to end data transmission stack. On top of this solid foundation we can layer whatever protocols we want to send data, e.g. [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) for websites or [SMTP](https://en.wikipedia.org/wiki/Simple_Mail_Transfer_Protocol) for mail! Below the IP layer we also have the [link layer](https://en.wikipedia.org/wiki/Data_link_layer) and [physical layers](https://en.wikipedia.org/wiki/Physical_layer) with still more protocols like [Ethernet](https://en.wikipedia.org/wiki/Ethernet) and [SONET](https://en.wikipedia.org/wiki/Synchronous_optical_networking). But the point is, the middle network layer is \"narrow\" - it just consists of IP. This means it is fairly easy to get any device on the network, it just has to speak IP!\n\n![the key parts of OSI model](https://images.slideplayer.com/24/7320353/slides/slide_25.jpg)\n\n## Secondary Goals\n\nThe secondary goals all consist of further defining what the word \"effective\" really means in the \"fundamental goal\" listed above:\n\n1. **Survivability**: Internet communication **must continue despite loss**\nof networks or gateways.\n2. **Heterogeneity 1**: The Internet must support **multiple types of\ncommunications service**.\n3. **Heterogeneity 2**: The Internet architecture must accommodate **a variety of networks**.\n4. **Decentralization**: The Internet architecture must **permit distributed management** of its resources.\n5. **Cost**: The Internet architecture must be **cost effective**.\n6. **Ease**: The Internet architecture must permit **host attachment with a low level of effort**.\n7. **Trackability**: The resources used in the internet architecture must\nbe **accountable**.\n\nWe don't have space to discuss the implications of them all here, but will dwell on a few important ones.\n\n## Survivability\n\nWe take this to mean that the network should continue to work even if some devices fail or are compromised.\n\nHow do we achieve this? One way to do it is through **replication**, where we duplicate state in another node so that there is always a backup ready to take over in case of failure. However this trades off against cost, another of our goals.\n\nAnother interesting method is [fate-sharing](https://en.wikipedia.org/wiki/Fate-sharing) - where if the node disconnects, we consider it acceptable to simply lose all information relevant to that node.\n\n## Heterogeneity\n\nThe Internet's \"narrow waist\" design allows multiple protocols to be supported over IP, not just TCP (as previously discussed). This lends itself very well to picking the right protocol for the job, for example picking UDP for the purpose of streaming data since it is more important to be realtime than to be completely lossless. You can even use a [combination of protocols](https://en.wikipedia.org/wiki/Domain_Name_System#DNS_Protocol_transport) to get the best of both speed and losslessness.\n\nThe \"best-effort\" principle of delivery also means data can be delivered over and between any sort of network (including [carrier pigeon](https://www.cnet.com/news/pigeon-powered-internet-takes-flight/)!!) with the tradeoff of difficulty of debugging since no failure data comes back.\n\n## Decentralization\n\nDistributed management of resources can be seen in how [Addressing](http://what-when-how.com/data-communications-and-networking/addressing-data-communications-and-networking/) is achieved - which I will detail in a future primer. We have five [Regional Internet Registries](https://en.wikipedia.org/wiki/Regional_Internet_registry) worldwide - In the US we have [ARIN](https://en.wikipedia.org/wiki/American_Registry_for_Internet_Numbers) and the European equivalent is [RIPE](https://en.wikipedia.org/wiki/R%C3%A9seaux_IP_Europ%C3%A9ens_Network_Coordination_Centre).\n\nWe also have decentralization in naming - [DNS](https://en.wikipedia.org/wiki/Domain_Name_System) delegates the responsibility of assigning domain names and mapping those names to Internet resources by designating authoritative name servers for each domain. Routing through [Border Gateway Protocol (BGP)](https://en.wikipedia.org/wiki/Border_Gateway_Protocol) is decentralized as well between peers and communities. We will also return to both in a future primer.\n\nBecause no single entity is in charge, the Internet has been able to grow organically and is very stable. However the lack of ownership also makes it hard to address problems.\n\n## Problems and Growing Pains\n\n![ipv4 rundown](https://images.theconversation.com/files/83083/original/image-20150527-4812-1iqroxb.png?ixlib=rb-1.1.0&q=45&auto=format&w=1000&fit=clip)\n\nMany of the problems we see on the Internet today are a direct result of the needs of the network outgrowing the design considerations of the 70's and 80's:\n\n- We are [running out of IPv4 addresses](https://www.theregister.co.uk/2017/02/15/no_more_ipv4/) - only 4 billion IP addresses, [inefficiently allocated](https://www.quora.com/How-did-MIT-end-up-with-an-entire-class-A-subnet-of-the-IP-address-space).\n- [Congestion Control](http://ecomputernotes.com/computernetworkingnotes/communication-networks/what-is-congestion-control-describe-the-congestion-control-algorithm-commonly-used) - our networks get very unperformant over flaky connections, especially because retransmissions are triggered. I will cover this in a future primer.\n- [Routing](http://www.enterprisenetworkingplanet.com/netsp/article.php/3615896/Networking-101-Understanding-BGP-Routing.htm) - a topic I will cover in a later primer, but basically it has no security, is easily misconfigured, and exhibits poor convergence and non-determinism. But it sorta works.\n- [Security](https://isc.sans.edu/presentations/first_things_first.html) deserves its own primer as well: methods for encryption and authentication exist, but we haven't done so well at making sure that they are *used*. Also key management and secure software deployment are open problems.\n- [Denial of Service](https://www.digitalattackmap.com/understanding-ddos/) - the Internet maybe does too good of a job delivering packages to the recipient, even if the recipient doesn't wan't them.\n\n## Next in our series\n\nHopefully this has been a good high level overview of why we need Switches and how they might work if we were to make our own Internet. I am planning more primers and would love your feedback and questions on:\n\n\n- [Architecture and Principles](https://dev.to/swyx/networking-essentials-architecture-and-principles-2g5e)\n- [Switching](https://dev.to/swyx/networking-essentials-switching-3eba)\n- [Routing](https://dev.to/swyx/networking-essentials-routing-5gb7/)\n- [Naming/Addressing/Forwarding](https://dev.to/swyx/networking-essentials-naming-addressing-and-forwarding-13kk)\n- [DNS](https://dev.to/swyx/networking-essentials-dns-1dl7)\n- [Congestion Control and Streaming](https://dev.to/swyx/networking-essentials-congestion-control-26n2)\n- [Rate Limiting and Traffic Shaping](https://dev.to/swyx/networking-essentials-rate-limiting-and-traffic-shaping-43ii)\n- [Content Distribution](https://dev.to/swyx/networking-essentials-content-distribution-jag)\n- [Software Defined Networks](https://dev.to/swyx/networking-essentials-software-defined-networking-35n9)\n- [Traffic Engineering](https://dev.to/swyx/networking-essentials-traffic-engineering-13c4)\n- [Network Security](https://dev.to/swyx/networking-essentials-network-security-1fcp)"
  },
  {
    "slug": "a-glance-through-docusaurus-docz-and-react-static-47in",
    "data": {
      "title": "A Glance through Docusaurus, Docz, and React-Static",
      "description": "a survey of new documentation/static site generators",
      "tag_list": [
        "react"
      ]
    },
    "content": "\r\nDocGens/[SSGs](https://www.staticgen.com/) are hard to evaluate because they all look similar on the surface and you have to really invest time before you understand important features and differences between them. I know [Gatsby fairly well](https://gist.github.com/sw-yx/09306ec03df7b4cd8e7469bb74c078fb) and have used Hugo/Jekyll and wanted to check out some of the new generation of React based site generators that have recently come out (yes 2 of the 3 have a specific documentation focus, I don't mind).\r\n\r\n# Docusaurus\r\n\r\n[Docusaurus](https://docusaurus.io/) is very focused on the docs usecase and is used for docs for every major Facebook project -except- React, which uses Gatsby. The install and spin up is very fast, but basic demo doesn't impress at first glance because it literally focuses on Markdown for blog and docs with some components in `/core` and pages in `/pages`. Comparable to a constrained Gatsby. The `siteConfig.js` and `sidebars.json` choices to configure things felt a bit ugly/arbitrary but unimportant. \r\n\r\n**Killer Features**: I think where Docusaurus shines is in [Search](https://docusaurus.io/docs/en/search), [i18n/l20n](https://docusaurus.io/docs/en/translation), and [versioning](https://docusaurus.io/docs/en/versioning). Here it benefits from having a very focused usecase and opinionated choices about partner/problem domain - it is as minimal config as it gets. I think the Versioned docs are a killer feature.\r\n\r\nDocusaurus also ships with some [provided components](https://docusaurus.io/docs/en/api-pages#provided-components) that are helpful in docs, and ships with some inbuilt theming (basically colors that can be set through `siteConfig.js`). Prismjs is also included for zero-setup syntax highlighting. An interesting model that provides a lot of convenience without restricting you if you want to add custom React components or CSS.\r\n\r\nThe deploy story is also a nice touch, PARTICULARLY providing a [publish-gh-pages](https://docusaurus.io/docs/en/publishing#using-github-pages) script, which from experience is a sore pain point. Of course, Netlify is also present. If you `yarn build` and check out the build folder you can even see they include a `sitemap.xml` for you which is super sweet. (The blog also comes with `atom.xml` and `feed.xml` for RSS).\r\n\r\nDocusaurus itself is impressively well documented, which I guess maybe isn't a surprise but I appreciate nonetheless.\r\n\r\n**Cons?**: I honestly struggle to come up with real cons. There is no plugin system so it's not extensible. The config/sidebars is a bit unintuitive, and if you need to use more build processes like SASS you're on your own, but honestly those are just nitpicks. Extremely impressed.\r\n\r\n**Makers**: Docusaurus is made and sponsored by Facebook - it seems Eric Nakagawa and Joel Marcey led the charge and you can check out the rest of the team via [their Twitter](https://twitter.com/docusaurus/following) or [their commits.](https://github.com/facebook/Docusaurus/graphs/contributors).\r\n\r\n---\r\n\r\n# Docz\r\n\r\n[Docz](https://www.docz.site/)'s demo is slick - seriously go watch the video. The value proposition is immediately obvious - you can drop this in to an existing project to generate documentation by colocating [`mdx` files](https://www.docz.site/introduction/writing-mdx) next to your JSX files. \r\n\r\nThis does mean that Docz is more constrained to the React ecosystem than Docusaurus is (though they are working on Preact/Vue support), but again that tradeoff enables the (optional) ability to use MDX, which is very nice. Together with [the supplied components APIs](https://www.docz.site/documentation/components-api), in particular `Playground` and `PropsTable` components (which are great ideas!!) it makes documenting a React component library extremely ridiculously easy. But it doesn't do much else than that for the time being :)\r\n\r\n**Killer Features**: Zero config MDX docs, and the `Playground` and `PropsTable` components with Typescript support.\r\n\r\nThe ability to spin up the docz server just by doing `yarn docz dev` *without even adding in an npm script* is a very very nice touch. I didn't even know you could do that!\r\n\r\nI feel I am not the target audience for Docz because about half the docs on Docz are spent on [Theming](https://www.docz.site/documentation/creating-themes), which I don't particularly care about. It's cool if you need it I guess.\r\n\r\nThe plugin story has a lot of potential, a bunch of well documented lifecycle hooks already exist. Although there aren't many plugins to boast of yet this project is still very young (only announced on Jun 11 2018).\r\n\r\nA very nice touch is the console output - this thing looks designer - very sexy.\r\n\r\n**Cons?**: It really is best suited for INTERNALLY documenting a React (in JS or Typescript) component library. Similar to Storybook it doesn't help you generate a nice looking landing page or blog or anything, it is literally a bunch of MDXes put together. So it's my top choice if I'm doing that, but not anything else. `yarn docz build` also doesn't build a static site, it just makes a production JS bundle to serve with `index.html`. (I didnt know this before i included Docz in the mix, too late now).\r\n\r\nThe nice thing about this extreme focus on generating docs for JSX components is that Docz can actually coexist with other static site gens and you can still get value out of it. So say your Docusaurus site has a reusable component library; you can use Docz to help develop and keep that in check.\r\n\r\n**Makers**: [Pedro Nauck](https://twitter.com/pedronauck?lang=en) who has done a bunch of other interesting things like [react-adopt](https://github.com/pedronauck/react-adopt). Definitely one to keep an eye on.\r\n\r\n---\r\n\r\n# React-Static\r\n\r\n[React-Static](https://react-static.js.org/) is in my mind -the- current Gatsby alternative, so I expect more degrees of freedom and perhaps complexity than the above two. (It is also older by a bit, and already at v6.0.0)\r\n\r\nFirst thing to notice is the [stepped CLI experience](https://react-static.js.org/docs/#quick-start). This is a small touch of course but still a level up from Gatsby. There are a bunch of [super interesting offered templates](https://react-static.js.org/docs/#examples-and-templates) right within the CLI, which is nice. My eye was drawn to the \"animated routes\" one since I know that is a struggle with SSGs.\r\n\r\nAs someone who's contributed to Gatsby's docs, I'll just come right out and say it: React-Static's docs are super well written, particularly with [the introduction of core concepts](https://react-static.js.org/docs/concepts#overview). Dynamic routing is also [easier](https://react-static.js.org/docs/concepts#non-static-routing). [Template generation](https://react-static.js.org/docs/concepts/#pagination) feels somewhat similar to Gatsby's templates inside `gatsby-node.js` but perhaps with fewer files to wrangle. GraphqQL is no longer a first class citizen and I will have to play around with the data fetching to see how I feel about it.\r\n\r\n**Killer feature**: It's hard to articulate this but React-Static is notable for what it -lacks- which are counterintuitively good features. [all data fetching](https://react-static.js.org/docs/concepts#code-data-and-prop-splitting) is done within [`static.config.js`](https://react-static.js.org/docs/config), no magic graphql components, heck no graphql at all. data comes in with the integrated [RouteData](https://react-static.js.org/docs/components#routedata). there aren't a bunch of other files to deal with, and much fewer lifecycle hooks. It doesn't support plugins, so presumably to \"plug in\" you just write something compatible with `static.config.js`. All in all, there is a lot less *magic*, and I never knew how much I appreciated that til now. Who knows if this is the right level of but it certainly feels like the appropriate balance of simplicity and functionality for the 80% of use cases.\r\n\r\nNice touch: [one-line config for Preact](https://react-static.js.org/docs/concepts#using-preact-in-production), [Components](https://react-static.js.org/docs/components) (react router components enhanced with static site concerns including data and scrollto) and [Methods](https://react-static.js.org/docs/methods)\r\n\r\n**Cons?**: The lack of a plugin ecosystem means more custom work has to be done to setup/configure data sources to provide data for page generation. The starters/templates amount to a bunch of boilerplates which isn't very composable or reusable. I guess the tradeoff of having less magic is more work to make up for it.\r\n\r\n**Makers**: Tanner Linsley of [Nozzle.io](https://nozzle.io/about). Origins are important and you should definitely check out Tanner's [Next vs Gatsby article](https://medium.com/@tannerlinsley/%EF%B8%8F-introducing-react-static-a-progressive-static-site-framework-for-react-3470d2a51ebc) to understand why he made React-Static. (much more indepth than my superficial survey - for example he pays attention to the JS shipped per-route, something I definitely didnt look into)\r\n\r\n\r\n---\r\n\r\nSpecial mention: Other noteworthy resources on React static sitegens:\r\n\r\n- [Cuttlebelle](https://cuttlebelle.com/) (bonus points for featuring my favorite animal)\r\n- [static-react](http://jxnblk.com/writing/posts/zero-configuration-react-static-site-generator/) a 2016 exploration into SSG with react (with a [howto](http://jxnblk.com/writing/posts/static-site-generation-with-react-and-webpack/) here)\r\n- [Phenomic](https://phenomic.io/)\r\n- [Nextein](https://nextein.now.sh/) - based on next.js\r\n- [Leo](https://github.com/superawesomelabs/leo)\r\n\r\n---\r\n\r\n# Overall\r\n\r\nI started out this research with only a vague idea of what each do, and I believe it would be irresponsible to pick any one of these over the other. They are apples and oranges and tomatoes, and they all address different problems in unique and interesting ways. The world is wide enough for a diversity of solutions to the wide array of problems, and I welcome these additions to my toolkit. \r\n\r\nI will note that probably the biggest positive surprise to me was Docusaurus, as I had no idea how easy some of these difficult problems in documentation are in Docusaurus."
  },
  {
    "slug": "quick-guide-to-setup-your-react--typescript-storybook-design-system-1c51",
    "data": {
      "title": "Quick Guide to setup your React + Typescript Storybook Design System",
      "description": "a no-bs guide to set up your new React and Typescript Design System powered by Storybook!",
      "tag_list": [
        "react",
        "typescript",
        "storybook"
      ]
    },
    "content": "\r\nDesign systems are all the rage these days - here's how to make your own.\r\n\r\nBecause React is built on a plug and play component philosophy, every company has rushed to build and open source their component libraries, which are both displayed on a hot reloadable Storybook as well as importable as an npm library. [Look at all these companies!!!](https://storybook.js.org/examples/)\r\n\r\n![image](https://user-images.githubusercontent.com/35976578/38384106-4ec86d9a-38dc-11e8-952b-ee542cf14ef5.png)\r\n\r\nBecause companies also care about maintainability, they also like creating Design Systems in Typescript. The prop typing that Typescript enforces helps us autogenerate documentation for our design systems, so it is a win-win!\r\n\r\nToday we are going to walk through how to build and ship a React + Typescript Storybook Design System with handy addons for documentation. The end result looks like this:\r\n\r\n\r\n![image](https://user-images.githubusercontent.com/35976578/38381904-51eb3c6a-38d6-11e8-8ebd-800c29d12258.png)\r\n\r\n# The Short Version\r\n\r\n```bash\r\ngit clone https://github.com/sw-yx/react-typescript-storybook-starter\r\nyarn\r\nnpm run storybook\r\n```\r\n\r\nRead more at [the repo here](https://github.com/sw-yx/react-typescript-storybook-starter).\r\n\r\n# The DIY Version\r\n\r\nReady? Lets go!\r\n\r\nAssuming you are in an empty folder:\r\n\r\n```bash\r\nyarn init -y\r\nyarn add -D @storybook/react @storybook/addon-info @storybook/addon-knobs storybook-addon-jsx @types/react babel-core typescript awesome-typescript-loader react-docgen-typescript-webpack-plugin jest \"@types/jest\" ts-jest \r\nyarn add react react-dom\r\nmkdir .storybook src\r\ntouch .storybook/config.js .storybook/addons.js .storybook/welcomeStory.js utils.js\r\n```\r\n\r\nI have gone for a \"colocated stories\" setup where your story for a component lives next to the component. There is another setup where the stories are in a totally separate stories folder. I find this to be extra hassle when working on a component and its associated Story. So we will set up the rest of this app with colocated stories.\r\n\r\nTo have a runnable storybook, add this npm script to your `package.json`:\r\n\r\n```json\r\n{\r\n  \"scripts\": {\r\n    \"storybook\": \"start-storybook -p 6006 -c .storybook\"\r\n  }\r\n}\r\n```\r\n\r\nThere's no strong reason why we want to run storybook on port 6006, it's just what seems to be common.\r\n\r\nIn `.storybook/config.js`:\r\n\r\n```js\r\nimport { configure } from '@storybook/react';\r\nimport { setAddon, addDecorator } from '@storybook/react';\r\nimport JSXAddon from 'storybook-addon-jsx';\r\nimport { withKnobs, select } from '@storybook/addon-knobs/react';\r\naddDecorator(withKnobs);\r\nsetAddon(JSXAddon);\r\n\r\n// automatically import all files ending in *.stories.js\r\nconst req = require.context('../src', true, /.stories.js$/);\r\nfunction loadStories() {\r\n  require('./welcomeStory');\r\n  req.keys().forEach(filename => req(filename));\r\n}\r\n\r\nconfigure(loadStories, module);\r\n```\r\n\r\nIn `.storybook/addons.js`:\r\n\r\n```js\r\nimport '@storybook/addon-knobs/register';\r\nimport 'storybook-addon-jsx/register';\r\n```\r\n\r\nIn `utils.js`:\r\n\r\n```js\r\nimport { withInfo } from '@storybook/addon-info';\r\nconst wInfoStyle = {\r\n  header: {\r\n    h1: {\r\n      marginRight: '20px',\r\n      fontSize: '25px',\r\n      display: 'inline'\r\n    },\r\n    body: {\r\n      paddingTop: 0,\r\n      paddingBottom: 0\r\n    },\r\n    h2: {\r\n      display: 'inline',\r\n      color: '#999'\r\n    }\r\n  },\r\n  infoBody: {\r\n    backgroundColor: '#eee',\r\n    padding: '0px 5px',\r\n    lineHeight: '2'\r\n  }\r\n};\r\nexport const wInfo = text =>\r\n  withInfo({ inline: true, source: false, styles: wInfoStyle, text: text });\r\n```\r\n\r\nIn `.storybook/welcomeStory.js`:\r\n\r\n```js\r\nimport React from 'react';\r\n\r\nimport { storiesOf } from '@storybook/react';\r\nimport { wInfo } from '../utils';\r\n\r\nstoriesOf('Welcome', module).addWithJSX(\r\n  'to your new Storybook🎊',\r\n  wInfo(`\r\n\r\n\r\n    ### Notes\r\n\r\n    Hello world!:\r\n\r\n    ### Usage\r\n    ~~~js\r\n    <div>This is an example component</div>\r\n    ~~~\r\n\r\n    ### To use this Storybook\r\n\r\n    Explore the panels on the left.\r\n  `)(() => <div>This is an example component</div>)\r\n);\r\n```\r\n\r\nLet's see it work! `npm run storybook`:\r\n\r\n![image](https://user-images.githubusercontent.com/35976578/38380223-18b5a282-38d1-11e8-8c44-8395015435a1.png)\r\n\r\n# Your first Typescript component\r\n\r\nTime to make a Typescript component.\r\n\r\n```bash\r\nmkdir src/Button\r\ntouch src/Button/Button.tsx src/Button/Button.css src/Button/Button.stories.js\r\n```\r\n\r\nIn `src/Button/Button.tsx`:\r\n\r\n```tsx\r\nimport * as React from 'react';\r\nimport './Button.css';\r\nexport interface Props {\r\n  /** this dictates what the button will say  */\r\n  label: string;\r\n  /** this dictates what the button will do  */\r\n  onClick: () => void;\r\n  /**\r\n   * Disables onclick\r\n   *\r\n   * @default false\r\n   **/\r\n  disabled?: boolean;\r\n}\r\nconst noop = () => {}; // tslint:disable-line\r\nexport const Button = (props: Props) => {\r\n  const { label, onClick, disabled = false } = props;\r\n  const disabledclass = disabled ? 'Button_disabled' : '';\r\n  return (\r\n    <div\r\n      className={`Button ${disabledclass}`}\r\n      onClick={!disabled ? onClick : noop}\r\n    >\r\n      <span>{label}</span>\r\n    </div>\r\n  );\r\n};\r\n\r\n```\r\n\r\nIn `src/Button/Button.css`:\r\n\r\n```css\r\n.Button span {\r\n  margin: auto;\r\n  font-size: 16px;\r\n  font-weight: bold;\r\n  text-align: center;\r\n  color: #fff;\r\n  text-transform: uppercase;\r\n}\r\n.Button {\r\n  padding: 0px 20px;\r\n  height: 49px;\r\n  border-radius: 2px;\r\n  border: 2px solid var(--ui-bkgd, #3d5567);\r\n  display: inline-flex;\r\n  background-color: var(--ui-bkgd, #3d5567);\r\n}\r\n\r\n.Button:hover:not(.Button_disabled) {\r\n  cursor: pointer;\r\n}\r\n\r\n.Button_disabled {\r\n  --ui-bkgd: rgba(61, 85, 103, 0.3);\r\n}\r\n```\r\n\r\n\r\nIn `src/Button/Button.stories.js`:\r\n\r\n```js\r\nimport React from 'react';\r\n\r\nimport { storiesOf } from '@storybook/react';\r\nimport { Button } from './Button';\r\nimport { wInfo } from '../../utils';\r\nimport { text, boolean } from '@storybook/addon-knobs/react';\r\n\r\nstoriesOf('Components/Button', module).addWithJSX(\r\n  'basic Button',\r\n  wInfo(`\r\n\r\n  ### Notes\r\n\r\n  This is a button\r\n\r\n  ### Usage\r\n  ~~~js\r\n  <Button\r\n    label={'Enroll'}\r\n    disabled={false}\r\n    onClick={() => alert('hello there')}\r\n  />\r\n  ~~~`\r\n)(() => (\r\n    <Button\r\n      label={text('label', 'Enroll')}\r\n      disabled={boolean('disabled', false)}\r\n      onClick={() => alert('hello there')}\r\n    />\r\n  ))\r\n);\r\n```\r\n\r\nWe also have to make Storybook speak typescript:\r\n\r\n```bash\r\ntouch .storybook/webpack.config.js tsconfig.json\r\n```\r\n\r\nIn `webpack.config.js`:\r\n\r\n```js\r\nconst path = require('path');\r\nconst TSDocgenPlugin = require('react-docgen-typescript-webpack-plugin');\r\nmodule.exports = (baseConfig, env, defaultConfig) => {\r\n  defaultConfig.module.rules.push({\r\n    test: /\\.(ts|tsx)$/,\r\n    loader: require.resolve('awesome-typescript-loader')\r\n  });\r\n  defaultConfig.plugins.push(new TSDocgenPlugin());\r\n  defaultConfig.resolve.extensions.push('.ts', '.tsx');\r\n  return defaultConfig;\r\n};\r\n```\r\n\r\nNote - you may have seen old instructions from `const genDefaultConfig = require('@storybook/react/dist/server/config/defaults/webpack.config.js');` but that is now deprecated. [We are using Full control mode + default instead.](https://storybook.js.org/configurations/custom-webpack-config/#full-control-mode--default)\r\n\r\nIn `tsconfig.json`:\r\n\r\n```json\r\n{\r\n  \"compilerOptions\": {\r\n    \"outDir\": \"build/lib\",\r\n    \"module\": \"commonjs\",\r\n    \"target\": \"es5\",\r\n    \"lib\": [\"es5\", \"es6\", \"es7\", \"es2017\", \"dom\"],\r\n    \"sourceMap\": true,\r\n    \"allowJs\": false,\r\n    \"jsx\": \"react\",\r\n    \"moduleResolution\": \"node\",\r\n    \"rootDir\": \"src\",\r\n    \"baseUrl\": \"src\",\r\n    \"forceConsistentCasingInFileNames\": true,\r\n    \"noImplicitReturns\": true,\r\n    \"noImplicitThis\": true,\r\n    \"noImplicitAny\": true,\r\n    \"strictNullChecks\": true,\r\n    \"suppressImplicitAnyIndexErrors\": true,\r\n    \"noUnusedLocals\": true,\r\n    \"declaration\": true,\r\n    \"allowSyntheticDefaultImports\": true,\r\n    \"experimentalDecorators\": true\r\n  },\r\n  \"include\": [\"src/**/*\"],\r\n  \"exclude\": [\"node_modules\", \"build\", \"scripts\"]\r\n}\r\n```\r\n\r\nOk that should be it. `npm run storybook` again!\r\n\r\nBoom!\r\n\r\n![image](https://user-images.githubusercontent.com/35976578/38381904-51eb3c6a-38d6-11e8-8ebd-800c29d12258.png)\r\n\r\n\r\n# Time to build and ship your (one-Button) Design System\r\n\r\nTypescript is only responsible for your Typescript-to-JS compiled code, but you're also going to want to ship CSS and other assets. So you have to do an extra copy process when you build your storybook:\r\n\r\n```bash\r\nyarn add -D cpx\r\ntouch src/index.tsx\r\necho \"node_modules\" >> .gitignore\r\ngit init # version control is good for you\r\n```\r\n\r\nIn your `package.json`, add:\r\n\r\n```json\r\n{\r\n  \"main\": \"build/lib/index.js\",\r\n  \"types\": \"build/lib/index.d.ts\",\r\n  \"files\": [\r\n    \"build/lib\"\r\n  ],\r\n  \"scripts\": {\r\n    \"storybook\": \"start-storybook -p 6006 -c .storybook\",\r\n    \"build\": \"npm run build-lib && build-storybook\",\r\n    \"build-lib\": \"tsc && npm run copy-css-to-lib\",\r\n    \"build-storybook\": \"build-storybook\",\r\n    \"copy-css-to-lib\": \"cpx \\\"./src/**/*.css\\\" ./build/lib\"\r\n  },\r\n}\r\n```\r\n\r\nNote that you already have a `main` from your init, so overwrite it.\r\n\r\nIn `src/index.tsx`:\r\n\r\n```js\r\nexport {Button} from './Button/Button'\r\n```\r\n\r\nThis is where you re-export all your components in one file so that you can import them all together. [This is known as the Barrel pattern](https://basarat.gitbooks.io/typescript/docs/tips/barrel.html)\r\n\r\nNow when you run `npm run build`, it builds just your Design system in `build` without any of the storybook stuff, AND when you run `npm run build-storybook`, it builds a static page storybook you can host anywhere!\r\n\r\nDid I leave out anything? let me know!\r\n"
  },
  {
    "slug": "how-to-try-react-suspense-in-5-minutes-474c",
    "data": {
      "title": "Try React Suspense In 5 Minutes",
      "description": "a quick guide to setting up an Async React sandbox using Parcel 1.7.0",
      "tag_list": [
        "reactsuspense",
        "react",
        "parcel",
        "parceljs"
      ]
    },
    "content": "\r\n# DISCLAIMER\r\n\r\nI am just messing around with this on my own and have no affiliation with the React Team. I am just doing this to learn in public as well as potentially save other people time.\r\n\r\n# REALLY, DON'T TRY THIS ESPECIALLY IF YOU'RE A BEGINNER AND DEFINITELY DON'T USE THIS IN PRODUCTION\r\n\r\n# THIS IS JUST FOR FUN AND EDIFICATION\r\n\r\n---\r\n\r\n### The Short Version\r\n\r\n```\r\ngit clone https://github.com/sw-yx/tryasyncreact.git\r\ncd tryasyncreact\r\nyarn\r\nnpm start\r\n```\r\n\r\n---\r\n\r\n### Less Short Version\r\n\r\nStart from an empty folder and...\r\n\r\n```bash\r\nyarn init -y\r\nyarn add react@canary react-dom@canary simple-cache-provider async-react-future\r\n```\r\n\r\n`async-react-future` is the high level API library that I have published, it just includes community-sourced implementations of the functions and components demoed by Dan Abramov. You can check the [source](https://github.com/sw-yx/async-react-future) as well as read up on other info at my news-tracking repo, [fresh-async-react](https://github.com/sw-yx/fresh-async-react/).\r\n\r\nInstead of Webpack we're going to also use [ParcelJS v1.7](https://medium.com/@devongovett/parcel-v1-7-0-9aac0c505837) to bundle and run our app so make sure your Parcel version is up to date (`parcel -V`). Parcel 1.7 does some autoinstall magic which we don't want, so we're going to write an npm script to disable that (this flag is undocumented and cost me some time). To save you the hassle, here's the final `package.json` you should have:\r\n\r\n```\r\n{\r\n  \"name\": \"testasyncreact\",\r\n  \"version\": \"1.0.0\",\r\n  \"main\": \"index.js\",\r\n  \"author\": \"sw-yx <myemail@gmail.com>\",\r\n  \"license\": \"MIT\",\r\n  \"scripts\": {\r\n    \"start\": \"parcel index.html --no-cache --no-autoinstall\"\r\n  },\r\n  \"dependencies\": {\r\n    \"async-react-future\": \"0.0.1\",\r\n    \"react\": \"^16.4.0-alpha.0911da3\",\r\n    \"react-dom\": \"^16.4.0-alpha.0911da3\",\r\n    \"simple-cache-provider\": \"^0.3.0\"\r\n  }\r\n}\r\n```\r\n\r\n---\r\n\r\nCoolio. Now we follow the rest of the standard [ParcelJS getting started](https://parceljs.org/getting_started.html) workflow:\r\n\r\n`index.html`:\r\n\r\n```html\r\n<html>\r\n<body>\r\n  <div id=\"root\"></div>\r\n  <script src=\"./index.js\"></script>\r\n</body>\r\n</html>\r\n```\r\n\r\n---\r\n\r\n`index.js`:\r\n\r\n```js\r\nimport React, { Component, unstable_AsyncMode as AsyncMode } from \"react\";\r\nimport ReactDOM from \"react-dom\";\r\nimport { createFetcher, Placeholder } from \"async-react-future\";\r\n\r\nconst fetchdata = delay =>\r\n  new Promise((res, rej) => {\r\n    setTimeout(() => res([1, 2, 3]), delay); // simulate an API fetch\r\n  });\r\nconst dataFetcher = createFetcher(fetchdata);\r\n\r\nfunction DataComponent({ timeForAPItoResolve }) {\r\n  const nums = dataFetcher.read(timeForAPItoResolve);\r\n  return <div>{nums.map(num => <div key={num}>{num}</div>)}</div>;\r\n}\r\nclass App extends Component {\r\n  render() {\r\n    return (\r\n      <AsyncMode>\r\n        <Placeholder delayMs={1000} fallback={<div>Loading....</div>}>\r\n          <div>\r\n            <h1>welcome to the async react future </h1>\r\n            <DataComponent timeForAPItoResolve={2000} />\r\n          </div>\r\n        </Placeholder>\r\n      </AsyncMode>\r\n    );\r\n  }\r\n}\r\n\r\nReactDOM.render(<App />, document.getElementById(\"root\"));\r\n```\r\n\r\nand that's about it! hit `npm start` and check out the snazzy async loading!\r\n\r\n![tryasyncreact](https://user-images.githubusercontent.com/6764957/38276875-710bfb48-3764-11e8-8324-7f51d660080b.gif)\r\n\r\n# Try More Things\r\n\r\n`async-react-future` exports more goodies which were all demoed in the [JSConf Iceland talk](https://www.youtube.com/watch?v=v6iR3Zk4oDY). Here's the full list:\r\n\r\n- `createFetcher`\r\n- `Placeholder`\r\n- `Component.deferSetState`\r\n- `Loading`\r\n\r\nThese are completely untested, use at your own risk. I will try to add usage examples to the README of [async-react-future](https://github.com/sw-yx/async-react-future). You are of course welcome to write your own versions of these, as others have been doing. If you come up with something cool, come [post it to `fresh-async-react`!](https://github.com/sw-yx/fresh-async-react)\r\n\r\nIf you would like to learn more, [check my slides and my React Suspense Cheat Sheet here](https://twitter.com/swyx/status/978880884031066112). Enjoy!"
  },
  {
    "slug": "source-code-for-life-v01-3pal",
    "data": {
      "title": "Source Code for Life v0.1",
      "description": "Compiled Insights for Infinite Learners",
      "tag_list": [
        "life"
      ]
    },
    "content": "\r\nHappy Easter. In this first [12 week year](https://www.amazon.com/dp/B00CU9P31K/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1) of 2018 I summarized 80 popular books, nearly one for every day of the year. \r\n\r\n- I let whim and interest guide my book choice, but they tended to be around the top recommended lists of gurus and entrepreneurs. \r\n- I think learning needs to be complemented by doing, so for the next 12 week year of 2018 I will only be executing the source code I have compiled here.\r\n- Your mileage may vary when reading jumbled thoughts from random self-help gurus.\r\n- I am merely open sourcing what resonated with me. I'd love for you to write your own version of this, ESPECIALLY if you disagree.\r\n\r\nWhat follows is compiled insights for Infinite Learners. Enjoy. You can [watch this Github repo for future updates.](https://github.com/sw-yx/life)\r\n\r\n---\r\n\r\n# Header - Top Level Rules\r\n\r\n> The topics I have split out below consist of: **Productivity**, **Interpersonal Skills**, **Sales & Psychology**, **Tech Leadership**, **Life & Career Philosophy**, and **Mental Models and Systems Thinking**. It is highly artificial to split these interrelated things, so I extracted the most important, common threads among them to this top level Header section.\r\n\r\n**Micro: Tactical rules to run your life**\r\n\r\n- Smile! It makes you, not just people around you, feel better. 😄\r\n- Learning is more important than earning.\r\n- Your brain is not a storage device. Have a system to write stuff down (say, on [dev.to](http://dev.to)?) and make it organizable and searchable. \r\n- Play to your strengths, hire and work with other people based on their strengths.\r\n- Remember the power of storytelling. A good story has more power than a good argument.\r\n- Take pride in artistry in your work, and be generous with the results. People crave the authenticity only you can provide.\r\n- Mastery is achieved by having a craftsman mindset focusing on rare and valuable skills, making small bets with a remarkable end goal, in the Adjacent Possible. (be [So Good They Can't Ignore You](https://twitter.com/swyx/status/980286751657865216))\r\n- Always be on the lookout for your Purpose in life, and when you find it, align everything to it. As a company, rally employees and customers to your Purpose too. When you find yourself in [Flow](https://en.wikipedia.org/wiki/Flow_(psychology)), that's a big hint.\r\n- **Flow is everything**. Seek out jobs where you can exist in a Flow state. Arrange your environment, calendar, and life around optimizing your time spent in Flow. Your best work will be done here, and the world will reward you for it. Ignore the noise that aims to interrupt your Flow. Destroy, eliminate, extinguish, kill with fire the Unimportant things that pretend to be Urgent (like notifications).\r\n- Understand what people want. There are many, many, many theories of what people want, but the three big ones are the study of Incentives, of Motivation, and of Fascination.\r\n\r\n**Macro: Big truths to shape your worldview**\r\n- **[Stoicism](https://en.wikipedia.org/wiki/Stoicism)** will get you through tough times and not mislead you during emotional highs and flashes of anger.\r\n- Technology has profoundly changed so, so many of the rules of our society and economy. \r\n    - The dominant Medium of our era defines our ideas of truth.\r\n    - Companies and experts used to profit from information asymmetry, and now have to find better ways of delivering value. \r\n    - Effective marketing has switched from constantly interrupting you (eg with commercials and banner ads) to having your permission (eg with blogs and newsletters and referral codes). \r\n    - The Information Age valued left-brain skills, but as these get commoditized and automated, the \"Conceptual Age\" may now put right-brain aptitudes like Design, Story, Symphony, Empathy, Play, and Meaning on top.\r\n- Understand the [12 Leverage Points in All Systems](http://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/): Parameters < Buffer size < Structure < Delay < Negative feedback < Positive feedback < Info flow < Rules < Self Organization < Goals < Mindset < Transcendence.\r\n\r\n---\r\n\r\n# Productivity\r\n\r\n- Your brain is not a storage device. Put ideas and tasks in a central source of truth, for example a Checklist or a Trello board. Modern human knowledge is too huge to store, organize, and apply all within our brains.\r\n- Discard things that have served their purpose. Say No more often.\r\n- Don't multitask. Multitasking is not Productivity.\r\n- Your willpower is limited.\r\n- Be proactive, take control. \r\n- Begin with the end in mind: have a Mission, Vision, and specific Goals.\r\n- Put first things first. Urgent != Important; do Urgent & Important things first, then Important Non-urgent things. Avoid Urgent Unimportant things.\r\n- Sharpen the Saw and Recharge. Make sure to balance personal with professional life.\r\n- Make room and Define your working space. Define your work time vs free time. Define your online time vs offline time. Boundaries help you do deep work.\r\n- Do premortems envisioning how your project might fail. Talk to your future self, and use the \"uh-oh\" effect to spur you to a strong finish.\r\n- Use short Checklists to prevent avoidable mistakes, and schedule communication for teams.\r\n\r\n### Books:\r\n\r\n- [Getting Things Done by David Allen](https://twitter.com/swyx/status/947884052194893824)\r\n- [The Life Changing Magic of Tidying Up by Marie Kondo](https://twitter.com/swyx/status/951035829664788480)\r\n- [7 Habits of Highly Effective People by Stephen Covey](https://twitter.com/swyx/status/952795883799654400)\r\n- [The One Thing by Jay Papasan](https://twitter.com/swyx/status/953507150294372352)\r\n- [Deep Work by Cal Newport](https://twitter.com/swyx/status/953888242935443457)\r\n- [When by Dan Pink](https://twitter.com/swyx/status/978089040841977858)\r\n- [Checklist Manifesto by Atul Gawande](https://twitter.com/swyx/status/979603386919260160)\r\n\r\n---\r\n\r\n# Interpersonal Skills\r\n\r\n- Smile!\r\n- Be genuine and sincere.\r\n- Be interested in others and use their names. Make them look good. When they suggest ideas, do a \"yes, and\" instead of rejecting it offhand.\r\n- Admit mistakes immediately. Be buoyant to rejection. \r\n- Think win-win, Try for positive sum.\r\n- Seek first to understand, then to be understood. Empathize, and Listen. Ask, Define, and Frame the problem.\r\n- Synergize; combine the strengths of people through positive teamwork.\r\n- **Difficult Conversations**: Be mindful of what you want and what you want to avoid. Make them feel safe. Focus on respect & shared goals. Contrast criticism with praise. Show you care. Paraphrase to ensure understanding. Get the facts straight. Make sure you have a resolution by reaching consensus or someone to decide.\r\n- **Common Communication mistakes**: Overemphasizing content, being inauthentic, being unprepared, being not self-aware, repeating the status quo.\r\n- **Communication tactics**: Eye contact, Posture, Use Gestures, Vary your voice, Pause, Have a point of view\r\n- **Pitching people**: Shorten your pitch. Involve them in the process. Ask a question. Rhyme. Invoke purpose.\r\n- **Practice the 5 aspects of Emotional Intelligence**: Self-awareness, Self control, Motivation, Empathy, and Influence.\r\n\r\n### Books:\r\n\r\n- [How To Win Friends and Influence People by Dale Carnegie](https://twitter.com/swyx/status/948431173331582978)\r\n- [7 Habits of Highly Effective People by Stephen Covey](https://twitter.com/swyx/status/952795883799654400)\r\n- [Crucial Conversations by Al Switzler et al](https://twitter.com/swyx/status/959945093129166848)\r\n- [Communicate to Influence by Ben Decker](https://twitter.com/swyx/status/977730393825775616)\r\n- [To Sell Is Human by Dan Pink](https://twitter.com/swyx/status/978079226011242496)\r\n- [Search Inside Yourself by Chade Meng Tan](https://twitter.com/swyx/status/980252724418793474)\r\n\r\n---\r\n\r\n# Sales & Psychology\r\n\r\n- We justify our actions, not vice versa.\r\n- 40% of actions are unconscious habit. Habits stick because you crave reward. Redirect your cravings, don't resist them.\r\n- Habits are formed through a Trigger -> Action -> Reward -> Investment cycle. Spot companies doing this to you, and use it responsibly.\r\n- Establish Keystone Habits, which help you gain a series of small wins and have positive spillover effects for your willpower and habit formation.\r\n- Beware of System 1 Unconscious flaws (Priming, Halo Effect, Confirmation bias) and System 2 Cognitive flaws (Heuristics, Risk Assessment, Memory).\r\n- **Your thinking is being influenced by Priming**: Fact recall, Word choice, Credit taking, etc. When making critical decisions, try to take them out of the given context.\r\n- **Things go viral if** they have Social Currency (makes people look good), Triggers (top of mind), Emotional value, Publicity, Practical Value, or a entertaining/educational Story.\r\n- **Ideas are sticky when presented as**: Simple, Unexpected, Concrete (with examples or imagery), Credible (backed up with Experts, Stats, or with your audience), Emotional, and having a Story.\r\n- **There are 6 weapons of Influence**: Reciprocation, Commitment & Consistency, Social Proof, Liking, Authority, and Scarcity.\r\n- **There are 4 ways people respond to Outer and Inner expectations**: Upholders value self-reliance; just tell them what you need. Questioners value purpose; justify what you want to them. Obligers value teamwork; hold them accountable. Rebels value freedom; let them decide what to do.\r\n- **There are 7 triggers of Fascination**: Lust (triggering anticipation of pleasure), Mystique (filling knowledge gaps), Alarm (tending to relevant threats), Prestige (going for social status), Power (desire for control), Vice (forbidden things), Trust (familiar things).\r\n- When shit happens, you can reconstruct your reality in a positive way.\r\n- Snap judgments are good at filtering signal from noise and when primed with intelligence and experience. They are bad when rationalizing gut feel, associating competence with looks, under stress, and when unconscious bias is present.\r\n- **People have 3 levels of motivation**: Basic needs, Extrinsic (eg money, force), Intrinsic. Intrinsic motivation is strongest and is driven by the Flow state, pursuit of meaning, self determination, challenging tasks, and having a voice in decisions. \r\n- **You can also phrase this as the study of incentives**: economic, social, and moral. Beware unintended consequences applying incentives out of context, or replacing a big, unquantifiable incentive (eg moral) with a smaller but more measurable one (eg economic).\r\n- **Flow** is enjoyment of novelty and growth, skillful, immersive, involves challenge and intrinsic reward.\r\n- Gamify work.\r\n- **Do Permission marketing** instead of interruption marketing: Appeal to self-interest, focus your efforts on a qualified audience. Your goal is building a relationship, not making a sale. Teach the benefits of your product, and earn trust. This builds long term loyalty and virality.\r\n\r\n### Books:\r\n\r\n- [Hooked by Nir Eyal](https://twitter.com/swyx/status/948129498473443328)\r\n- [Thinking, Fast and Slow by Daniel Kahneman](https://twitter.com/swyx/status/949695448196902912)\r\n- [Pre-suasion by Robert Cialdini](https://twitter.com/swyx/status/950313239233028096)\r\n- [Contagious by Jonah Berger](https://twitter.com/swyx/status/955492366940389376)\r\n- [Influence by Robert Cialdini](https://twitter.com/swyx/status/957875794268819456)\r\n- [Made to Stick by Chip and Dan Heath](https://twitter.com/swyx/status/959829699496169474)\r\n- [The Four Tendencies by Gretchen Rubin](https://twitter.com/swyx/status/964185050802769921)\r\n- [Why Buddhism Is True by Robert Wright](https://twitter.com/swyx/status/964193985530036224)\r\n- [Blink by Malcolm Gladwell](https://twitter.com/swyx/status/964256017407184896)\r\n- [The Power of Habit by Charles Duhigg](https://twitter.com/swyx/status/965974707144151040)\r\n- [Drive by Dan Pink](https://twitter.com/swyx/status/968692205195087872)\r\n- [Flow by Mihaly Csikszentmihalyi](https://twitter.com/swyx/status/977328433217732608)\r\n- [Permission Marketing by Seth Godin](https://twitter.com/swyx/status/977743034052947968)\r\n- [Freakonomics by Levitt & Dubner](https://twitter.com/swyx/status/979608467232182272)\r\n- [Fascinate by Sally Hogshead](https://twitter.com/swyx/status/980277366579703808)\r\n\r\n---\r\n\r\n# Tech Leadership\r\n\r\n- Hire based on strength, not absence of weakness. Hire only when necessary. Build flexible teams. Always ask if you have the right people in the right place. Fire fast and fair.\r\n- Having the right People is more important than the right Ideas and Processes.\r\n- Know when you need to focus on strategy vs execution, being a peacetime vs wartime CEO.\r\n- Find and Articulate a simple vision. Make sure this message is delivered, and role model it. Communicate it with a KPI or North Star.\r\n- Motivate people. People work harder when needed and united by a goal. Evaluate performance by team results. Give them feedback and the support they need. Encourage healthy competition.\r\n- Assess people's Task Relevant Maturity. If they are very experienced at the task, give them room. If they aren't be more hands on. Reevaluate as time passes. Very similar to parenting. Help people eventually manage themselves.\r\n- **Decisionmaking**: Is this decision worth making? Define who is responsible for the decision, learn from the past, and solicit alternative views.\r\n- Monitor, pair, and trend indicators to run the business and understand the levers. Prioritize goals and identify obstacles. Proactively detect problems.\r\n- Avoid Secrecy. Promote open communications/radical truth in relationships and evaluations; use flowcharts and metrics to clarify how things work.\r\n- Try new things and expect to Fail. Fail early and plan iterations. Series of quick decisions is better than overplanning. Spend \"wasteful\" R&D in order to be first to market. Allow personal project days.\r\n- Build momentum.\r\n- Technology is JUST an accelerator. It won't fix what you don't have. Start from the problem, not the solution.\r\n- Put thought into designing the workplace.\r\n- **Level 5 leaders** are Excellent managers, Modest, Ambitious, do succession planning, Confront facts, and engage in Socratic Debate.\r\n- **How to Cross the Chasm from Visionaries to Pragmatists**: Have a Whole Product, establish a Beach-head, make a Powerful Claim, lock down your Distribution Channels, and price like a Market Leader.\r\n- **7 Questions to evaluate Companies**: Is this 10x better? Is this the right time? Can I get a large market share? Do I have the right people? Can I reach the right customers? Can this survive competition? Is this non-consensus?\r\n- B2B > B2C.\r\n- **Human-centered Design**: Helps users learn to use the product; avoids dangerous errors, and brings users and tech closer together. \r\n- The power of Word of Mouth means you have to **be remarkable or die**. Taking risk is SAFER than avoiding risk. Invent with marketing in mind.\r\n- **Launch earlier, stand for something, and make it inimitable. Launch early, repeatedly, and authentically.**\r\n- Identify the market's central Pivot; Isolate and create demand around it; Innovate and anticipate second-order effects; Test hypotheses; learn from others' past.\r\n- **How to form a Tribe, create a Movement around your cause**: Start by giving generously and authentically, connect to an existing yearning in a new way, tell a story of the future, create channels for horizontal and vertical connection, and give them something to DO.\r\n\r\n### Books:\r\n\r\n- [The Hard Thing about Hard Things by Ben Horowitz](https://twitter.com/swyx/status/949158306127458304)\r\n- [Principles by Ray Dalio](https://twitter.com/swyx/status/952232991455174656)\r\n- [Only The Paranoid Survive by Andy Grove](https://twitter.com/swyx/status/952578803771027456)\r\n- [Zero to One by Peter Thiel](https://twitter.com/swyx/status/953194724399054848)\r\n- [Good to Great by Jim Collins](https://twitter.com/swyx/status/956001706747224064)\r\n- [Creativity Inc by Ed Catmull](https://twitter.com/swyx/status/956008610521235456)\r\n- [Crossing the Chasm by Geoffrey Moore](https://twitter.com/swyx/status/956374350931795968)\r\n- [The Effective Executive by Peter Drucker](https://twitter.com/swyx/status/956728174460588033)\r\n- [The Design of Everyday Things by Don Norman](https://twitter.com/swyx/status/966062356525715456)\r\n- [Purple Cow by Seth Godin](https://twitter.com/swyx/status/971860940839845888)\r\n- [ReWork by Jason Fried](https://twitter.com/swyx/status/972944346432630785)\r\n- [Make by Pieter Levels](https://twitter.com/swyx/status/973246571809771520)\r\n- [Good Strategy, Bad Strategy by Richard Rumelt](https://twitter.com/swyx/status/974358449470562309)\r\n- [Tribes by Seth Godin](https://twitter.com/swyx/status/980208647430115329)\r\n- [High Output Management by Andy Grove](https://twitter.com/swyx/status/980300366595141632)\r\n\r\n---\r\n\r\n# Life & Career Philosophy\r\n\r\n- Avoid the hedonic treadmill.\r\n- Much of Media is noise. Don't listen to noise.\r\n- Life is too short for complaining or lounging.\r\n- When shit happens, you can reconstruct your reality in a positive way.\r\n- Take risks, have clear preferences, build a business. Avoid laziness and arrogance.\r\n- The Days are Long but the Years are Short. The little things you do every day matter more than the big things you try to do once in a while.\r\n- **Being right is an illusion**: choose values you can control, learn from failure, commit to things and narrow your options.\r\n- **Identify and write down your core Principles**: fundamental truths to keep you on goal.\r\n- Build up to big goals with low level goals. Balance deliberate vs emergent strategy.\r\n- Choose interesting work where you can make a difference, have autonomy, and can enter Flow state. Know that The Resistance opposes all our creative endeavors. Treat your dream like a job and work at it.\r\n- Avoid our hidden bias to blame and reward Talent. Effort and Grit is more valuable than Talent; you can go very very far with Deliberate and Intelligent Practice.\r\n- Be in permanent beta and prioritize learning. Learning is more important than Earning. Constant learning. Did I mention learning is good for you? If it comes to it, don't be afraid of career change.\r\n- Know yourself. Know your competitive advantage.\r\n- Own a territory: Grab attention, define a niche, and command loyalty.\r\n- Pursue breakout opportunities doggedly.\r\n- There are 2 levels of Networking you should pursue: Deep Professional Alliances, and broad, weak acquaintances. Both can be valuable. Relationships need constant work but have a long term payoff.\r\n- Satisfaction = Motivation + Discipline\r\n- Invoke muses. Apprentice under a mentor, then outgrow.\r\n- \"Do what you love\" is wrong. Learn to love what you do through achieving Mastery, Autonomy, and deep Relationships.\r\n- Mastery is achieved by having a craftsman mindset focusing on rare and valuable skills, making small bets with a remarkable end goal, in the Adjacent Possible.\r\n- To reach Mastery: Challenge what you have learned. Broaden your mind. Practice skills until they are automatic. Focus on the bigger picture.\r\n- **The dominant Medium of our era defines our ideas of truth.**\r\n- New tech is always underestimated as extending existing tech.\r\n- Play to your strengths; look for strength in others; deal with coworker frustration by involving their strengths.\r\n- Death is a part of life; live life with meaning while facing death.\r\n- A Growth mindset seeks Development, views Failures as Opportunities, and relish Difficulties. Changing your mindset: Changing role models, reaching out for support, talking about mistakes.\r\n- Play Infinite Games: see a series of moments, endless possibilities, no clear winners, treat it as play, and liberated from the past.\r\n- The Information age has become the Conceptual Age: Emphasis on left-brain has given way to \"right directed thinking\", where Design, Story, Symphony, Empathy, Play, and Meaning have gained more importance today.\r\n- Be a Linchpin. Linchpins are indispensable artists who find their own way instead of taking instruction, who put emotional labor into their work and gain a reputation for it, who choose to fight fear and discomfort by repeat exposure instead of retreat.\r\n- We mistake luck for skill (the narrative fallacy) and forget that life is non-linear and path-dependent. Going the last mile often has asymmetric payoff, and also pay attention to overconfidence in risks you don't fully know (Ludic fallacy).\r\n- Eat to Live: >2lbs of greens a day, get 90% of your calories from plants.\r\n\r\n### Books:\r\n\r\n- [Meditations by Marcus Aurelius](https://twitter.com/swyx/status/948844674315366400)\r\n- [Why Buddhism Is True by Robert Wright](https://twitter.com/swyx/status/964193985530036224)\r\n- [The Happiness Project by Gretchen Rubin](https://twitter.com/swyx/status/951514989755555840)\r\n- [The Subtle Art of Not Giving a F*ck by Mark Manson](https://twitter.com/swyx/status/951712433017516032)\r\n- [Principles by Ray Dalio](https://twitter.com/swyx/status/952232991455174656)\r\n- [Grit by Angela Duckworth](https://twitter.com/swyx/status/954751334959501312)\r\n- [The Startup of You by Reid Hoffman](https://twitter.com/swyx/status/954751334959501312)\r\n- [How will you Measure Your Life by Clay Christensen](https://twitter.com/swyx/status/957361180900757505)\r\n- [Mastery by Robert Greene](https://twitter.com/swyx/status/958239930454507520)\r\n- [Amusing Ourselves To Death](https://twitter.com/swyx/status/960271016529408002)\r\n- [Strengthsfinder by Tom Rath](https://twitter.com/swyx/status/961738346761543680)\r\n- [When Breath Becomes Air by Paul Kalanithi](https://twitter.com/swyx/status/964259552370839559)\r\n- [Mindset by Carol Dweck](https://twitter.com/swyx/status/970043624267370496)\r\n- [Finite and Infinite Games by James Carse](https://twitter.com/swyx/status/972936304441921536)\r\n- [How the World Sees You by Sally Hogshead](https://twitter.com/swyx/status/973253321518612480)\r\n- [Eat to Live by Joel Fuhrman](https://twitter.com/swyx/status/974341489710043140)\r\n- [The War of Art by Steve Pressfield](https://twitter.com/swyx/status/976644855899262977)\r\n- [How To Find Fulfilling Work by Roman Krznaric](https://twitter.com/swyx/status/977314695635001346)\r\n- [Rich Dad, Poor Dad by Robert Kiyosaki](https://twitter.com/swyx/status/977725758297772033)\r\n- [A Whole New Mind by Daniel Pink](https://twitter.com/swyx/status/978085452975673344)\r\n- [Linchpin by Seth Godin](https://twitter.com/swyx/status/980217246726443009)\r\n- [Fooled by Randomness by Nassim Taleb](https://twitter.com/swyx/status/980231808624283649)\r\n- [The Black Swan by Nassim Taleb](https://twitter.com/swyx/status/980236314468642816)\r\n- [So Good They Can't Ignore You by Cal Newport](https://twitter.com/swyx/status/980286751657865216)\r\n\r\n---\r\n\r\n# Mental Models and Systems Thinking\r\n\r\n- Antifragile systems benefit from volatility, depend on fragile constituents, and overcompensate to stressors.\r\n- Understand stocks vs flows, Positive vs Negative feedback, System resilience, Self organizing systems, and Hierarchical/Fractal systems.\r\n- Mistakes in systems: Managing by only looking at outcomes, doing linear projection and false isolation, having slow/no feedback.\r\n- Look at the 12 Leverage Points in a system: Parameters < Buffer size < Structure < Delay < -ve feedback < +ve feedback < Info flow < Rules < Self Organization < Goals < Mindset < Transcendence.\r\n- Know the difference between scalable info (where you can draw useful conclusions at different scales) and unscalable info.\r\n- Image matters more in situations where people (bankers, politicians etc) have no skin in the game, while Competence matters more where SITG is fully tied in (doctors, entrepreneurs)\r\n\r\n### Books:\r\n\r\n- [Antifragile by Nassim Taleb](https://twitter.com/swyx/status/949477941091565568)\r\n- [Thinking in Systems by Donella Meadows](https://twitter.com/swyx/status/975486199925178369)\r\n- [12 Leverage Points by Donella Meadows](https://twitter.com/swyx/status/977332913212567552)\r\n- [The Black Swan by Nassim Taleb](https://twitter.com/swyx/status/980236314468642816)\r\n\r\nNote: Not all of the 80 were worth including here, so it doesn't add up to 80.\r\n\r\nThat's it! What rules do you run your life by? Share it below!"
  },
  {
    "slug": "react-suspense-qa-28lc",
    "data": {
      "title": "React Suspense Q&A",
      "description": "a friendly cheat sheet for those wondering what React Suspense is and if they should care",
      "tag_list": [
        "reactsuspense"
      ]
    },
    "content": "\r\n# Why we want React Suspense, in a GIF\r\n\r\n![https://thumbs.gfycat.com/QueasyGrandIriomotecat-size_restricted.gif](https://thumbs.gfycat.com/QueasyGrandIriomotecat-size_restricted.gif)\r\n\r\n# Q&A\r\n\r\nDisclaimer: I do not speak for the React team, this is just what I, a regular React user, have learned over dozens of hours of studying the information available.\r\n\r\n- **Q: What Is React Suspense?**\r\n    - A: [A generic way for components to suspend rendering while they load async data.](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html)\r\n- **Q: What Does That Really Mean?**\r\n    - A: React is getting [2 new primitive](https://twitter.com/acdlite/status/969704631424053249) capabilities that give you a lot more control over loading *anything*, **while** staying interactive. A lot of boilerplatey things (e.g. fetching data without race conditions) will become much simpler to write, and some previously impossible things (e.g. partially rendering components while waiting for data) become easy.\r\n- **Q: I'm fatigued already. Do I really have to learn this?**\r\n    - A: No.\r\n- **Q: Really?**\r\n    - A: Yup. If you don't like any of this you don't have to change the way you do things one bit. [React really values API stability](https://reactjs.org/docs/design-principles.html#stability).\r\n- **Q: Then why should I care?**\r\n    - A: It will probably make your life a lot easier.\r\n    - A: Also its frickin cool'. You like cool things, right?\r\n- **Q: How does it make my life easier?**\r\n    - A: Anytime you need to display something that needs to be loaded, you create a `fetcher` for it and just use the `fetcher` inside your `render` method.\r\n        - If React tries to render something that hasn't been fetched, it stops the render and goes to fetch it. \r\n        - If something was previously fetched before, it is **cached** and renders immediately. \r\n    - If you want a `Placeholder` (eg spinner) to show if a fetch takes too long, thats just a couple lines of code. \r\n    - If you want to avoid cascading spinners as different things are fetched, that's how Suspense works by default. We really don't like cascading spinners.\r\n    - btw by cascading spinners we mean [all unintentional loading states](https://twitter.com/acdlite/status/955160681208168448)\r\n- **Q: Isn't that a job for Redux or a lifecycle method?**\r\n    - A: Not anymore, if you don't want it. No need to use `componentDidMount` or dispatch a `Redux` action just to fetch data. No need to define extra state variables like `isLoading` or `pastDelay` and deal with the state explosion that can come from multiple things loading at various different times. And did I mention that your UI stays interactive throughout this whole thing?\r\n- **Q: Doesn't `react-loadable` already do this?**\r\n    - A: Only kind of. The focus on keeping things interactive while loading (by rendering on a \"separate thread\"... we'll explain later) cannot be done in \"userland\" and requires handling in React's (coming) `AsyncMode`.\r\n- **Q: Cool so this `fetcher` and `Placeholder` are the new parts of React?**\r\n    - A: Nope. That's the \"High Level API\", and you're going to see a bunch of these come out over the next year as people experiment with them. Don't worry about the low level APIs for now, those are unstable.\r\n- **Q: Are you trying to make fetch happen? It's never going to happen.**\r\n    - A: Everyone's already tired of [that joke](http://knowyourmeme.com/memes/stop-trying-to-make-fetch-happen) and its only March. Please be more original. Great movie tho.\r\n- **Q: Why's it called Suspense?**\r\n    - A: That's a reference to how it works internally. When you try to render something that requires data, your `fetcher` **`throw`s a promise** up to React, and React **suspends** the rendering (within [limits](https://twitter.com/dan_abramov/status/971187182621872128)) while the data is being fetched. While the fetch is ongoing, your UI is still interactive, as though the fetching is on a different thread (note: [not technically true](https://twitter.com/aweary/status/913619299087949824)).\r\n- **Q: Is it safe to `throw` things willy nilly like that?**\r\n    - A: It was [supposed to be controversial](https://twitter.com/acdlite/status/969172311067713537), but so far nobody's found a real objection to it. And extensive [tests](https://github.com/acdlite/react/blob/7166ce6d9b7973ddd5e06be9effdfaaeeff57ed6/packages/react-reconciler/src/tests/ReactSuspense-test.js) have been written. You'll see the words \"algebraic effects\" a lot, but for our purposes you can think of them as [resumable exceptions](https://www.youtube.com/watch?v=hrBq8R_kxI0). As a user, you'll probably never actually write this yourself.\r\n- **Q: What's this about a cache? Is React getting a cache?**\r\n    - A: Caching is great and kinda central to the whole \"do I fetch or not\" thing. React provides the ability for users to make caches, and we're gonna see [a whole lot of innovation](https://dev-blog.apollodata.com/a-first-look-at-async-react-apollo-10a82907b48e) there too. The React team has provided a [simple-cache-provider](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L148) which will probably be very popular for basic apps, but they don't handle a lot of nuance, such as fine-grained cache invalidation. You may have heard that's a hard thing.\r\n- **Q: Enough talk. Where do I see some code?**\r\n    - A: First check out Dan's JSConf talk on Time Slicing + Suspense (the official video is [here](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html) but I can't embed it):\r\n\r\n{% youtube v6iR3Zk4oDY %}\r\n\r\n- **Q: More!?**\r\n    - A: Then check the followup where he just talked Suspense:\r\n\r\n{% youtube 6g3g0Q_XVb4 %}\r\n\r\n- **Q: More!??**\r\n    - A: Then have a play of the Movie demo [here](https://github.com/facebook/react/pull/12279)\r\n    - A: Then check out the [implementation pull request](https://github.com/facebook/react/pull/12279), the [tests](https://github.com/acdlite/react/blob/7166ce6d9b7973ddd5e06be9effdfaaeeff57ed6/packages/react-reconciler/src/tests/ReactSuspense-test.js) and the [async react demo](https://gist.github.com/acdlite/f31becd03e2f5feb9b4b22267a58bc1f) (check out the deployed app [here](https://build-mbfootjxoo.now.sh/), that's what made async rendering really click for me)\r\n- **Q: Ok, too long, didn't read. Spell it out for me, what concretely can I use React Suspense for?**\r\n    - A: **deep breath**\r\n    - Data fetching\r\n    - Code Splitting\r\n    - Image loading\r\n    - CSS loading\r\n    - Debouncing\r\n    - Streaming Server Rendering\r\n    - Loading without Race conditions\r\n    - Intentional loading states\r\n    - Eager Loading/Prefetching\r\n    - \"Speculative Rendering\" aka Rendering components before you actually need them\r\n    - \"Transparent Asynchrony\" aka Rendering child components without worrying if they're ready to be rendered\r\n    - ... (we're still figuring out how this works)\r\n    - all with less boilerplate!\r\n    - If that sounds like a lot, its because it is.\r\n- **Q: Wow. How did Dan Abramov do all this?? He's a beast?**\r\n    - A: Dan worked really, really hard on the (super awesome) demo. [The entire React team worked on React Suspense](https://twitter.com/dan_abramov/status/972856536073687040), with feedback from the community.\r\n- **Q: SO COOL! I want to use it in production now!!**\r\n    - A: That is a very bad idea and you should be ashamed of yourself.\r\n- **Q: But when can I have it?**\r\n    - A: Just mentally peg it \"by end 2018\" and you won't be too disappointed.\r\n- **Q: What should I read next?**\r\n    - A: If you want to walk through some code demos, [here's one I did of the (warning once again: Unstable!) low level API.](https://dev.to/swyx/a-walkthrough-of-that-react-suspense-demo--4j6a)\r\n    - Edit: I also made a more in depth [React Suspense Cheat Sheet](https://spectrum.chat/impostor-syndrome?thread=56dc92fb-46a1-49c5-9c42-47d3e90c081e) presentation, check it out!\r\n\r\n"
  },
  {
    "slug": "ocaml-speedrun-3f7g",
    "data": {
      "title": "OCaml Speedrun! 🐫🐪",
      "description": "a guided walk through Jane Street's OCaml workshop",
      "tag_list": [
        "ocaml"
      ]
    },
    "content": "\n[OCaml](https://ocaml.org/) is the basis of [ReasonML](https://reasonml.github.io/) which has been getting a lot of buzz as a leading compile-to-JS language. There was a free OCaml workshop in NYC offered by Jane Street today so I decided to take it and share my notes here. Jane Street has moved billions of dollars for years running an OCaml codebase so they are an extremely credible source of expertise.\n\nFair warning: This article is not like a normal Dev.To article in that this is a guided walkthrough of a workshop - you are expected to code along or this will be completely useless to you. But with these 24 examples (**taking about 2-3 hours**) you should get a solid taste of the key language features in OCaml!\n\nAnd a Disclaimer: I have prior experience working with Haskell so I might have some unconscious knowledge with static typed/functional languages here.\n\n---\n\n- [Installing OCaml on your system](#installing-ocaml-on-your-system)\n- [Basic knowledge](#basic-knowledge)\n- [Going through the workshop](#going-through-the-workshop)\n- [Hello World: `/01-introduction`](#hello-world----01-introduction-)\n- [Basic Data Types: `/02-basic_types`](#basic-data-types----02-basic-types-)\n- [Defining Functions: `/03-define_functions`](#defining-functions----03-define-functions-)\n- [Calling Functions: `/04-call_functions`](#calling-functions----04-call-functions-)\n- [Functions as arguments: `/05-twice`](#functions-as-arguments----05-twice-)\n- [Pattern matching: `/06-pattern-matching`](#pattern-matching----06-pattern-matching-)\n- [Recursion: `/07-simple_recursion`](#recursion----07-simple-recursion-)\n- [Data Type: Linked Lists: `/08-list_intro`](#data-type--linked-lists----08-list-intro-)\n- [Building Lists: `/09-list_range`](#building-lists----09-list-range-)\n- [Recursing through a List: `/10-list_product`](#recursing-through-a-list----10-list-product-)\n- [Abstracting functions: `/11-sum_product`](#abstracting-functions----11-sum-product-)\n- [Float functions: `/12-list_min`](#float-functions----12-list-min-)\n- [Abstractions and Float functions: `/13-largest_smallest`](#abstractions-and-float-functions----13-largest-smallest-)\n- [Data Type: Variant Types aka Enums `/14-variants`](#data-type--variant-types-aka-enums---14-variants-)\n- [Data Type: Tuples and Parameterized Types `/15-tuples`](#data-type--tuples-and-parameterized-types---15-tuples-)\n- [Labelled arguments `/16-labelled_arguments`](#labelled-arguments---16-labelled-arguments-)\n- [Data Type: Options `/17-options`](#data-type--options---17-options-)\n- [Anonymous functions `/18-anonymous_functions`](#anonymous-functions---18-anonymous-functions-)\n- [List operations `/19-list_operations`](#list-operations---19-list-operations-)\n- [Type Definitions! `/20-reading_sigs`](#type-definitions----20-reading-sigs-)\n- [Folding cardio `/21-writing_list_operations`](#folding-cardio---21-writing-list-operations-)\n- [Data Type: Immutable Records `/22-records`](#data-type--immutable-records---22-records-)\n- [Data Type: Mutable Records `/23-mutable_records`](#data-type--mutable-records---23-mutable-records-)\n- [Data Type: refs `/24-refs`](#data-type--refs---24-refs-)\n- [THAT'S ALL FOLKS!](#that-s-all-folks-)\n- [Extra knowledge](#extra-knowledge)\n\n\n---\n# Installing OCaml on your system\n\nFollow this: <https://github.com/janestreet/install-ocaml>. We had no issues following these instructions. I went for the \"reason IDE\" extension in VS Code for my dev environment but vim/emacs seem well supported too. Sublime is not supported.\n\n# Basic knowledge\n\n[opam](https://opam.ocaml.org/) is the package manager of OCaml. If you followed the instructions above you've already used it.\n\nAs part of the above process you also install [utop](https://github.com/diml/utop) which Jane Street recommends as a better \"toplevel\" than the default. A \"toplevel\" is also known as a [REPL](https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop).\n\n[merlin](https://github.com/ocaml/merlin) is what is used under the hood for compiling/syntax highlighting/code completion.\n\nWe are not using \"raw OCaml\" - we are using Jane Street's [Base flavor](https://opensource.janestreet.com/) which overrides OCaml's stdlib with some of their opinions. This is what you will see in the first line of all the problem sets:\n\n```\nopen! Base\n```\n\nModule imports are all done like this. We'll see more of this later.\n\n# Going through the workshop\n\n`git clone https://github.com/janestreet/learn-ocaml-workshop`\n\nand open up `/02-exercises`. We're going to go through all of these!\n\n# Hello World: `/01-introduction`\n\nAs it says in `problem.ml`, just run `jbuilder runtest` and you should see the error:\n\n```bash\n ✝  learn-ocaml-workshop/02-exercises/01-introduction> jbuilder runtest\nEntering directory '/Users/swyx/ocaml/learn-ocaml-workshop'\n         ppx 02-exercises/01-introduction/problem.pp.ml (exit 1)\n(cd _build/default && ./.ppx/ppx_jane/ppx.exe --dump-ast --cookie 'library-name=\"problem_1\"' -o 02-exercises/01-introduction/problem.pp.ml --impl 02-exercises/01-introduction/problem.ml)\nFile \"02-exercises/01-introduction/problem.ml\", line 25, characters 22-23:\nError: String literal not terminated\n```\n\nso if you fix line 25: `let () = Stdio.printf \"Hello, World!\"` by adding that last quote, you get\n\n```bash\n ✝  learn-ocaml-workshop/02-exercises/01-introduction> jbuilder runtest\nEntering directory '/Users/swyx/ocaml/learn-ocaml-workshop'\n         run alias 02-exercises/01-introduction/runtest\nHello, World!\n```\n\nJoy to the world! Notice how a new `.merlin` file is added when you run `jbuilder` - this is the compiler at work.\n\n# Basic Data Types: `/02-basic_types`\n\nHead to `problem.ml` again and give it a read. Your task is to implement the two functions on line 65 and 68:\n\n```\nlet int_average x y = failwith \"For you to implement\"\n(* val float_average : float -> float -> float *)\nlet float_average x y = failwith \"For you to implement\"\n```\n\nIf you run `jbuilder` again you will see errors because these functions are currently implemented with \"failwith\". Time to get implementing!\n\nNotice that the type signatures are commented out. This folder also has a `problem.mli` file. This file declares interfaces for the associated file and happens to have the signatures you need so we don't need to worried about it.\n\n### solution\n\n`int_average` can be solved with: `let int_average x y = (x + y) / 2` which makes sense. but `float_average` needs the float specific operators (this is different from Haskell) or you will get this error:\n\n```bash\nFile \"02-exercises/02-basic_types/problem.ml\", line 163, characters 27-29:\nError: This expression has type float but an expression was expected of type\n         Base__Int.t = int\n```\n\nNotice how if you actually go to line 163 you can see the test that generated that error. You solve this with the float specific operators (mentioned in line 13-15):\n\n`let float_average x y = (x +. y) /. 2.`\n\n# Defining Functions: `/03-define_functions`\n\nThis one is pretty straight forward. I did like the fact that\n\n> In OCaml, outside of strings, whitespace and newlines are the same.\n\n### solution\n\n```\nlet plus x y   = x + y\n\nlet times x y  = x * y\n\nlet minus x y  = x - y\n\nlet divide x y = x / y\n```\n\n# Calling Functions: `/04-call_functions`\n\nAverage of two numbers is adding and then halving.\n\n### solution\n\n```\nlet average x y = half (add x y)\n```\n\nPlaying around with the multiline syntax and implicit return , this also works:\n\n```\nlet average x y =\n  let res = add x y in\n  half res\n```\n\nand so does this:\n\n```\nlet average x y =\n  let res = add\n    x\n    y in\n  half\n    res\n```\n\n# Functions as arguments: `/05-twice`\n\nFunctions as a first class citizen are a pretty well accepted concept everywhere now, I feel like.\n\n### solution\n\n```\nlet add1 x = x + 1\nlet square x = x * x\nlet twice f x = f (f x)\nlet add2 = twice add1\nlet raise_to_the_fourth = twice square\n```\n\n# Pattern matching: `/06-pattern-matching`\n\nAnother familiar pattern from [Haskell](https://www.haskell.org/tutorial/patterns.html) and is being proposed in [Javascript](https://github.com/tc39/proposal-pattern-matching). But needs a special keyword `match _ with`\n\n### solution\n\n```\nlet non_zero x =\n  match x with\n  | 0 -> false\n  | _ -> true\n```\n\n# Recursion: `/07-simple_recursion`\n\nSee: Recursion. recursive functions need to be declared with `let rec`\n\n### solution\n\n```\nlet rec add_every_number_up_to x =\n  (* make sure we don't call this on negative numbers! *)\n  assert (x >= 0);\n  match x with\n  | 0 -> 0\n  | _ -> x + (add_every_number_up_to (x-1))\n\n(* Let's write a function to multiply every number up to x. Remember: [factorial 0] is 1 *)\nlet rec factorial x =\n  assert (x >= 0);\n  match x with\n  | 0 -> 1\n  | 1 -> 1\n  | _ -> x * (factorial (x-1))\n```\n\n# Data Type: Linked Lists: `/08-list_intro`\n\nThis exercise pairs arrays with pattern matching and recursion. The tricky new bit here is immediately destructuring the list that you are matching, which tripped me up a bit. But the provided `length` example is instructive if you look at it carefully.\n\n### solution\n\n```\nlet rec sum lst =\n  match lst with\n  | [] -> 0\n  | hd :: tl -> hd + (sum(tl))\n```\n\n# Building Lists: `/09-list_range`\n\nthis is another recursive answer again. you want to make the `range` function recursive and then call itself all the way until `from` equals `to_`. i initially tried this:\n\n```\nlet rec range from to_ =\n  match from with\n  | to_ -> []\n  | _ -> (from :: (range (from + 1) to_))\n```\n\nand this didnt work because the matching *assigns* to the `to_` instead of compares with it which is annoying.\n\n### solution\n\n```\nlet rec range from to_ =\n  match from = to_ with\n  | true -> []\n  | false -> (from :: (range (from + 1) to_))\n```\n\nthe single `=` here is an \"infix equality\" operator which works fine for ints, which is why we use it here. but notice how they had to use a \"PPX Rewriter\" (see Extra Knowledge section) to implement the list comparison `[%compare.equal: int list]` in this same problem set.\n\n# Recursing through a List: `/10-list_product`\n\nthis one is pretty much the same as exercise 8 where you did the sum of the list.\n\n### solution\n\n```\nlet rec product xs =\n  match xs with\n  | [] -> 1\n  | a :: b -> a * product(b)\n```\n\n# Abstracting functions: `/11-sum_product`\n\nThis is a pretty tricky one. We are creating a function that creates a function, abstracting repeated behavior. From line 5-36 they walk you through an example of how it is done, and then from 47 onward you are expected to do this for a similar but different pattern of behavior. Good luck and pay attention to the patterns that are being used!\n\n### solution\n\n```\nlet rec every answer combine xs =\n  match xs with\n  | [] -> answer\n  | x :: ys -> combine x (every answer combine ys)\n\n(* Now let's rewrite sum and product in just one line each using every *)\nlet simpler_sum xs     = every 0 plus xs\nlet simpler_product xs = every 1 times xs\n```\n\npretty neat! You can also pass the infix operator as a function (`let simpler_sum xs     = every 0 (+) xs`) but you can't do this for the `*` operator because `(*)` collides with commenting in OCaml.\n\n# Float functions: `/12-list_min`\n\nWe encounter `Float.max`, `Float.min`, `Float.infinity` and `Float.neg_infinity` for the first time. pretty straight forward.\n\n### solution\n\n```\nlet rec smallest xs =\n  match xs with\n  | [] -> Float.infinity\n  | x :: ys -> Float.min x (smallest ys)\n```\n\n# Abstractions and Float functions: `/13-largest_smallest`\n\nCombining the last two exercises - abstracting functions again and using the Float functions.\n\n### solution\n\n```\nlet simpler_largest  xs = every Float.neg_infinity Float.max xs\nlet simpler_smallest xs = every Float.infinity Float.min xs\n```\n\n# Data Type: Variant Types aka Enums `/14-variants`\n\nVariants kind of work like Enums except that they can actually carry data:\n\n```\ntype card_value =\n  | Ace\n  | King\n  | Queen\n  | Jack\n  | Number of int\n\nlet one_card_value : card_value = Queen\nlet another_card_value : card_value = Number 8\n```\n\nand this is nice :)\n\n### solution\n\n```\nlet card_value_to_score card_value =\n  match card_value with\n  | Ace      -> 11\n  | King     -> 10\n  | Queen    -> 10\n  | Jack     -> 10\n  | Number i -> i\n```\n\nthis also works for the \"or\" matching\n\n```\nlet card_value_to_score card_value =\n  match card_value with\n  | Ace      -> 11\n  | King\n  | Queen\n  | Jack     -> 10\n  | Number i -> i\n```\n\n# Data Type: Tuples and Parameterized Types `/15-tuples`\n\nTuples are what they are in other languages, but their typedefs look a little weird\n\n```\ntype int_and_string_and_char = int * string * char\nlet example : int_and_string_and_char = (5, \"hello\", 'A')\n```\n\nFunctions dont have to define their types before hand. they can return the same types of things that are passed to them: \n\n```\ntype 'a pair = 'a * 'a`\n```\n\n### solution\n\nyou can destructure within the funciton definition:\n\n```\n(* this works *)\n(* let add coord1 coord2 =\n  let (a, b) = coord1 in\n  let (c, d) = coord2 in\n  (a+c, b+d) *)\n\n(* this also works *)\nlet add (a, b) (c, d) = (a+c, b+d)\n```\n\nand again \n\n```\n(* this works *)\n(* let first pair =\n  let (a, _) = pair in\n  a *)\n(* this too *)\nlet first (a, _) = a\n(* this *)\nlet second (_,b) = b\n```\n\nthe inbuilt functions `fst` and `snd` also do the same things these do.\n\n# Labelled arguments `/16-labelled_arguments`\n\nlabelling arguments...\n\n```\nval divide : dividend:int -> divisor:int -> int\nlet divide ~dividend ~divisor = dividend / divisor\n```\n\nLabelled arguments can be passed in in any order (!)\n\n\n### solution\n\n```\nlet modulo ~dividend ~divisor = dividend - (divisor * divide ~dividend ~divisor)\n```\n\n# Data Type: Options `/17-options`\n\n An ['a option] is either [None], meaning absence of data, or [Some x] meaning\n   the data exists, and that data specifically is [x].\n\n### solution\n\n```\nlet safe_divide ~dividend ~divisor =\n  match divisor = 0 with\n  | true -> None\n  | false -> Some (dividend / divisor)\n```\n\n# Anonymous functions `/18-anonymous_functions`\n\nlambda functions! defined with the `fun` keyword. eg the double function:\n\n```\n(fun i -> 2 * i)\n```\n\nironically the question doesnt test this knowledge at all.\n\n### solution\n\n```\nlet apply_if_nonzero f i =\n  match i = 0 with\n  | true -> 0\n  | false -> f i\n```\n\n# List operations `/19-list_operations`\n\n\nNow were being introduced to `List` operations: `List.map`, `List.iter`, `List.fold_left`, `List.find`, `List.find_exn`.\n\n### solution\n\nThis was my first gnarly answer:\n\n```\nlet divide dividend divisor  = dividend / divisor\nlet modulo ~dividend ~divisor = dividend - (divisor * divide dividend divisor)\nlet mod2 x = modulo x 2\nlet ifEven x =\n  match mod2 x with\n  | 0 -> 1\n  | _ -> 0\nlet num_even_ints ints =\n  let first = List.map\n    ~f:(fun x -> ifEven x)\n    ints in\n  sum_of_my_ints first\n```\n\nbut Jane Street's Core apparently has a filter and a count function:\n\n```\nCore.List.count ~f:(fun x -> x mod 2 = 0)\n```\n\n# Type Definitions! `/20-reading_sigs`\n\nSo far we havent had any practice writing our own type definitions so this is going to be tricky. we have to write our own typedefs in line 80-ish. There are two things to be careful of here: \n\n1. we have to return the abstract type `t` instead of hardcoding it to `int` even though the test is `int`\n2. labelled arguments make it into the typedef too!\n\nHere you also see the module import syntax. We import `prelude.ml` by adding `open! Prelude` (note the capital first letter) at the start.\n\nWe also start defining scoped modules here with the `module` keyword, with a `sig/end` pair for types, and then `struct/end` pair for code:\n\n```\nmodule Example : sig\n  (*  type defs *)\nend = struct\n  (*  code *)\nend\n```\n\n### solutions\n\n```\n  val create: numerator:int -> denominator:int -> t\n  val value: t -> float\n```\n\n# Folding cardio `/21-writing_list_operations`\n\na bunch of exercises here. i failed the first time because the straight append method `a :: [b]` was appending things in the wrong order, so I needed to use `List.append` to switch the order around because `[b] :: a` is not valid syntax. (you can also use `List.fold_right`)\n\n### solutions\n\n```\n  let map f lst = List.fold_left\n    lst\n    ~init:[]\n    ~f:(fun acc x ->  List.append acc [f(x)])\n\n  let iter f lst = List.fold_left\n    lst\n    ~init:()\n    ~f:(fun acc x -> f(x))\n\n  let filter f lst = List.fold_left\n    lst\n    ~init:[]\n    ~f:(fun acc x ->\n      match f(x) with\n      | true -> List.append acc [x]\n      | _ -> acc\n    )\n```\n\n# Data Type: Immutable Records `/22-records`\n\nnew data type! and making a function that returns records.\n\n### solutions\n\n```\nlet modify_person (person : person) =\n  match person.first_name with\n  | \"Jan\" -> {person with age = 30}\n  | _ -> {person with number_of_cars = person.number_of_cars + 6}\n```\n\n# Data Type: Mutable Records `/23-mutable_records`\n\nMutable records are declared with:\n\n```\ntype color =\n  | Red\n  | Yellow\n  | Green [@@deriving compare]\n\ntype stoplight =\n  { location      : string (* stoplights don't usually move *)\n  ; mutable color : color  (* but they often change color *)\n  } [@@deriving compare]\n```\n\nand modified with:\n\n```\nlet set_color stoplight color =\n  stoplight.color <- color\n```\n\n### solutions\n\n```\nlet advance_color stoplight =\n  match stoplight.color with\n  | Red -> set_color stoplight Green\n  | Yellow -> set_color stoplight Red\n  | Green -> set_color stoplight Yellow\n```\n\n# Data Type: refs `/24-refs`\n\nthey are declared with:\n\n```\nlet x = ref 0\n```\n\nand modified with:\n\n```\nlet () =\n  x := !x + 1\n```\n\nthis solution works without a ref:\n\n```\nlet take op a b =\n  match op a b with\n  | true -> a\n  | false -> b\n\nlet min_and_max lst =\n  List.fold_left\n    lst\n    ~init:(Int.max_value, Int.min_value)\n    ~f:(fun (curmin, curmax) x ->\n      (take (<) curmin x, take (>) curmax x)\n    )\n```\n\n### solutions\n\nsome notes on using a `ref`:\n\n- you should scope it in your function or you have a persistent state between functions\n- `List.iter` is `List.map` without returning a value which will have a warning.\n\n```\nlet take op a b =\n  match op a b with\n  | true -> a\n  | false -> b\n\nlet min_and_max lst =\n  let min = ref Int.max_value in\n  let max = ref Int.min_value in\n  List.iter\n    ~f:(fun x ->\n      min := take (<) !min x;\n      max := take (>) !max x;\n      )\n    lst;\n  (!min, !max)\n```\n\n**note: see Christophe's solution in the comments as well **\n\n# THAT'S ALL FOLKS!\n\nWasn't too bad was it? You can try tackling their \"froggy\" example next, but it is a lot of implementation specific stuff using the `Js_of_ocaml` library.\n\n# Extra knowledge\n\n[PPX Rewriters](https://whitequark.org/blog/2014/04/16/a-guide-to-extension-points-in-ocaml/) extend the base OCaml language with new syntax that will compile to raw ocaml. They are the \"Babel\" of OCaml. for example\n\n```\nassert([%compare.equal: int list]\n        (5 : :[1;8;4])\n        [5;1;8;4])\n```\n\n### `let` goes with `in`, and the `;;` trick\n\n`let`s that aren't paired with `in` can bleed to the next line of code which could be unrelated. you can trap errors by adding double semicolons so that the compiler knows you are done with a toplevel definition.\n\n```\nlet min_and_max lst =\n  let min = ref Int.max_value in\n  let max = ref Int.min_value in\n  List.iter\n    ~f:(fun x ->\n      min := take (<) !min x;\n      max := take (>) !max x;\n      )\n    lst;\n  (!min, !max)\n;;\n\n```\n\n### Four ways to compare things\n\nThese are basically things that were broken in our test run of the workshop; you shouldn't encounter these but these are useful references for ways to invoke the OCaml syntax (not just for comparing)\n\n```\n\n  assert ([%compare.equal: string] s \"hello\");\n  assert (String.equal s \"hello\");\n  assert (String.(=) s \"hello\");\n  assert String.(s = \"hello\");\n```"
  },
  {
    "slug": "introduction-to-mobx-4-for-reactredux-developers-3k07",
    "data": {
      "title": "Introduction to MobX 4 for React/Redux Developers",
      "description": "an introduction to mobx 4 for people coming from react and redux",
      "tag_list": [
        "mobx",
        "react",
        "mobxreact"
      ]
    },
    "content": "\nMobX uses the \"magic\" of observables to manage state and side effects. This not only has a learning curve but is a different programming paradigm altogether, and there is not a lot of up-to-date training material on how to use React with Mobx, while there is far, far more content on using React with Redux. \n\nIn this intro we will progressively build up a simple app that pings a mock API to see how MobX works with React, and then make a MobX + React Kanban board to show off the power of MobX! \n\n![it will look like this](https://user-images.githubusercontent.com/6764957/37550034-5a31617a-295d-11e8-8799-60c6f57bde0a.gif)\n\nHow we will proceed:\n\n- **Example A**. Build a basic app that lets you type an text **Input** that is reflected in a **Display**. _We show the basics of establishing `observable`s and `observer` components._\n- **Example B**. We split up the **Input** and **Display** into siblings to simulate a more complex app. We also introduce async state updating by pinging a mock API. _To do this we use the `mobx-react` `Provider` to put MobX state into React context to demonstrate easy sibling-to-sibling or sibling-to-parent communication similar to `react-redux`._\n- **Example C**: We add a secondary **Display** to our app. _Demonstrates the usefulness of `computed` variables (a Mobx concept)._\n- **Example D**: We scale our app up to do an arbitrary number of Displays. _Demonstrates using arrays and maps for our MobX state._\n- **Example E**: Tune up and Cleanup! _We add the MobX dev tools, put our whole app in `useStrict` mode and explain the formal use of MobX `action`s and `transaction`s for better app performance._\n\nThis tutorial will use [the recently released MobX 4](https://medium.com/@mweststrate/mobx-4-better-simpler-faster-smaller-c1fbc08008da) and MobX-React 5. A lot of people associate MobX with decorators, which are only a [stage 2 proposal](https://www.github.com/tc39/proposal-decorators). That (rightfully) causes hesitation for some people, but MobX 4 introduces non-decorator based syntax so we don't have that excuse anymore! However; for tutorial writers this is a problem, because you have to decide to either teach one or the other or both. To resolve this, every example here will use the non decorator syntax as the primary version, but will have a clone that uses decorators to show the equivalent implementation (e.g. Example A vs Decorators A).\n\n_Note to Reader: There is not an attempt at recommending MobX over Redux or vice versa. This is solely aimed at factually introducing core MobX concepts for people like myself who were only familiar with Redux. I will attempt to draw some conclusions but reasonable people will disagree. Additionally, Michel Weststrate has stated repeatedly that [both libraries address completely different requirements and values](https://codeburst.io/the-curious-case-of-mobx-state-tree-7b4e22d461f)._\n\n# EXAMPLE A1: React + MobX\n\nHere is our very basic app using React + MobX:\n\n```js\nimport { decorate, observable } from \"mobx\";\nimport { observer } from \"mobx-react\";\n\nconst App = observer(\n  class App extends React.Component {\n    text = \"\"; // observable state\n    render() {\n      // reaction\n      return (\n        <div>\n          Display: {this.text} <br />\n          <input\n            type=\"text\"\n            onChange={e => {\n              this.text = e.target.value; // action\n            }}\n          />\n        </div>\n      );\n    }\n  }\n);\ndecorate(App, { text: observable });\n```\n\n_([Example A1](https://codesandbox.io/s/236xqx6qn0), [Decorators A1](https://codesandbox.io/s/n7ynrm72op))_\n\nYou can see here that `observer` connects the observable `text` property of `App` so that it rerenders whenever you update `text`. \n\nWhile this is nice, it really isn't any different from using `state` and `setState`. If you have React you don't need MobX just to do this.\n\n# EXAMPLE A2: So what?\n\nLet's try separating the concerns of state and view model:\n\n```js\n// this deals with state\nconst appState = observable({\n  text: \"\" // observable state\n});\nappState.onChange = function(e) { // action\n  appState.text = e.target.value;\n};\n\n// this deals with view\nconst App = observer(\n  class App extends React.Component {\n    render() { // reaction\n      const { text, onChange } = this.props.store;\n      return (\n        <div>\n          Display: {text} <br />\n          <input type=\"text\" onChange={onChange} />\n        </div>\n      );\n    }\n  }\n);\n\n// you only connect state and view later on...\n// ... \n<App store={appState} />\n```\n\n_([Example A2](https://codesandbox.io/s/n995rqm67j), [Decorators A2](https://codesandbox.io/s/wqowjv9rvw))_\n\nHere the `store`:\n\n- is explicitly passed in as a prop (we will use the `Provider` pattern later)\n- brings its own action handlers along with it (no separate reducers to import)\n\n# EXAMPLE A3: But that's not OO\n\nLook at this part of the above code.\n\n```js\nconst appState = observable({\n  text: \"\" // observable state\n});\nappState.onChange = function(e) { // action\n  appState.text = e.target.value;\n};\n```\n\nYeah, I dont like that. The method isn't encapsulated within the observable. Can we make it more object oriented?\n\n```js\n// import { decorate } from 'mobx'\n\nclass State {\n  text = \"\"; // observable state\n  onChange = e => (this.text = e.target.value); // action\n};\ndecorate(State, { text: observable });\nconst appState = new State()\n```\n\n_([Example A3](https://codesandbox.io/s/5xmo5n513n), [Decorators A3](https://codesandbox.io/s/wo8k6j190k))_\n\nahh. much better (especially the Decorators example where you don't need to use `decorate`)!\n\n# EXAMPLE B1: But I hate prop drilling!\n\nJust like `react-redux` lets you put your store in a `Provider`, `mobx-react` also has a `Provider` that works in the same way. We will refactor our Display and our Input components into sibling apps:\n\n```js\n\nimport { inject, observer, Provider } from \"mobx-react\";\n\nclass State {\n  text = \"\"; // observable state\n  onChange = e => (this.text = e.target.value); // action\n}\ndecorate(State, { text: observable });\nconst appState = new State();\n\nconst Display = inject([\"store\"])(\n  observer(({ store }) => <div>Display: {store.text}</div>)\n);\n\nconst Input = inject([\"store\"])(\n  observer(\n    class Input extends React.Component {\n      render() {\n        // reaction\n        return <input type=\"text\" onChange={this.props.store.onChange} />;\n      }\n    }\n  )\n);\n\n// look ma, no props\nconst App = () => (\n  <React.Fragment>\n    <Display />\n    <Input />\n  </React.Fragment>\n);\n\n// connecting state with context with a Provider later on...\n// ...\n<Provider store={appState}>\n    <App />\n  </Provider>\n```\n\n_([Example B1](https://codesandbox.io/s/n71y76okym), [Decorators B1](https://codesandbox.io/s/mjz6jlyj0j))_\n\nNote that if I were to add a -second- store, I could simply define another `observable`, and pass it in to `Provider` as another prop, which I can then call from any child. No more redux style `combineReducers`!\n\nUsing a Provider also helps avoid creating global store instances, something that is strongly advised against in [MobX React Best Practices](https://medium.com/dailyjs/mobx-react-best-practices-17e01cec4140).\n\n_MobX 4 Note: If you just try to use the old MobX `observer(['store'])` shorthand, which was always synonymous with `observer` + `inject(['store'])`, you will get a very nice deprecation warning to not do that anymore._\n\nI found this inject/observer syntax a bit fiddly, so this is a nice little utility function you can define to type less:\n\n```js\nconst connect = str => Comp => inject([str])(observer(Comp));\n```\n\nHey! that's like our good friend `connect` from `react-redux`! The API is a little different, but you can define whatever you want 🤷🏼‍♂️.\n\n# EXAMPLE B2: Ok but what about async\n\nWell for async API fetching we have a few choices. We can go for:\n\n- `mobx-thunk`\n- `mobx-observable`\n- `mobx-saga`\n- and about 300 other options.\n\nThey're all special snowflakes and we can't wait to see what you decide on!\n\n_pause for rage quit..._\n\nOk if you couldnt tell, I was kidding. Using observables means you can \"just\" mutate the observables and your downstream states will react accordingly. You might have observed that I have been annotating the code examples above with `// reaction`, `// action`, and `// observable state`, and they mean what they normally mean in English. We'll come back to this.\n\nBack to code! Assume we now have an async API called `fetchAllCaps`. This is a `Promise` that basically capitalizes any text you pass to it, after a 1 second wait. So this simulates a basic request-response flow for any async action you want to take. Let's insert it into our example so far!\n\n```js\nclass State {\n  text = \"\"; // observable state\n  onChange = e => {\n    // action\n    this.text = e.target.value;\n    fetchAllCaps(e.target.value).then(val => (this.text = val));\n  };\n}\ndecorate(State, { text: observable });\nconst appState = new State();\n```\n\n_([Example B2](https://codesandbox.io/s/nn957yx004), [Decorators B2](https://codesandbox.io/s/mjz6jlyj0j))_\n\nWell that was... easy?\n\nNote that here we are using the [public class fields](https://tc39.github.io/proposal-class-public-fields/) stage 2 feature for that `onChange` property, while not using decorators, which are also stage 2. I decided to do this because public class fields are so widespread in React (for example, it comes with `create-react-app`) that you likely already have it set up or can figure out how to set it up in Babel if you need to).\n\n---\n\n# CONCEPT BREAK! Time to recap!\n\nWe've come this far without discussing core MobX concepts, so here they are:\n\n- Observable state\n- Actions\n- Derivations (Reactions and Computed values)\n\nIn our examples above we've already used **observable states** as well as defined **actions** that modify those states, and we have used `mobx-react`'s `@observer` to help bind our React components to **react** to changes in state. So that's 3 out of 4. Shall we check out Computed values?\n\n---\n\n## EXAMPLE C: Computed Values\n\n**Computed values** are essentially **reactions** without side effects. Because [Observables are lazy](https://github.com/ReactiveX/rxjs/blob/master/doc/observable.md) by default, MobX is able to defer calculations as needed. They simply update whenever the **observable state** updates. Another way of phrasing it, computed values are **derived** from observable state. \n\nLet's add a computed value that just reverses whatever is in `text`:\n\n```js\nclass State {\n  text = \"\";\n  get reverseText() {\n    return this.text\n      .split(\"\")\n      .reverse()\n      .join(\"\");\n  }\n  onChange = e => {\n    // action\n    this.text = e.target.value;\n    fetchAllCaps(e.target.value).then(val => (this.text = val));\n  };\n}\ndecorate(State, { text: observable, reverseText: computed });\nconst appState = new State();\n\n// lower down...\nconst Display2 = inject([\"store\"])(\n  observer(({ store }) => <div>Display: {store.reverseText}</div>)\n);\n```\n\n_([Example C1](https://codesandbox.io/s/jnmz0v2189), [Decorators C1](https://codesandbox.io/s/qk933p79j4))_\n\nCool! It \"just works\" (TM) !\n\nA fair question to have when looking at this is: **why bother**?? I can always put synchronous business logic in my React `render` function, why have computed values at the appState level at all?\n\nThat is a fair criticism in this small example, but imagine if you rely on the same computed values in **multiple** places in your app. You'd have to copy the same business logic all over the place, or extract it to a file and then import it everywhere. Computed values are a great way to model derivations of state by *locating them nearer to the state* rather than nearer to the *view*. It's a minor nuance but can make a difference at scale.\n\nBy the way, [vue.js](https://vuejs.org/v2/guide/computed.html) also has computed variables, while [Angular](https://stackoverflow.com/questions/43710642/does-angular-have-the-computed-property-feature-like-in-vue-js) just uses them implicitly.\n\n## EXAMPLE D1: Observable Arrays\n\nMobX can make basically anything observable. Let me quote [the docs](https://mobx.js.org/refguide/observable.html):\n\n1. If value is an ES6 Map: a new [Observable Map](https://mobx.js.org/refguide/map.html) will be returned. Observable maps are very useful if you don't want to react just to the change of a specific entry, but also to the addition or removal of entries.\n2. If value is an array, a new [Observable Array](https://mobx.js.org/refguide/array.html) will be returned.\n3. If value is an object without prototype, all its current properties will be made observable. See [Observable Object](https://mobx.js.org/refguide/object.html)\n4. If value is an object with a prototype, a JavaScript primitive or function, a [Boxed Observable](https://mobx.js.org/refguide/boxed.html) will be returned. MobX will not make objects with a prototype automatically observable; as that is the responsibility of its constructor function. Use extendObservable in the constructor, or @observable in its class definition instead.\n\nIn the examples above we have so far been making [Boxed Observables](https://mobx.js.org/refguide/boxed.html) and [Observable Objects](https://mobx.js.org/refguide/object.html), but what if we wanted to make an array of observables?\n\n[Observable Arrays](https://mobx.js.org/refguide/array.html) are array-*like* objects, not actual arrays. This can bite people in the behind, particularly when passing data to other libraries. To convert to a normal JS array, call `observable.toJS()` or `observable.slice()`.\n\nBut most of the time you can just treat Arrays as arrays. Here's a very simple Todo app using an observable array:\n\n```js\nclass State {\n  text = [\"get milk\"]; // observable array\n  onSubmit = e => this.text.push(e); // action\n}\ndecorate(State, { text: observable });\nconst appState = new State();\n\nconst Display = inject([\"store\"])(\n  observer(({ store }) => (\n    <ul>Todo: {store.text.map(text => <li key={text}>{text}</li>)}</ul>\n  ))\n);\n\nconst Input = observer(\n  [\"store\"],\n  class Input extends React.Component {\n    render() {\n      // reaction\n      return (\n        <form\n          onSubmit={e => {\n            e.preventDefault();\n            this.props.store.onSubmit(this.input.value);\n            this.input.value = \"\";\n          }}\n        >\n          <input type=\"text\" ref={x => (this.input = x)} />\n        </form>\n      );\n    }\n  }\n);\n\nconst App = () => (\n  <React.Fragment>\n    <Display />\n    <Input />\n  </React.Fragment>\n);\n```\n\n_([Example D1](https://codesandbox.io/s/p0m2l15vm), [Decorators D1](https://codesandbox.io/s/o9y428q9zq))_\n\nnote that \"just `push`\" just works!\n\n## Example D2: Observable Maps\n\nWhat's the difference between Observable Objects (what we used in Examples A, B, and C) and [Observable Maps](https://mobx.js.org/refguide/map.html)? Well, its the same difference between Plain Old Javascript Objects and [ES6 Maps](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map). I will quote the MobX doc in explaining when to use Maps over Objects:\n\n> Observable maps are very useful if you don't want to react just to the change of a specific entry, but also to the addition or removal of entries.\n\nSo if we want to have a bunch of Todo lists, where we can add new todo lists, this is the right abstraction. So if we take that App from Example D1, rename it to `TodoList` and put it in `todolist.js` with some other superficial tweaks, then on `index.js`, we can do this:\n\n```js\n// index.js\nconst connect = str => Comp => inject([str])(observer(Comp)); // helper function\n\nconst listOfLists = observable.map({\n  Todo1: new TodoListClass(),\n  Todo2: new TodoListClass()\n  // observable map rerenders when you add new members\n});\nconst addNewList = e => listOfLists.set(e, new TodoListClass());\n\nconst App = connect(\"lists\")(\n  class App extends React.Component {\n    render() {\n      const { lists } = this.props;\n      return (\n        <div className=\"App\">\n          <span />\n          <h1>MobX Kanban</h1>\n          <span />\n          {Array.from(lists).map((k, i) => (\n            <div key={i}>\n              {/*Provider within a Provider = Providerception */}\n              <Provider todolist={k}>\n                <TodoList />\n              </Provider>\n            </div>\n          ))}\n          <div>\n            <h3>Add New List</h3>\n            <form\n              onSubmit={e => {\n                e.preventDefault();\n                addNewList(this.input.value);\n                this.input.value = \"\";\n              }}\n            >\n              <input type=\"text\" ref={x => (this.input = x)} />\n            </form>\n          </div>\n        </div>\n      );\n    }\n  }\n);\n```\n\n_([Example D2](https://codesandbox.io/s/x3pl9p042w), [Decorators D2](https://codesandbox.io/s/ryonwww274))_\n\nAnd hey presto! We have a Kanban board (an expandable list of lists)!\n\n![kanban](https://user-images.githubusercontent.com/6764957/37550034-5a31617a-295d-11e8-8799-60c6f57bde0a.gif)\n\nThis was enabled by the dynamically expanding ability of that `listOfLists` which is an Observable Map. To be honest, you could probably also use Arrays to achieve this but if you have a use case that is better suited for demonstrating Observable Maps, please **let me know in the comments below.**\n\n# Example E1: MobX Dev Tools\n\n[Redux dev tools](https://github.com/gaearon/redux-devtools/blob/master/docs/Walkthrough.md) are (rightfully) an important part of Redux's value, so let's check out [MobX React dev tools](https://github.com/mobxjs/mobx-react-devtools)!\n\n```js\nimport DevTools from 'mobx-react-devtools'; // npm install --save-dev mobx-react-devtools\n\n// somewhere within your app...\n        <DevTools />\n```\n\n_([Example E1](https://codesandbox.io/s/7m0xoy9336), [Decorators E1](https://codesandbox.io/s/oo2n902jr5))_\n\n\nYou can see the three icons pop up:\n\n- Visualize rerenders\n- Audit the dependency tree\n- Log everything to console (use Browser console not Codepen console)\n\nYou can't do time travel but this is a pretty good set of tools to audit any unexpected state changes going on in your app.\n\n# Stay tuned...\n\nThere is a blocking bug with `mobx-dev-tools` and `mobx` 4: <https://github.com/mobxjs/mobx-react-devtools/issues/86> and I will finish this out  when the bug is fixed.\n\n\nHowever in the mean time you can check out how to explicitly define `actions` so that MobX can batch your state changes into `transaction`s, which is a big performance saver:\n\n<https://mobx.js.org/refguide/action.html>\n\n\nNotice how we were able to do all our demos without using the `action`s - MobX has a (poorly) documented strict mode (formerly `useStrict`, now `configure({enforceActions: true});`) - see [the MobX 4 docs](https://github.com/mobxjs/mobx/blob/gh-pages/docs/refguide/api.md#configure). But we need the dev tools to really show the benefits for our example app.\n\n\n## Acknowledgements\n\nThis introduction borrows a lot of code and structure from [Michel Weststrate's egghead.io course](https://egghead.io/courses/manage-complex-state-in-react-apps-with-mobx), but updates the 2 year old course for the current Mobx 4 API. I would also like to thank my employer for allowing me to learn in public.\n\nThe examples here were done with the help of [Javid Askerov](http://twitter.com/askerovlab), [Nader Dabit](http://twitter.com/dabit3), and [Michel](http://twitter.com/mweststrate).\n\n## Other Tutorials and Further Reading\n\nOther recent guides\n- [MobX + React Native](https://dev.to/satansdeer/react-native-mobx-tutorial---part-1--2df0)\n- [MobX + React best practices](https://medium.com/dailyjs/mobx-react-best-practices-17e01cec4140)\n- [MobX 4 launch blogpost](https://medium.com/@mweststrate/mobx-4-better-simpler-faster-smaller-c1fbc08008da)\n\nDocs\n- [MobX docs - common pitfalls and best practices](https://mobx.js.org/best/pitfalls.html)\n- [MobX changelog - be very careful on v3 vs v4 changes](https://github.com/mobxjs/mobx/blob/master/CHANGELOG.md)\n- [official MobX+React 10 minute guide](https://mobx.js.org/getting-started.html)\n\nOlder\n- [one developer's pros and cons comparison of redux vs mobx](https://dannyherran.com/2017/03/react-redux-mobx-takeaways/)\n- [Adam Rackis' old evaluation of MobX](https://medium.com/@adamrackis/a-redux-enthusiast-tries-mobx-af675f468c11)\n\nRelated libraries to explore\n- [MobX state tree](https://github.com/mobxjs/mobx-state-tree) and associated [blogpost](https://codeburst.io/the-curious-case-of-mobx-state-tree-7b4e22d461f)\n\n## Contribute\n\nWhat other current (<1yr) resources should I include in this guide? Have I made any mistakes? Let me know below!"
  },
  {
    "slug": "a-walkthrough-of-that-react-suspense-demo--4j6a",
    "data": {
      "title": "A Walkthrough of *that* React Suspense Demo",
      "description": "Annotated commentary on the code behind the Movie search demo featuring React Suspense",
      "tag_list": [
        "react"
      ]
    },
    "content": "\r\n## Update from Nov 2018: The API's below are out of date, check https://github.com/sw-yx/fresh-concurrent-react for an up to date guide!\r\n\r\n---\r\n\r\n![the suspense is real!](https://user-images.githubusercontent.com/3624098/36625698-50323b20-18d9-11e8-87bf-de1133b023ec.gif)\r\n\r\n---\r\n\r\nBottomline up front: In this walkthrough of the 300ish line Movie Search demo, we learn the various aspects of the React Suspense API:\r\n\r\n- `simple-cache-provider.SimpleCache` - puts a `cache` in `createContext`\r\n- `simple-cache-provider.createResource` - which 1) takes a **promise** for your data and 2) outputs a function that takes a `cache` and an arg to call your **promise** (also called the **suspender**)\r\n- How to delegate updates to a lower priority with `ReactDOM.unstable_deferredUpdates`\r\n- How `createResource` loads data asynchronously by **throwing Promises** (!!!)\r\n- `React.Timeout` - just gives you a boolean for flipping between children and fallback\r\n- How to use `createResource` to do **async image loading** (!!!)\r\n\r\nRead on if you want to learn React Suspense!\r\n\r\n---\r\n\r\nThe Async React demo at JSConf Iceland lived up to the hype: Time Slicing and React Suspense are on the way! (See the [official blogpost](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html), [video](https://www.youtube.com/watch?v=v6iR3Zk4oDY), and [HN discussion](https://news.ycombinator.com/item?id=16492973) for more). Watching the [video](https://www.youtube.com/watch?v=v6iR3Zk4oDY) is a prerequisite for the rest of this article!\r\n\r\nDev Twitter was buzzing with prominent devs working through the implications of Async React for everything from [React-Loadable](https://twitter.com/jamiebuilds/status/969166935836344321?s=21) to [React Router](https://twitter.com/ryanflorence/status/969235637244018688?s=21) to [Redux](https://twitter.com/wsokra/status/969284466043703297?s=21), and the always-on-the-ball Apollo Team even pushed out [a demo app built with Async React and Apollo](https://twitter.com/apollographql/status/969313298440126466)!\r\n\r\nNeedless to say, people were excited (read the whole thing, its hilarious):\r\n\r\n{% twitter 969135585909792768 %}\r\n\r\nAnd the [spectrum.chat](https://spectrum.chat/?t=586129b0-845c-4025-bd0e-f4a2200a971b) folks were veeery excited:\r\n\r\n![image](https://user-images.githubusercontent.com/6764957/36888678-88ba5d66-1dc4-11e8-9813-7f6c034c07a2.png)\r\n\r\nHeady stuff. This is the culmination of a years-long process, starting with [this tweet from Jordan Walke in 2014](https://twitter.com/jordwalke/status/500587022890061824), to [Lin Clark's intro to React Fiber](https://www.youtube.com/watch?v=ZCuYPiUIONs) (where you see Time Slicing working almost a year ago), to [the actual React Fiber release](https://code.facebook.com/posts/1716776591680069/react-16-a-look-inside-an-api-compatible-rewrite-of-our-frontend-ui-library/) in Sept 2017, to [Sebastian coming up with the suspender API](https://twitter.com/acdlite/status/969172311067713537) in Dec 2017.\r\n\r\nBut if you're just a regular React-Joe like me, you're feeling a little bit left behind in all this (as it should be - this is advanced stuff and not even final yet, so if you're a React newbie STOP READING AND GO LEARN REACT).\r\n\r\nI learn by doing, and am really bad at grokking abstract things just by talking about them.\r\n\r\nFortunately, Andrew Clark published [a version of the Movie search demo](https://codesandbox.io/s/5zk7x551vk) on CodeSandbox! So I figured I would walk through just this bit since it's really all the demo usage code we have (apart from the Apollo demo which is a fork of this Movie search demo) and I didn't feel up to walking through the entire source code (I also happen to be really sick right now, but learning makes me happy :)).\r\n\r\nFinally, some disclaimers because people get very triggered sometimes:\r\n\r\n1. I'm a [recent bootcamp grad](https://hackernoon.com/no-zero-days-my-path-from-code-newbie-to-full-stack-developer-in-12-months-214122a8948f). You're not reading the divinings of some thought leader here. I'm just some guy learning in public.\r\n2. This API is EXTREMELY UNSTABLE AND SUBJECT TO CHANGE. So forget the specifics and just think about if the concepts make sense for you.\r\n3. If you're a React newbie YOU DO NOT NEED TO KNOW THIS AT ALL. None of this needs to be in any sort of React beginner curriculum. I would put this -after- your learning Redux, and -after- learning the [React Context API](https://reactjs.org/docs/context.html)\r\n\r\nBut learning is fun! Without further ado:\r\n\r\n# Diving into React Suspense\r\n\r\nPlease have [the demo](https://codesandbox.io/s/5zk7x551vk) open in another screen as you read this, it will make more sense that way.\r\n\r\n---\r\n\r\nonce again for the people who are skimming:\r\n\r\n# [HEY! YOU! OPEN THE DEMO BEFORE YOU READ ON!](https://codesandbox.io/s/5zk7x551vk)\r\n\r\n--- \r\n\r\n## Meet `simple-cache-provider.SimpleCache`\r\n\r\nThe majority of the app is contained in `index.js`, so that's where we start. I like diving into the tree from top level down, which in the code means you read from the bottom going up. Right off the bat in line 303, we see that the top container is wrapped with the `withCache` HOC. This is defined in `withCache.js`:\r\n\r\n```js\r\nimport React from 'react';\r\nimport {SimpleCache} from 'simple-cache-provider';\r\n\r\nexport default function withCache(Component) {\r\n  return props => (\r\n    <SimpleCache.Consumer>\r\n      {cache => <Component cache={cache} {...props} />}\r\n    </SimpleCache.Consumer>\r\n  );\r\n}\r\n```\r\n\r\nHere we see the second React API to adopt the child render prop (see [Kent Dodds' recap for the first](https://dev.to/kentcdodds/reacts--new-context-api-dpi)), and it simply provides a `cache` prop to whatever Component is passed to it. [The source for simple-cache-provider](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js) comes in just under 300 lines of Flow-typed code, and you can see it uses [createContext](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L297) under the hood. You may have heard a lot of fuss about the \"throw pattern\", but this is all nicely abstracted for you in `simple-cache-provider` and you never actually have to use it in your own code.\r\n\r\nJust because it really is pretty cool, you can check it out in [line 187](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L187) where the promise is thrown and then called in the `load` function in [line 128](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L128). We'll explore this further down.\r\n\r\n## Side Effects in Render\r\n\r\nThe main meat of the Movie Search demo is in the `MoviesImpl` component:\r\n\r\n```js\r\nclass MoviesImpl extends React.Component {\r\n  state = {\r\n    query: '',\r\n    activeResult: null,\r\n  };\r\n  onQueryUpdate = query => this.setState({query});\r\n  onActiveResultUpdate = activeResult => this.setState({activeResult});\r\n  clearActiveResult = () => this.setState({activeResult: null});\r\n  render() {\r\n    const cache = this.props.cache;\r\n    const state = this.state;\r\n    return (\r\n      <AsyncValue value={state} defaultValue={{query: '', activeResult: null}}>\r\n      /*just renders more JSX here */\r\n      </AsyncValue>\r\n    );\r\n  }\r\n}\r\n```\r\n\r\nThe first thing to notice is that there are no side effects outside of `render`. Pause to think about how you would normally do side effects in a React component - either do it in a lifecycle method like `componentDidMount` or `componentDidUpdate`, or in your event handlers like `onQueryUpdate` and `onActiveResultUpdate` above. How is this app updating as you type in queries in to the input box?\r\n\r\nThis is where things start to look really weird. The answer is in that AsyncValue component.\r\n\r\n## Meet ReactDOM.unstable_deferredUpdates\r\n\r\nThe answer, as with everything, is 42. Specifically, scroll up to line 42 to find the source of `AsyncValue`:\r\n\r\n```js\r\nclass AsyncValue extends React.Component {\r\n  state = {asyncValue: this.props.defaultValue};\r\n  componentDidMount() {\r\n    ReactDOM.unstable_deferredUpdates(() => {\r\n      this.setState((state, props) => ({asyncValue: props.value}));\r\n    });\r\n  }\r\n  componentDidUpdate() {\r\n    if (this.props.value !== this.state.asyncValue) {\r\n      ReactDOM.unstable_deferredUpdates(() => {\r\n        this.setState((state, props) => ({asyncValue: props.value}));\r\n      });\r\n    }\r\n  }\r\n  render() {\r\n    return this.props.children(this.state.asyncValue);\r\n  }\r\n}\r\n```\r\n\r\n`ReactDOM.unstable_deferredUpdates` is an undocumented API but it is not new, going [as far back as Apr 2017](https://twitter.com/koba04/status/854926955463950337?lang=en) (along with [unstable_AsyncComponent](https://github.com/koba04/react-fiber-resources)). My uneducated guess is that this puts anything in `asyncValue` (namely, `query` and `activeResult`) as a lower priority update as compared to UI updating.\r\n\r\n## Skipping MasterDetail, Header, and Search\r\n\r\nGreat! back to parsing the innards of `AsyncValue`.\r\n\r\n```js\r\n      <AsyncValue value={state} defaultValue={{query: '', activeResult: null}}>\r\n        {asyncState => (\r\n          <MasterDetail\r\n            header={<Header />} // just a string: 'Movie search'\r\n            search={ // just an input box, we will ignore\r\n            }\r\n            results={ // uses <Results />\r\n            }\r\n            details={ // uses <Details />\r\n            }\r\n            showDetails={asyncState.activeResult !== null}\r\n          />\r\n        )}\r\n      </AsyncValue>\r\n```\r\n\r\nNothing too controversial here, what we have here is a `MasterDetail` component with FOUR render props (yo dawg, I heard you like render props...). `MasterDetail` 's only job is CSS-in-JS, so we will skip it for now. `Header` is just a string, and `Search` is just an input box, so we can skip all that too. So the remaining components we care about are `Results` and `Details`.\r\n\r\n## Digging into `simple-cache-provider.createResource`\r\n\r\nIt turns out that both use similar things under the hood. Here is `Results` on line 184:\r\n\r\n```js\r\nfunction Results({query, cache, onActiveResultUpdate, activeResult}) {\r\n  if (query.trim() === '') {\r\n    return 'Search for something';\r\n  }\r\n  const {results} = readMovieSearchResults(cache, query);\r\n  return (\r\n    <div css={{display: 'flex', flexDirection: 'column'}}>\r\n       /* some stuff here */\r\n    </div>\r\n  );\r\n}\r\n```\r\n\r\nThe key bit is `readMovieSearchResults`, which is defined like so:\r\n\r\n```js\r\nimport {createResource} from 'simple-cache-provider';\r\n\r\n// lower down...\r\n\r\nasync function searchMovies(query) {\r\n  const response = await fetch(\r\n    `${TMDB_API_PATH}/search/movie?api_key=${TMDB_API_KEY}&query=${query}&include_adult=false`,\r\n  );\r\n  return await response.json();\r\n}\r\n\r\nconst readMovieSearchResults = createResource(searchMovies);\r\n```\r\n\r\nNote that the `Results` component is still in the \"render\" part of the overall app. We are passing the `searchMovies` promise to the new `createResource` API, which is in the `simple-cache-provider` [source](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L251) \r\n\r\nNow createResource uses some dark magic I don't totally understand and isnt strictly necessary for the demo, but indulge me. The rough process goes from\r\n\r\n- [createResource defined in line 251](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L251)\r\n- [cache.read called in line 268](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L268)\r\n- [cache.read defined in line 175](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L175)\r\n- since the cache state is empty, [throw the suspender in line 187](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L187)!!!\r\n- We have a thrown promise! where do we catch it! \r\n- I have no. frigging. clue. There's no `catch` anywhere! (Update from the future: [Andrew confirms](https://twitter.com/acdlite/status/969488267073630208) this was part of the special hacky build of React they did to put this together)\r\n- At some point, the promise bubbles up to `createCache` (which we declared aallll the way up at the top level with `SimpleCache`) and `load` is called on the cache. How do I know this? [Line 128 is the only `.then` in the entire app](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L128).\r\n- From here, it gets easier. the cache is either in a `Resolved` or `Rejected` state. If `Resolved`, the [record.value](https://github.com/facebook/react/blob/master/packages/simple-cache-provider/src/SimpleCacheProvider.js#L192) is returned and emerges as the new `activeResult` in `AsyncValue` which re-renders the whole thing.\r\n\r\nThis circuitous roundabout method is the core innovation of React Suspense and you can tell it's just a bit above my level of understanding right now. But that is how you achieve side effects inside of your render (without causing an infinite loop).\r\n\r\n**THIS IS THE KEY INSIGHT: \"Suspense\" is where `readMovieSearchResults(cache, query)` is used synchronously in the code example above. If the `cache` doesn't contain the results for your `query` (stored internally as a `Map` using a hash), it \"suspends\" the render and throws the promise.**\r\n\r\nApollo and others will have alternative cache implementations.\r\n\r\nYikes, that was gnarly! Let me know in the comments if there's something I got wrong. I'm learning too.\r\n\r\nSo that's `Results` (mostly) done. On to `Details`!\r\n\r\n## The devil is in the Details\r\n\r\nActually, `Details` is just a thin wrapper around `MovieInfo`, which is defined on line 227:\r\n\r\n```js\r\nfunction MovieInfo({movie, cache, clearActiveResult}) {\r\n  const fullResult = readMovie(cache, movie.id);\r\n  return (\r\n    <Fragment>\r\n      <FullPoster cache={cache} movie={movie} />\r\n      <h2>{movie.title}</h2>\r\n      <div>{movie.overview}</div>\r\n    </Fragment>\r\n  );\r\n}\r\n```\r\n\r\n`readMovie` is a similar cache call to `readMovieSearchResults`, it just calls that new `createResource` with a different URL to `fetch`. What I want to highlight is rather `FullPoster`:\r\n\r\n```js\r\nfunction FullPoster({cache, movie}) {\r\n  const path = movie.poster_path;\r\n  if (path === null) {\r\n    return null;\r\n  }\r\n  const config = readConfig(cache);\r\n  const size = config.images.poster_sizes[2];\r\n  const baseURL =\r\n    document.location.protocol === 'https:'\r\n      ? config.images.secure_base_url\r\n      : config.images.base_url;\r\n  const width = size.replace(/\\w/, '');\r\n  const src = `${baseURL}/${size}/${movie.poster_path}`;\r\n  return (\r\n    <Timeout ms={2000}>\r\n      <Img width={width} src={src} />\r\n    </Timeout>\r\n  );\r\n}\r\n```\r\n\r\nHere we have a bunch of new things to deal with. `readConfig` is yet another cache call (see how we're just casually making all these calls as we need them in the render?), then we have some normal variable massaging before we end up using the `Timeout` and the `Img` components.\r\n\r\n## Introducing `React.Timeout`\r\n\r\nHere's `Timeout.js`:\r\n\r\n```js\r\nimport React, {Fragment} from 'react';\r\n\r\nfunction Timeout({ms, fallback, children}) {\r\n  return (\r\n    <React.Timeout ms={ms}>\r\n      {didTimeout => (\r\n        <Fragment>\r\n          <span hidden={didTimeout}>{children}</span>\r\n          {didTimeout ? fallback : null}\r\n        </Fragment>\r\n      )}\r\n    </React.Timeout>\r\n  );\r\n}\r\n\r\nexport default Timeout;\r\n```\r\n\r\nYes, this is new ([here's the PR to add it](https://github.com/facebook/react/pull/12279/files#diff-50d8f0a9fb4af9baa0c2d4b8905567a7), its mixed in with a bunch of other React Fiber code so explore at your own risk). But its intuitive: Feed in a `ms` prop, which then controls a boolean `didTimeout`, which if true hides the `children` and shows the `fallback`, or if false shows the `children` and hides the `fallback`. The third React API to use a render prop, for anyone keeping count! \r\n\r\nPop quiz: why do this children/fallback behavior using `<span hidden>` rather than encapsulate the whole thing in `{didTimeout ? fallback : children}` and not have a `<span>` tag at all? Fun thing to consider if you haven't had to before (reply in the comments if you're not sure!)\r\n\r\nOn to the other thing.\r\n\r\n## Async Image Loading, or, how to make just passing a string not boring\r\n\r\nHere's `Img.js`:\r\n\r\n```js\r\nimport React from 'react';\r\nimport {SimpleCache, createResource} from 'simple-cache-provider';\r\nimport withCache from './withCache';\r\n\r\nfunction loadImage(src) {\r\n  const image = new Image();\r\n  return new Promise(resolve => {\r\n    image.onload = () => resolve(src);\r\n    image.src = src;\r\n  });\r\n}\r\n\r\nconst readImage = createResource(loadImage);\r\n\r\nfunction Img({cache, src, ...props}) {\r\n  return <img src={readImage(cache, src)} {...props} />;\r\n}\r\n\r\nexport default withCache(Img);\r\n\r\n```\r\n\r\nWhat's this! We're creating another cache! Yes, there's no reason we cant have multiple caches attached to different components, since we're \"just\" using `createContext` under the hood as we already established. But what we are using it -for- is new: async image loading! w00t! To wit:\r\n\r\n- use the `Image()` constructor (yea, I didn't know this was a thing either, [read the MDN and weep](https://developer.mozilla.org/en-US/docs/Web/API/HTMLImageElement/Image))\r\n- wrap it in a `Promise` and set the `src`\r\n- pass this `Promise` to `createResource` which does its thing (don't even ask.. just.. just scroll up, thats all I got for you)\r\n- and when the loading is done, we pass it through to the `<img src`!\r\n\r\nTake a moment to appreciate how creative this is. at the end of the day we are passing `src`, which is a string, to `<img src`, which takes a string. Couldn't be easier. But IN BETWEEN THAT we insert our whole crazy `createResource` process to load the image asynchronously, and in the meantime `<img src` just gets nothing to render so it shows nothing. \r\n\r\n**HELLO KEY INSIGHT AGAIN: We \"suspend\" our render if the `cache` does not have the hash for `src`, and throw the Promise, which doesn't resolve until the `image` gets loaded, which is when React knows to rerender `Img` again.**\r\n\r\nBOOM MIC DROP.\r\n\r\nDoes this look familiar? Passing a string now has side effects. This is just the same as passing JSX to have side effects. **React Suspense lets you insert side effects into anything declarative, not just JSX!**\r\n\r\n\r\n## Homework\r\n\r\nThere are only two more things to explore: `Result` and `PosterThumbnail`, but you should be able to recognize the code patterns from our analysis of `FullPoster` and `Img` now. I leave that as an exercise for the reader.\r\n\r\nSo taking a step back: What have we learned today?\r\n\r\n- `simple-cache-provider.SimpleCache` - puts a `cache` in `createContext`\r\n- `simple-cache-provider.createResource` - which 1) takes a **promise** for your data and 2) outputs a function that takes a `cache` and an arg to call your **promise** (also called the **suspender**)\r\n- How to delegate updates to a lower priority with `ReactDOM.unstable_deferredUpdates`\r\n- How `createResource` loads data asynchronously by **throwing Promises** (!!!)\r\n- `React.Timeout` - just gives you a boolean for flipping between children and fallback\r\n- How to use `createResource` to do **async image loading** (!!!)\r\n\r\nThat is a LOT packed into 300 lines of code! Isn't that nuts? I certainly didn't get this from just watching the talk; I hope this has helped you process some of the finer details as well.\r\n\r\nHere are some other notable followups from the post-talk chatter:\r\n\r\nFor people who want to use createFetcher from the talk (although [simple-cache-provider is the official implementation for now](https://twitter.com/acdlite/status/969168681644179456)):\r\n\r\n{% twitter 969169357094842368 %}\r\n\r\n*(read entire thread not just this tweet)*\r\n\r\nWant to see a createFetcher (without simple-cache-provider) in action? [Jamie is on it in this sandbox demo](https://codesandbox.io/s/zk0y314yqp)\r\n\r\nNeed more demos? Dan Abramov is somehow still writing live examples (using his implementation of `createFetcher`):\r\n\r\n{% twitter 969344290567704577 %}\r\n\r\nIf you are worried about multiple throws:\r\n\r\n{% twitter 969291270064451584 %}\r\n\r\n*(read entire thread not just this tweet)*\r\n\r\nIf you still aren't sure if throwing Promises are a good thing, you're not alone (this was supposed to be controversial!):\r\n\r\n{% twitter 969254508978155520 %}\r\n\r\n*(read entire thread not just this tweet)*\r\n\r\nWhy use Promises? What if I want to cancel my fetching? Why not generators? or Observables?\r\n\r\n{% twitter 969311753015185408 %}\r\n\r\n*(read entire thread not just this tweet - [Idempotence](https://twitter.com/acdlite/status/969173378937470977) is the keyword)*\r\n\r\nWhere can you -not- use suspend? Andrew Clark's got you:\r\n\r\n{% twitter 969428655238557697 %}\r\n\r\n*(read entire thread not just this tweet)*\r\n\r\nWhat have I missed or got wrong? please let me know below! Cheers!\r\n\r\n---\r\n\r\nEdit March 27 2018 \r\n\r\nI am now rewatching the combined [JSConf](https://www.youtube.com/watch?v=v6iR3Zk4oDY&t=1126s) and [ReactFest](https://www.youtube.com/watch?v=6g3g0Q_XVb4) Demos to teas out the Suspense use cases. Here goes.\r\n\r\n- https://youtu.be/v6iR3Zk4oDY?t=15m50s: createFetcher - basic fetching\r\n- https://youtu.be/v6iR3Zk4oDY?t=17m43s: this.deferSetState - need to tell React that it's ok for this to be asynchronous\r\n- https://youtu.be/v6iR3Zk4oDY?t=18m5s: showing that it is interactive and has no race conditions\r\n- https://youtu.be/v6iR3Zk4oDY?t=18m53s: high latency demo - Placeholder, delayMs, fallback. If any of its children need data, its going to wait for them. Screen remains interactive\r\n- https://youtu.be/v6iR3Zk4oDY?t=21m51s: second createFetcher demo - movie reviews. React waiting for both MovieDetails and MovieReviews as siblings. Even though one gets loaded, it still doesnt display until its sibling is also loaded.\r\n- https://youtu.be/v6iR3Zk4oDY?t=22m43s: what if you dont want siblings to wait for each other? do another Placeholder!\r\n- https://youtu.be/v6iR3Zk4oDY?t=24m1s: having visual indication of loading. use Loading! Easy inline spinner, and navigation controlled by `isLoading` render prop. No race conditions.\r\n- https://youtu.be/v6iR3Zk4oDY?t=26m9s: easy code splitting using createFetcher\r\n- https://youtu.be/v6iR3Zk4oDY?t=27m56s: avoid page jumping when picture loads - async fetch images as well! using createFetcher to create ImageFetcher, and putting it in src!\r\n- https://youtu.be/v6iR3Zk4oDY?t=30m13s: end of IO demo: We've built a generic way for components to suspend rendering while they load async data. \r\n- https://youtu.be/v6iR3Zk4oDY?t=31m32s: git metaphor\r\n- https://youtu.be/v6iR3Zk4oDY?t=33m12s: CPU + IO = Async Rendering\r\n\r\nReactFest\r\n\r\n- https://youtu.be/6g3g0Q_XVb4?t=2m6s Problems we are solving\r\n- https://youtu.be/6g3g0Q_XVb4?t=5m15s hardcoded movie demo\r\n- https://youtu.be/6g3g0Q_XVb4?t=7m34s basic createFetcher demo - solves race conditions\r\n- https://youtu.be/6g3g0Q_XVb4?t=10m52s placeholder demo - to solve the potentially laggy connection problem\r\n- https://youtu.be/6g3g0Q_XVb4?t=12m56s adding second fetcher - second doesnt render even if it loads before the first one\r\n- https://youtu.be/6g3g0Q_XVb4?t=14m43s adding placeholder to show nonblocking\r\n- https://youtu.be/6g3g0Q_XVb4?t=15m29s keeping old screen interactive is a need\r\n- https://youtu.be/6g3g0Q_XVb4?t=16m3s so use `<Loading />`!\r\n- https://youtu.be/6g3g0Q_XVb4?t=18m35s code splitting\r\n- https://youtu.be/6g3g0Q_XVb4?t=21m41s image loading\r\n- https://youtu.be/6g3g0Q_XVb4?t=23m37s SLOW image loading with placeholder\r\n- https://youtu.be/6g3g0Q_XVb4?t=24m48s N+1 preloading: `hidden={true}` **this is super cool and is new**\r\n- https://youtu.be/6g3g0Q_XVb4?t=29m15s react suspense benefits\r\n- https://youtu.be/6g3g0Q_XVb4?t=30m1s git metaphor"
  },
  {
    "slug": "scraping-my-twitter-social-graph-with-python-and-selenium--hn8",
    "data": {
      "title": "Scraping my Twitter Social Graph with Python and Selenium",
      "description": "Using python and selenium to find better follows on Twitter",
      "tag_list": [
        "datascience",
        "python",
        "selenium",
        "javascript"
      ]
    },
    "content": "\r\nI have been on Twitter for 9 years, but only just realized this: Twitter is at its best when used like Messenger or WhatsApp, not when it is used like Facebook. \r\n\r\n{% twitter 964497956987654146 %}\r\n\r\nIn other words, I get the most out of Twitter when I use it to connect with real people with shared interests, not to keep up with news or companies or celebrities, and definitely not for arguing with random internet strangers.\r\n\r\n# Finding Dev Twitter\r\n\r\nAfter 9 years (mostly dormant) on Twitter, I had amassed about 4000 Twitter follows. They reflect my background: Some are finance accounts, some musicians, some product/makers, some joke accounts, some devs. But in line with my realization above, I found myself wanting to decrease the noise and turn my Twitter usage into something that helps improve my new career.\r\n\r\nFor better or worse, a large proportion of the developer community is on Twitter. I only started getting involved in \"Dev Twitter\" about midway through [my career change from finance to software engineering](https://hackernoon.com/no-zero-days-my-path-from-code-newbie-to-full-stack-developer-in-12-months-214122a8948f), but quickly got lost in the wild noise.\r\n\r\n# The state of Dev Twitter\r\n\r\nDev Twitter is wonderful: You can [engage with senior developers](https://twitter.com/sophiebits/status/960539240655241216), [get help](https://twitter.com/kyleshevlin/status/963111519851495424) when you run into trouble, [publicize your work](https://twitter.com/ryanflorence/status/964682338981486593), and even [get jobs](https://twitter.com/sehurlburt/status/923830594240372736).\r\n\r\nHowever Twitter can also be every bit [the dumpster fire](https://twitter.com/internethippo/status/874985315424972801?lang=en) people make it out to be: A continuous cacophony of confusing context-light criticism-heavy comments covering everything from sports to politics to celebrities to politics to tech to politics to finance to politics to bots. Even outside of cheap jabs at politics, you also get [occasional](https://twitter.com/acemarke/status/9631241516649553920) [meltdowns](https://twitter.com/littlecalculist/status/963946952403505152) on Dev Twitter that nobody really needs. (Javascript even has [Horse_JS](https://twitter.com/horse_js), a dedicated but loved troll account that calls things out!) It even prompted [SecondCareerDev's](https://secondcareerdevs.com/) [Kyle Shevlin](https://twitter.com/kyleshevlin) to formulate [Twitter Rules of Engagement](https://twitter.com/kyleshevlin/status/964636669390409728) (which I highly recommend).\r\n\r\nNow to be clear: I support political involvement. I also believe that people should have a diversity of interests, and should be free to openly disagree with each other. This post isn't about any that.\r\n\r\nTwitter, like many social platforms, has a \"trust me I know what's best for you\" recommendation algorithm. As you scroll down your main feed, you see tweets from people who are followed by the people you follow. If you head to the Search tab (on the mobile app) and hit Connect, you see a list of people suggested by \"Because you follow\", \"People you may know\", and \"Based on your activity\" algorithms (the latter is the worst since it makes recommendations off a single data point). If you have used Twitter a bit, you will recognize the groupings these algorithms are making: Here's the \"Women in Tech\" group, here's the \"massively popular content creators\" group. While technically correct, a lot of the options end up just feeling *wrong*. I follow [the ReactJS twitter account](https://twitter.com/reactjs), and it suggests that I follow the [Angular](https://twitter.com/angular) and [EmberJS](https://twitter.com/emberjs) accounts. They are great frameworks but are simply not accounts I want to follow at this time. I'm no American football fan but I'd hazard that this same algorithm would suggest the Patriots account to a Seahawks fan too, the way it seems to think.\r\n\r\nAnyway.\r\n\r\nTwitter users complement this automated recommendation by retweeting others for exposure, and also calling them out in special posts. This even got its own special hashtag, known as [#FollowFriday](https://www.lifewire.com/a-guide-to-follow-friday-2655376). Because bias happens, there are occasionally special posts [like these](https://twitter.com/sehurlburt/status/936530247092133890?lang=en) from prominent community members helping out underrepresented groups. But it is very ad-hoc and manual.\r\n\r\nSo being a developer, the natural question arises: What if I take the recommendation algorithm into my own hands?\r\n\r\n# The basic idea\r\n\r\nDevelopers are familiar with the idea that [everything is a graph](http://tcl.sfs.uni-tuebingen.de/~cornell/prolog/Graphs001.html). Twitter is a manually explored, social graph of users with varying (even probabilistic) signal quality and an unclear, varying optimization function. The highest signal is a follow, which is more persistent, whereas likes, retweets, and replies are also signals but are more of a one-off nature. If you follow a bunch of people you consider to be high quality follows, then their follows have a better than random chance of being interesting to you too. There's no real term for \"follow-squared\" so I've taken to calling them \"fofollows\". \r\n\r\nAll this of course has more academic grounding than I am qualified to speak about, but basically you will want to look into [Network Centrality Algorithms](https://en.wikipedia.org/wiki/Centrality) to see how academics formally define various measures of network centrality.\r\n\r\nTo be honest, I don't like the idea of defining a \"good follow\" by \"number of fofollows\". Because people (including myself) follow with a herd mentality, this overly biases towards celebrity culture, and disadvantages those who also put out quality content but for whatever reason have not yet gained recognition for it. So for example, this algorithm would favor someone famous who just sets up their twitter account to crosspost from instagram may get a ton of follows and likes and retweets, *even though this person doesn't even use twitter*. I would definitely favor someone who actually gives thoughtful replies to people but has far less follows. I have some ideas on how to do this but will only have space for addressing them in a future post. (I just wanted to register upfront that I know this is a very flawed algorithm, and invite constructive suggestions.)\r\n\r\n# The technical challenges\r\n\r\nWhile I won't be quite able to solve society's ills in this post alone, there are some interesting things that we can do with the info we have:\r\n\r\n1. AUTOMATION: first, we have to scrape our data from Twitter. This will be the majority of the value of this post if you are coding along.\r\n2. ANALYSIS: second, we have to process the data to surface metrics that we want aka feature engineering\r\n3. DISPLAY: lastly, we have to show the results in an easily understandable way so I (and interested others) can iterate on it and then finally act on it\r\n\r\nThese three things are very different skill sets and in a real company would be a bunch of different jobs for different people. But I'm just doing this on my own time to improve my own personal situation. So as ambitious as I'd like to be to produce an authoritative result, I'd frankly be happy with just a 10% better experience (not that that can even be measured).\r\n\r\n# AUTOMATION - Scraping Twitter\r\n\r\nFirst off: I am no legal expert so proceed at your own caution. But let's just say Twitter has bigger bots to deal with than you if you write one.\r\n\r\nOk. Although I am a professional JS guy, and there are [ways to do scraping in NodeJS](https://codeburst.io/a-guide-to-automating-scraping-the-web-with-javascript-chrome-puppeteer-node-js-b18efb9e9921), the Python scraping and number crunching ecosystem has simply been around for far, far longer, so that's what I'm going with.\r\n\r\nTo follow along, make sure you have [Jupyter Notebook and the Anaconda distribution of Python](http://jupyter.org/install). If you are completely new to Python/Jupyter Notebook you will need to find another tutorial to guide you through that, we are not doing introductory stuff here. the code snippets that follow correspond directly to Jupyter Notebook cells.\r\n\r\n## getting started with selenium and python\r\n\r\nNow import all the stuff we are going to need (pip install anything you have missing):\r\n\r\n```python\r\n%matplotlib inline\r\nfrom selenium import webdriver\r\nfrom selenium.webdriver.common.by import By\r\nfrom selenium.webdriver.common.keys import Keys\r\nfrom selenium.webdriver.support.ui import Select\r\nfrom selenium.webdriver.support.ui import WebDriverWait\r\nfrom selenium.common.exceptions import TimeoutException\r\nfrom selenium.webdriver.support import expected_conditions as EC\r\nfrom selenium.common.exceptions import NoSuchElementException\r\nfrom selenium.common.exceptions import NoAlertPresentException\r\nimport sys\r\n\r\nimport unittest, time, re\r\nfrom bs4 import BeautifulSoup as bs\r\nfrom dateutil import parser\r\nimport pandas as pd\r\nimport itertools\r\nimport matplotlib.pyplot as plt\r\n```\r\n\r\nNow you can see we are going to use [Selenium](http://www.seleniumhq.org/) to do the automation. We will use it to automate Firefox so it can go on running in the background while we carry on in our normal browser (I know [well over 60% of you use Chrome](https://netmarketshare.com/browser-market-share.aspx)).\r\n\r\n```python\r\ndriver = webdriver.Firefox()\r\ndriver.base_url = \"https://twitter.com/swyx/following\"\r\ndriver.get(driver.base_url)\r\n```\r\n\r\nSwap out my username for yours. If you run this bit of code, it opens up Firefox to the twitter login page. If you log in with your own credentials, it then goes to your page of follows. The problem with scraping this page is that it is an \"infinite scroll\" page, so just scraping whatever loads on the first view isn't enough. You have to scroll down, wait for it to load, and scroll down again, and again and again until you load ALL your follows. You could try to [get this from the official Twitter API](https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-friends-list) but they only give you 15 requests every 15 minutes. So we scrape.\r\n\r\nOnce you're logged in, you can use the Firefox devtools inspector to look at the HTML tags and attributes that are of interest to you. If you're new to HTML/Devtools, that's ok too, but again I don't have the space to teach that here. Check out [FreeCodeCamp](https://www.freecodecamp.org/), [CodeCademy](https://www.codecademy.com/learn/learn-html) or [MDN](https://developer.mozilla.org/en-US/docs/Web/HTML).\r\n\r\n## a basic infinite scroll strategy\r\n\r\nThe easiest way to automate the infinite scroll is to do something like this:\r\n\r\n```python\r\nfor i in range(1,230):\r\n    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\r\n    time.sleep(2)\r\n    print(i)\r\n```\r\n\r\nI have 4000 follows so I arrived at `range(1,230)` by just doing a few test runs and then calculating how many loops I needed to cover all follows. Since other people will have less or more follows than 4000, we will have to make this a dynamic strategy, and I will cover that below.\r\n\r\nI use `time.sleep(2)` to allow for the page load to happen. This is probably longer than I need based on my high speed connection, but I chose to trade off longer automated execution time for a lower risk of not loading all the data I need. I also `print` my progress just as a way to indicate how far along I am in my process since it can sometimes be hard to tell how close I am to being done. In this case, this only takes about 8 minutes to run but we will be running future stuff for far longer and I wanted to explain the basic intuition.\r\n\r\n## saving the data\r\n\r\n```python\r\nhtml_source = driver.page_source\r\nsourcedata= html_source.encode('utf-8')\r\nsoup=bs(sourcedata)\r\narr = [x.div['data-screen-name'] for x in soup.body.findAll('div', attrs={'data-item-type':'user'})]\r\nbios = [x.p.text for x in soup.body.findAll('div', attrs={'data-item-type':'user'})]\r\nfullnames = [x.text.strip() for x in soup.body.findAll('a', 'fullname')][1:] # avoid your own name\r\nd = {'usernames': arr, 'bios': bios, 'fullnames': fullnames}\r\ndf = pd.DataFrame(data=d)\r\ndf.to_csv('data/BASICDATA.csv')\r\n```\r\n\r\nThis gives you a dataframe `df` that has the usernames, fullnames, and bios of everyone that you follow. Woohoo! You're done! right??\r\n\r\nNope. You're just getting started.\r\n\r\n**We now have to scale up what you just did for one user (you) to ALL your users.**\r\n\r\nSome quick automation math - say everything we just did took 10 minutes to do. 10 minutes x 4000 users = 40,000 minutes = 666 hours = 28 days!!! That's not impossible but is too high to be reasonable. How can we do this in reasonable time?\r\n\r\n# Parallelizing\r\n\r\nThe great thing about this scraping process is they can all happen concurrently. If we had 4000 machines, we could run each on a machine and have all 4000 done in ten minutes. But we don't.\r\n\r\nHow I addressed this is by splitting it up into 8 blocks of 500 users. Thats approximately 1.4 hours to do 28 days of work. Not too bad?\r\n\r\nBy the end of this section you will be doing total black magic with selenium:\r\n\r\n{% twitter 962604465886040064 %}\r\n\r\nSpin up 8 different jupyter notebooks and log in on Twitter on each Firefox instance (see `driver = webdriver.Firefox()` above). Name them clearly so you dont accidentally confuse each notebook.\r\n\r\nNow in each notebook, you can read the data you output from your initial run:\r\n\r\n```python\r\ndf = pd.read_csv('data/BASICDATA.csv', encoding = \"ISO-8859-1\")\r\narr = df.usernames\r\n```\r\n\r\n## a dynamic infinite scroll strategy\r\n\r\ndont execute this code but just to show you how to make the basic infinite scroll strategy above more dynamic:\r\n\r\n```python\r\n    loopCounter = 0\r\n    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\r\n    while True:\r\n        if loopCounter > 499:\r\n            break; # if the account follows a ton of people, its probably a bot, cut it off\r\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\r\n        time.sleep(2)\r\n        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\r\n        if newHeight == lastHeight:\r\n            break\r\n        lastHeight = newHeight\r\n        loopCounter = loopCounter + 1\r\n```\r\n\r\nessentially, store the document height, and if it stops growing after you scroll to the bottom, then conclude you have reached the end (`lastHeight == newHeight`) and break out of the loop.\r\n\r\n## the parallelized code\r\n\r\nand then you set your range appropriately for each notebook. So this book covers user 500 - 999:\r\n\r\n```python\r\nfor i in range(500,1000):\r\n    currentUser = arr[i]\r\n    print('now doing user ' + str(i) + ': ' + currentUser)\r\n    driver.base_url = \"https://twitter.com/\" + currentUser + \"/following\"\r\n    driver.get(driver.base_url)\r\n    time.sleep(3) # first load\r\n    loopCounter = 0\r\n    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\r\n    while True:\r\n        if loopCounter > 499:\r\n            break; # if the account follows a ton of people, its probably a bot, cut it off\r\n        if loopCounter > 0 and loopCounter % 50 == 0:\r\n            print(loopCounter)\r\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\r\n        time.sleep(2)\r\n        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\r\n        if newHeight == lastHeight:\r\n            break\r\n        lastHeight = newHeight\r\n        loopCounter = loopCounter + 1\r\n    print('ended at: ' + str(loopCounter))\r\n    html_source = driver.page_source\r\n    sourcedata = html_source.encode('utf-8')\r\n    soup=bs(sourcedata)\r\n    temparr = [x.div['data-screen-name'] for x in soup.body.findAll('div', attrs={'data-item-type':'user'})]\r\n    tempbios = [x.p.text for x in soup.body.findAll('div', attrs={'data-item-type':'user'})]\r\n    fullnames = [x.text.strip() for x in soup.body.findAll('a', 'fullname')][1:] # avoid your own name\r\n    d = {'usernames': temparr, 'bios': tempbios, 'fullnames': fullnames}\r\n    df = pd.DataFrame(data=d)\r\n    df.to_csv('data/' + currentUser + '.csv')\r\n```\r\n\r\nI want to be very clear what happens when so i err on the excessive site of logging. Every now and then when developing automation like this you will run into an error and you dont want to have to go back and restart hours of automation that ran fine. so the ability to pick up where you crashed is a good thing. (you could also implement better error handling but that would limit your ability to respond when errors happen and _fix future errors_.)\r\n\r\n## Collecting deeper data for first degree follows\r\n\r\nThe first time I did this, the above was all I did, but I soon found I wanted more data for my first-degree follows. So I fired up another notebook. This time I wanted to visit the \"with_replies\" page of each user to grab some data from their timeline. With this I can get some idea of \"engagement\" (total amount of comments, likes, and retweets of original content) and their positivity (sentiment score based on automated parsing of tweets to see if the account is primarily positive or negative).\r\n\r\nDo the same login in firefox process as above, and then read in the raw data:\r\n\r\n```python\r\ndf = pd.read_csv('data/BASICDATA.csv', encoding = \"ISO-8859-1\")\r\narr = df.usernames\r\n\r\n```\r\n\r\nwe are just using this for the list of usernames.\r\n\r\n\r\nthen we initialize the dataframe:\r\n\r\n```python\r\nmain = pd.DataFrame(data = {\r\n        'user': ['swyx'],\r\n        'text': ['text'],\r\n        'tweetTimestamps': ['tweetTimestamps'],\r\n        'engagements': ['engagements'],\r\n        'name': ['name'],\r\n        'loc': ['loc'],\r\n        'url': ['url'],\r\n        'stats_tweets': ['stats_tweets'],\r\n        'stats_following': ['stats_following'],\r\n        'stats_followers': ['stats_followers'],\r\n        'stats_favorites': ['stats_favorites'],\r\n    })\r\n```\r\n\r\nand now we go through each user's profile in the `arr` array:\r\n\r\n```python\r\ndef getTimestamps(x):\r\n    temp = x.findAll('span', '_timestamp')\r\n    if len(temp) > 0:\r\n        return temp[0].get('data-time')\r\n    else:\r\n        return None\r\n# now get the user's own timeline\r\nfor i in range(0,len(arr)):\r\n    currentUser = arr[i]\r\n    print('doing user:' + str(i) + ' ' + currentUser)\r\n    driver.base_url = \"https://twitter.com/\" + currentUser + '/with_replies'\r\n    driver.get(driver.base_url)\r\n    html_source = driver.page_source\r\n    dailyemail_links = html_source.encode('utf-8')\r\n    soup=bs(dailyemail_links, \"lxml\")\r\n    time.sleep(2)\r\n    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\r\n    time.sleep(1)\r\n    # name\r\n    name = soup.find('a', \"ProfileHeaderCard-nameLink\").text\r\n    # loc\r\n    temp = soup.find('span', 'ProfileHeaderCard-locationText')\r\n    temp = temp.text if temp else ''\r\n    loc = temp.strip() if temp else ''\r\n    # url\r\n    temp = soup.find('span', 'ProfileHeaderCard-urlText')\r\n    temp = temp.a if temp else None\r\n    temp2 = temp.get('title') if temp else None\r\n    url = temp2 if temp2 else (temp.get('href') if temp else None)\r\n    # stats\r\n    temp = soup.find('a',{'data-nav': 'tweets'})\r\n    stats_tweets = temp.find('span', 'ProfileNav-value')['data-count'] if temp else 0\r\n    temp = soup.find('a',{'data-nav': 'following'})\r\n    stats_following = temp.find('span', 'ProfileNav-value')['data-count'] if temp else 0\r\n    temp = soup.find('a',{'data-nav': 'followers'})\r\n    stats_followers = temp.find('span', 'ProfileNav-value')['data-count'] if temp else 0\r\n    temp = soup.find('a',{'data-nav': 'favorites'})\r\n    stats_favorites = temp.find('span', 'ProfileNav-value')['data-count'] if temp else 0\r\n    # all text\r\n    text = [''.join(x.findAll(text=True)) for x in soup.body.findAll('p', 'tweet-text')]\r\n    # most recent activity\r\n    alltweets = soup.body.findAll('li', attrs={'data-item-type':'tweet'})\r\n    tweetTimestamps = list(map(getTimestamps, alltweets)) if len(alltweets) > 0 else 0\r\n    # engagements\r\n    noretweets = [x.findAll('span', 'ProfileTweet-actionCount') for x in alltweets if not x.div.get('data-retweet-id')]\r\n    templist = [x.findAll('span', 'ProfileTweet-actionCount') for x in alltweets if not x.div.get('data-retweet-id')]\r\n    templist = [item for sublist in templist for item in sublist]\r\n    engagements = sum([int(x.get('data-tweet-stat-count')) for x in templist if x.get('data-tweet-stat-count')])\r\n    main = pd.concat([main, pd.DataFrame(data = {\r\n        'user': [currentUser],\r\n        'text': [text],\r\n        'mostrecentTimestamp': [tweetTimestamps],\r\n        'engagements': [engagements],\r\n        'name': [name],\r\n        'loc': [loc],\r\n        'url': [url],\r\n        'stats_tweets': [stats_tweets],\r\n        'stats_following': [stats_following],\r\n        'stats_followers': [stats_followers],\r\n        'stats_favorites': [stats_favorites],\r\n    })])\r\n    main.to_csv('data/BASICDATA_profiles.csv')\r\n```\r\n\r\nand now our `main` dataframe has all this more detailed data on each account! it is also exported to the `BASICDATA_profiles.csv` file.\r\n\r\n# ANALYSIS\r\n\r\nWhile all that automation is going on, we can keep going on our main dataset!\r\n\r\nSpin up a new jupyter notebook, this time just for data analysis. Import the usual stuff but this time we will also use Textblob for sentiment analysis, so go ahead and import TextBlob: `from textblob import TextBlob`\r\n\r\nnote that you will also need to download some corpuses for Texblob to work, but the error prompts when you run the below code will guide you to do the download fairly easily (its a one-liner in Anaconda).\r\n\r\nWe can do a bit of feature engineering on the meager data we get out of Twitter. In particular, we can try to:\r\n\r\n- categorize the kind of account (developer, maker, founder, etc)\r\n- guess the gender of the account (based on full name of the user) - people want to follow women in tech\r\n- rate the positivity of the accounts tweets - people want more positivity in their twitter feed.\r\n\r\nThese are all error prone but still worth a try if they can surface a better signal I can use.\r\n\r\n\r\n```python\r\ndf1 = pd.read_csv('data/BASICDATA.csv', encoding = \"ISO-8859-1\")\r\ndf2 = pd.read_csv('data/BASICDATA_profiles.csv', encoding = \"ISO-8859-1\").set_index('user')[1:].drop(['Unnamed: 0'], axis=1).drop(['tweetTimestamps'], axis=1)\r\ndf2['bios'] = df1.set_index('usernames')['bios']\r\narr = df1.usernames\r\njslist = [ 'react', 'webpack', ' js', 'javascript','frontend', 'front-end', 'underscore','entscheidungsproblem', 'meteor']\r\nosslist = [' oss', 'open source','maintainer']\r\ndesignlist = ['css', 'designer', 'designing']\r\ndevlist = [' dev','web dev', 'webdev', 'code', 'coding',  'eng',  'software', 'full-stack', 'fullstack', 'backend', 'devops', 'graphql', 'programming',  'computer', 'scien']\r\nmakerlist = ['entrepreneur', 'hacker', 'maker', 'founder', 'internet', 'web']\r\ndef categorize(x):\r\n    bio = str(x).lower()\r\n    if any(s in bio for s in jslist):\r\n        return 'js'\r\n    elif any(s in bio for s in osslist):\r\n        return 'oss'\r\n    elif any(s in bio for s in designlist):\r\n        return 'design'\r\n    elif any(s in bio for s in devlist):\r\n        return 'dev'\r\n    elif any(s in bio for s in makerlist):\r\n        return 'maker'\r\n    else:\r\n        return ''\r\ndf2['cat'] = list(map(categorize,df2['bios']))\r\ndf2['stats_followers'] = list(map(lambda x: int(x), df2['stats_followers']))\r\ndf2['stats_following'] = list(map(lambda x: int(x), df2['stats_following']))\r\ndf2['stats-ratio'] = df2.apply(lambda x: x['stats_followers']/x['stats_following'] + math.sqrt(x['stats_followers']) if x['stats_following'] > 1 else math.sqrt(x['stats_followers']), axis=1) \r\ndf2['stats-ratio'] = list(map(lambda x: min(200,x), df2['stats-ratio']))\r\ndf2['positivity'] = df2['text'].apply(lambda y: sum([x.sentiment.polarity for x in TextBlob(' '.join(y)).sentences]))\r\ndf2['eng_ratio'] = df2.apply(lambda x: math.log(int(x['engagements']))/math.log(x['stats_followers']) if int(x['engagements']) > 0 and int(x['stats_followers']) > 1 else 0, axis=1)\r\n```\r\n\r\nSo if you check out `df2` you now have a few fields that you can use. The 'cat' field represents our efforts to bucket our follows into distinct groups based on keywords in their bios. To the extent that no one person can really ever be put in one bucket this is a Sisyphean task, but we can try :) (if we were to apply some machine learning to this, a [K nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) method might work here since we can break down the keywords using Textblob)\r\n\r\nHere's how my categories broke out:\r\n\r\n```python\r\nprint(len(df2[df2['cat'] == 'maker'])) # 573\r\nprint(len(df2[df2['cat'] == 'design'])) # 136\r\nprint(len(df2[df2['cat'] == 'oss'])) # 53\r\nprint(len(df2[df2['cat'] == 'js'])) # 355\r\nprint(len(df2[df2['cat'] == 'dev'])) # 758\r\n```\r\n\r\nOk, now we're getting somewhere.\r\n\r\nWe are also engineering a bunch of other metrics, for example the `stats-ratio`, which is the ratio of followers to following plus the square root of followers, subject to a max of 200. This is an arbitrary formula to allow the influence of high influence people, but to limit the influence of superstars.\r\n\r\n`eng_ratio` is Engagement ratio which attempts to do something similar for the engagement (likes, retweets and comments of original content) as a ratio to followers (if you have more followers you naturally probably have more engagement anyway so its best to look at a ratio).\r\n\r\nWe're skipping a lot of work on analysis and feature engineering but that is what I have right now :).\r\n\r\n# DISPLAY\r\n\r\nOk this is actually the toughest bit. If I pull up and merge my fofollower data for the 355 twitter accounts classified as \"js\" devs, I get over 200,000 edges between source and destination:\r\n\r\n```python\r\nimport os.path\r\ndef getData(x):\r\n    fp = 'data/' + x + '.csv'\r\n    if  os.path.isfile(fp):\r\n        temp = pd.read_csv(fp, encoding = \"ISO-8859-1\")[['usernames', 'bios']] \r\n        temp.columns = ['target', 'bios']\r\n        temp['source'] = x\r\n        temp['cat'] = list(map(categorize,temp['bios'])) # categorize the bios of the fofollows\r\n        return temp\r\ntemp = list(map(getData, list(df2[df2['cat'] == 'js'].index)))\r\ncombined = pd.concat(temp) # all target-source relationships originating from 'js'\r\n```\r\n\r\nI can then display data however I choose:\r\n\r\n```python\r\nscreened = combined.groupby(by='target').count().sort_values(by='source', ascending=False)[:50][['bios']]\r\nscreened.columns = ['fofollow_count'] \r\nscreened_with_combined_info = screened\r\nscreened_with_combined_info['bios'] = combined.groupby(by='target').first()[['bios']]\r\nscreened_with_combined_info['cat'] = combined.groupby(by='target').first()[['cat']]\r\n```\r\n\r\nformatting for markdown display...\r\n\r\n```python\r\n\r\ndf = screened_with_combined_info.reset_index()[['target','fofollow_count','cat','bios']]\r\ndf['target'] = df['target'].apply(lambda x: \"[\" + x + \"](https://twitter.com/\" + x + \")\")\r\n# Get column names\r\ncols = df.columns\r\n\r\n# Create a new DataFrame with just the markdown\r\n# strings\r\ndf2 = pd.DataFrame([['---',]*len(cols)], columns=cols)\r\n\r\n#Create a new concatenated DataFrame\r\ndf3 = pd.concat([df2, df])\r\n\r\n#Save as markdown\r\ndf3.to_csv(\"nor.md\", sep=\"|\", index=False)\r\n```\r\n\r\n# The Top 50 JS Dev Twitter Accounts\r\n\r\ntarget|fofollow_count|cat|bios\r\n---|---|---|---\r\n[dan_abramov](https://twitter.com/dan_abramov)|210|js|Working on @reactjs. Co-author of Redux and Create React App. Building tools for humans.\r\n[paul_irish](https://twitter.com/paul_irish)|190|maker|The web is awesome, let's make it even better ? I work on web performance, @____lighthouse & @ChromeDevTools. Big fan of rye whiskey, data and whimsy\r\n[reactjs](https://twitter.com/reactjs)|189|js|React is a declarative, efficient, and flexible JavaScript library for building user interfaces.\r\n[addyosmani](https://twitter.com/addyosmani)|181|dev|Eng. Manager at Google working on @GoogleChrome & Web DevRel ? Creator of TodoMVC, @Yeoman, Material Design Lite, Critical ? Team @workboxjs ??\r\n[sarah_edo](https://twitter.com/sarah_edo)|181|design|Award-winning speaker. Sr. Developer Advocate @Microsoft. @vuejs Core Team, Writer @Real_CSS_Tricks, cofounder @webanimworkshop, work: ?\r\n[rauchg](https://twitter.com/rauchg)|173||@zeithq\r\n[Vjeux](https://twitter.com/Vjeux)|169|js|Frenchy Front-end Engineer at Facebook. Working on React, React Native, Prettier, Yoga, Nuclide and some other cool stuff...\r\n[mjackson](https://twitter.com/mjackson)|158|js|Thriller, founder @ReactTraining, creator @unpkg, organizer @shape_hq, member @LDSchurch  \r\n[kentcdodds](https://twitter.com/kentcdodds)|157|js|Making software development more accessible · Husband, Father, Mormon, Teacher, OSS, GDE, @TC39 · @PayPalEng @eggheadio @FrontendMasters ?\r\n[sebmarkbage](https://twitter.com/sebmarkbage)|157|js|React JS · TC39 · The Facebook · Tweets are personal\r\n[mxstbr](https://twitter.com/mxstbr)|157|js|Cofounder @withspectrum Advisor @EducativeInc  Makes styled-components, react-boilerplate and micro-analytics   Speciality coffee geek,?\r\n[ryanflorence](https://twitter.com/ryanflorence)|156|js|Owner http://Workshop.me  and http://TotalReact.com \r\n[TheLarkInn](https://twitter.com/TheLarkInn)|155|js|Speaker, engineer, #webpack Core Team, Developer Advocate,  Farmer. Views are my own. TPM @Microsoft @MSEdgeDev @EdgeDevTools.?\r\n[jeresig](https://twitter.com/jeresig)|149|js|Creator of @jquery, JavaScript programmer, author, Japanese woodblock nerd (http://ukiyo-e.org ), work at @khanacademy.\r\n[sebmck](https://twitter.com/sebmck)|147|js|Australian  I write JavaScript  Married to @Anagobarreto \r\n[_developit](https://twitter.com/_developit)|145|js|Chrome DevRel at @google. Creator of @preactjs. Do more with less. http://github.com/developit \r\n[linclark](https://twitter.com/linclark)|144|dev|stuffing my head with code and turning it into @codecartoons. also, tinkering with WebAssembly, @ServoDev, and a little @rustlang at @mozilla\r\n[sophiebits](https://twitter.com/sophiebits)|143|js|I like fixing things. eng manager of @reactjs at Facebook. ex-@khanacademy.  she/her. kindness, intersectional feminism, music.\r\n[floydophone](https://twitter.com/floydophone)|143|js|Co-founder & CEO @HelloSmyte. Ex-FB and Instagram. Worked on React.js.\r\n[jlongster](https://twitter.com/jlongster)|142|dev|Contracting as Shift Reset LLC. Working on @actualbudget. Created @PrettierCode. Ex-Mozilla. Enjoys functional programming.\r\n[ken_wheeler](https://twitter.com/ken_wheeler)|141|oss|Director of OSS @FormidableLabs ? Professional American ? Manchild ? Dad ? @baconbrix's Dad  ? All opinions are the opinions of Miller Lite ? @toddmotto fan\r\n[left_pad](https://twitter.com/left_pad)|140||A volunteer in the community  and a steward of @babeljs. @Behance, @Adobe. Soli Deo Gloria\r\n[acdlite](https://twitter.com/acdlite)|140|js|@reactjs core at Facebook. Hi!\r\n[nodejs](https://twitter.com/nodejs)|137|js|The Node.js JavaScript Runtime\r\n[jordwalke](https://twitter.com/jordwalke)|135|js|Maker of things: ReactJS. Working on: @reasonml. At: Facebook Engineering.\r\n[github](https://twitter.com/github)|132|dev|\"How people build software. Need help? Send us a message at http://git.io/c  for support.\"\r\n[leeb](https://twitter.com/leeb)|132|js|Making things at Facebook since 2008: React, GraphQL, Immutable.js, Mobile, JavaScript, Nonsense\r\n[BrendanEich](https://twitter.com/BrendanEich)|130|js|Created JavaScript. Co-founded Mozilla and Firefox. Now founder & CEO @Brave Software (https://brave.com/ ).\r\n[cpojer](https://twitter.com/cpojer)|129|dev|Formerly Pojer ·  Engineering Manager at Facebook ·  Metro ·  Jest ·  Yarn\r\n[rauschma](https://twitter.com/rauschma)|128|js|\"JavaScript: blog @2ality, books @ExploringJS, training, newsletter @ESnextNews. ReasonML: tweets @reasonmlhub, newsletter ?\"\r\n[wesbos](https://twitter.com/wesbos)|125|js|Fullstack Dev ? JS CSS Node ? https://ES6.io  ? https://LearnNode.com  ? http://ReactForBeginners.com  ? http://JavaScript30.com  ?  Tips ? @KaitBos ? @SyntaxFM\r\n[wycats](https://twitter.com/wycats)|125|oss|Tilde Co-Founder, OSS enthusiast and world traveler.\r\n[BenLesh](https://twitter.com/BenLesh)|121|dev|Software engineer at @Google, #RxJS core team. Occasionally I act silly on the @moderndotweb podcast. Views are my own.\r\n[sindresorhus](https://twitter.com/sindresorhus)|120|oss|Maker of things; macOS apps & CLI tools. Currently into Swift and Node.js. Full-time open sourcerer. Started @AVA__js.\r\n[tjholowaychuk](https://twitter.com/tjholowaychuk)|119|dev|Founder & solo developer of https://apex.sh , not a startup. https://github.com/tj  https://medium.com/@tjholowaychuk . Asya's.\r\n[Una](https://twitter.com/Una)|118|dev|Director of Product Design @bustle, Google Dev Expert, & cohost @toolsday. Prev UI Eng @digitalocean @ibmdesign. Travel life: http://Instagram.com/unakravets \r\n[peggyrayzis](https://twitter.com/peggyrayzis)|117|oss|Exploring the world through code, travel, and music  Open Source Engineer @apollographql\r\n[elonmusk](https://twitter.com/elonmusk)|117||\r\n[jaffathecake](https://twitter.com/jaffathecake)|115|maker|Googler. I want the web to do what native does best, and fast. No thoughts go unpublished. 'IMO' implicit.\r\n[youyuxi](https://twitter.com/youyuxi)|115|js|Design, code & things in between. Full-time open source. Creator @vuejs, previously @meteorjs & @google, @parsonsamt alumnus.\r\n[jdalton](https://twitter.com/jdalton)|113|js|JavaScript tinkerer, bug fixer, & benchmark runner ? Creator of Lodash ? Former Chakra Perf PM ? Current Web Apps & Frameworks PM @Microsoft.\r\n[samccone](https://twitter.com/samccone)|113||harbourmaster @google\r\n[markdalgleish](https://twitter.com/markdalgleish)|113|design| CSS Modules co-creator, @MelbJS organiser. Full-stack ECMAScript addict, UI design enthusiast, coffee drinker  DesignOps Lead at @seekjobs \r\n[thejameskyle](https://twitter.com/thejameskyle)|112||\r\n[tomdale](https://twitter.com/tomdale)|112|js|JavaScript thinkfluencer\r\n[_chenglou](https://twitter.com/_chenglou)|109|js|There's an underscore before my name\r\n[mathias](https://twitter.com/mathias)|107|js|I work on @v8js at Google and on ECMAScript through TC39.  JavaScript, HTML, CSS, HTTP, performance, security, Bash, Unicode, i18n, macOS.\r\n[iam_preethi](https://twitter.com/iam_preethi)|106|dev|Blockchain Engineer. Building a new company (Schelling). Alum @coinbase @a16z @GoldmanSachs. Passionate about blockchain & crypto. Avid?\r\n[threepointone](https://twitter.com/threepointone)|106|js|Entscheidungsproblem\r\n[JavaScriptDaily](https://twitter.com/JavaScriptDaily)|105|js|Daily JavaScript / JS community news, links and events. Go to @reactdaily for React news.\r\n\r\n\r\nThese are the top 50 JS devs followed by other devs! Whoo! not a bad place to get after 4100 words, eh?\r\n\r\n\r\nI of course have much more data analysis to do but I will put up the results in a separate post, with more engagement and follow ratio data split by gender, location, and so on. Shameless plug time: [follow me](https://twitter.com/swyx) if you want to get notified when I put that out!\r\n\r\nWhat else can you do with your data? Post it up somewhere and I'd love to tweet it out!\r\n\r\n\r\n---\r\n\r\n_This is the first post in a possible series on applying data science to scraping and analysing my Twitter network graph so I might follow up with more on the finer details about doing this. Please let me know your feedback in the comments so I can incorporate it in future post._"
  },
  {
    "slug": "what-happens-when-a-user-edits-a-post-on-devto-a58",
    "data": {
      "title": "What Happens When A User Edits A Post on Dev.to?",
      "description": "I am catching up on Ben's posts on web perf and I still don't quite get what happens when a post is e...",
      "tag_list": [
        "discuss"
      ]
    },
    "content": "\r\nI am catching up on [Ben's posts on web perf](https://dev.to/ben/making-devto-insanely-fast) and I still don't quite get what happens when a post is edited. I understand that things are cached but doesn't that mean that if a post is edited it will take an unpredictable amount of time to update to CDNs all around the world? What are the best practices around this? Does it cost anything to do this? These are the kinds of questions I have when wondering about what I would run into if I were to make a dev.to clone."
  },
  {
    "slug": "hello-i-am-making-a-modern-fullstack-js-tutorial-what-questions-do-you-have-for-me-dg3",
    "data": {
      "title": "Hello, I am making a modern fullstack JS tutorial livestream. What questions do you have for me?",
      "description": "When I first started out in FreeCodeCamp, I found ClementineJS to be very helpful in integrating sepa...",
      "tag_list": [
        "discuss"
      ]
    },
    "content": "\r\nWhen I first started out in FreeCodeCamp, I found [ClementineJS](http://www.clementinejs.com/) to be very helpful in integrating separate bits of frontend and backend knowledge. However it is more than 2 years out of date and a lot of the code looks very janky now. \r\n\r\nI wanted to make an update to ClementineJS that focuses on different choices, eg using Create-React-App instead of writing the frontend without a framework, and using Mongoose instead of the raw mongodb interface. I also think there are a bunch of other smaller pain points around making a fullstack app including authentication, frontend routing, and styling.\r\n\r\nPrerequisite knowledge:\r\n- HTML/CSS/JS (including ES6)\r\n- Basic React\r\n- Very Basic NodeJS/Express\r\n- (optional) Webpack, npm\r\n\r\nI'm tentatively calling this SunburstJS and the format is basically me remotely teaching my partner John how to make [this repo](https://github.com/sw-yx/sunburstjs) from scratch in a series of live stream videos.\r\n\r\nMy purpose is simply teaching-to-learn and giving back to the community.\r\n\r\nI wanted to include the wonderful dev.to community so um here I am (actually not sure if this is the right place but whatev). Does this sound like it might help? what do you want included? do you want to participate? lmk."
  },
  {
    "slug": "how-to-google-your-errors-2l6o",
    "data": {
      "title": "How To Google Your Errors",
      "description": "A friendly guide for newbie developers on searching for solutions to an error",
      "tag_list": [
        "errors",
        "googling"
      ]
    },
    "content": "\r\nSo you're a newbie trying out a new technology, minding your own business when all of a sudden:\r\n\r\n```bash\r\nUncaught (in promise) TypeError: _super.call is not a function\r\n    at new ObservableQuery (b1f6a7d9f98d979758232d0dc3c394ce.js:26213)\r\n    at QueryManager.watchQuery (b1f6a7d9f98d979758232d0dc3c394ce.js:27305)\r\n    at b1f6a7d9f98d979758232d0dc3c394ce.js:27332\r\n    at new Promise (<anonymous>)\r\n    at QueryManager.query (b1f6a7d9f98d979758232d0dc3c394ce.js:27330)\r\n    at ApolloClient.query (b1f6a7d9f98d979758232d0dc3c394ce.js:27981)\r\n    at Object.require.2.react (b1f6a7d9f98d979758232d0dc3c394ce.js:29740)\r\n    at newRequire (b1f6a7d9f98d979758232d0dc3c394ce.js:41)\r\n    at require.39 (b1f6a7d9f98d979758232d0dc3c394ce.js:66)\r\n    at b1f6a7d9f98d979758232d0dc3c394ce.js:71\r\n```\r\n\r\nYou copy and paste the whole thing into Google, and a mere second later:\r\n\r\n```\r\nYour search - Uncaught (in promise) TypeError: _super.call is not a function - did not match any documents.\r\n\r\nSuggestions:\r\n\r\n    Make sure that all words are spelled correctly.\r\n    Try different keywords.\r\n    Try more general keywords.\r\n    Try fewer keywords.\r\n```\r\n\r\nOr perhaps worse, you get tons of results, but none of them lead anywhere useful!\r\n\r\nWhat now?\r\n\r\n---\r\n\r\nThis was the question [posed today by Brittany Storoz](https://twitter.com/brittanystoroz/status/948671247734407169). As a recent Javascript newbie myself I am pretty familiar with this. Javascript is not a typesafe language so it is particularly prone to cryptic errors. Over the past year I have done my fair share of Googling The Error with various degrees of success, so I am going to lay down some thoughts here (as well as combining the thoughts of others). Here goes!\r\n\r\n![Alt text of image](https://geekifyinc.com/wp-content/uploads/2017/09/IMG_0333-1280.jpg)\r\n\r\n## 1. Don't Panic\r\n\r\n[Don't Panic](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Don't_Panic)! 😂 Googling Errors is a rite of passage. In fact, treat it as a welcome opportunity to _practice_ researching errors because this is a key skill in your career. The process will likely turn up other bits of knowledge you didn't know you were missing!\r\n\r\nNow that you're calm: **READ THE ERROR**. Quote Mark Erikson:\r\n\r\n{% twitter 948036804975955968 %}\r\n\r\nBelieve it or not, someone somewhere put effort into writing that error you're reading. Does it make sense or are you just glazing over it in blind panic?\r\n\r\n## 2. Rubber Duck It\r\n\r\n[Rubber Duck Debugging](https://en.wikipedia.org/wiki/Rubber_duck_debugging) is a time-tested method of software engineering. Basically, try to explain to an inanimate object (preferably cute, squeakiness optional) in your own words what you are trying to do, and what the error is. Your natural language description of the problem is likely how others are also going to describe it, so plug that into Google and see what happens. You might be surprised! 🦆\r\n\r\n## 3. Go in Expanding Circles (remove irrelevant info)\r\n\r\nYour error likely has a lot of App-specific info in it. For example:\r\n\r\n```bash\r\n$ node_modules/.bin/parcel watch app/client/entry.html --out-dir public/dist\r\n🚨  Cannot read property 'type' of undefined\r\n    at Bundler.createBundleTree (/home/ben/projects/dg/node_modules/parcel-bundler/src/Bundler.js:373:52) \r\n    at Bundler.createBundleTree (/home/ben/projects/dg/node_modules/parcel-bundler/src/Bundler.js:412:12) \r\n    at Bundler.createBundleTree (/home/ben/projects/dg/node_modules/parcel-bundler/src/Bundler.js:412:12) \r\n    at Bundler.buildQueuedAssets (/home/ben/projects/dg/node_modules/parcel-bundler/src/Bundler.js:245:23)                                                                                                           \r\n    at <anonymous>                                   \r\n    at process._tickCallback (internal/process/next_tick.js:188:7) \r\n```\r\n\r\nPlugging all that into Google isn't going to help! Why? Because you've left a bunch of assumptions in there that only you are going to use, for example `app/client/entry.html` and `/home/ben/projects/dg`. So removing them and decreasing the specificity of your search is likely to improve the search results to find other people who had similar issues to you.\r\n\r\nJames Roe phrased this better than I can:\r\n\r\n{% twitter 948706229785739264 %}\r\n\r\nSo if you have an error code, Google that. If that doesn't work, Google the error message. If that doesn't work, Google the library you're using. and so on!\r\n\r\n## 4. Give more Context (add relevant keywords)\r\n\r\nErrors are also notable for what they _leave out_. In this case, there is an implicit assumption that you, the developer, know what language or library you are using. But there's no way for Google to know that! Help Google along by adding the tech stack you are using as keywords, for example, `parceljs Cannot read property 'type' of undefined`. Remember your end goal is to hopefully land on a Github Issue, Stackoverflow question, or blog post so give Google all the context you can to help it surface the answer you need!\r\n\r\nIncidentally, **version numbers** are a big part of context too. If you are experiencing an error on D3.js v4, then answers from D3.js v3 won't be very helpful! If you don't have version numbers or think your error is more generic, throwing a date limit on your query (Google lets you restrict it to the past 1 year) is likely to turn up more recent results which are likely to be more relevant.\r\n\r\n## 5. Use advanced search operators\r\n\r\nGoogle's searchbox packs a lot of power if you know how to wield it. Check out [this cheatsheet](https://support.google.com/websearch/answer/2466433?hl=en) (or [others](https://www.lifewire.com/advanced-google-search-3482174) like [this](https://bynd.com/news-ideas/google-advanced-search-comprehensive-list-google-search-operators/)) for advanced search tips to search only for quoted content, or for all search words, so on and so forth. (Thanks [Guinivere Saenger](https://twitter.com/guincodes/status/948740376705187840)). Special bonus, this increases your Google-fu for your non-dev life too!\r\n\r\n## 6. Don't Google!\r\n\r\nGoogle isn't the only search engine out there. There are plenty of venues where people go for help - like Stackoverflow and Github - and they have plenty fine search functionality too!\r\n\r\nGoogle is also not the most dev-friendly search engine. Quote [Quincy Larson](https://www.quora.com/How-do-I-master-the-art-of-Googling-as-a-programmer-I%E2%80%99m-currently-learning-bash-and-the-IT-seems-like-too-much-memorisation/answer/Quincy-Larson):\r\n\r\n*\"Another thing a lot of new programmers don’t realize is that Google omits most non-alphanumeric characters from its queries. Symbols that programmers use all the time like !@#$%^& and * aren’t searched. Neither are (){}[].\"*\r\n\r\nSo if you have a lot of symbols in your search, use [DuckDuckGo](https://duckduckgo.com/)!\r\n\r\n## 7. Think about your biggest unknown\r\n\r\nProgramming works in terms of layers of abstraction. At the biggest of the big picture level there is the [OSI model](https://en.wikipedia.org/wiki/OSI_model), but if you are an app developer then your layers can be something like:\r\n\r\n- Language\r\n- Environment\r\n- Framework\r\n- Library\r\n- App\r\n\r\nWhat do I mean by biggest unknown? Well, if you are new to a **language**, then when you have errors you are most likely making a simple syntactical error because you are not yet familiar with all the grammar and edge cases a new language can bring. \r\n\r\nConfident it's not a language problem? Then proceed to the **environment**. Languages are evolving specifications and the environment in which you are coding matters a LOT as to how you can actually use the language. As a Javascript developer I always see [people getting tripped up by whether they can or cannot use ES6 syntax](https://forum.freecodecamp.org/t/changing-js-version/166453/3). So they get errors that they then Google and get totally confused by.\r\n\r\nYou've ruled out environment as the cause? Proceed to **framework**. So on and so forth. You can even proceed in reverse order if that suits you. Whatever works.\r\n\r\nThis step isn't meant to take long, I just erred verbose to emphasize what I mean by \"Think about your biggest unknown\". Your biggest unknown is your biggest source of risk, and therefore the first (but not only!) place to look when you have errors.\r\n\r\n## 8. Read the Docs\r\n\r\nIf your error has to do with a specific Framework or Library, it is quite possible that there is some concept or language you may not know about that is the hidden cause of your error. Go read the docs, and look at the examples and understand how you might differ from that. Pay attention to the docs' **specific choice of words** and add that as keywords to your Google search to see if it surfaces any better results.\r\n\r\n## 9. Reproduce the Error\r\n\r\nStart a whole new project and make it very small so that you can isolate your Error. Copy over the bare minimum from your existing project that you will reproduce the Error, or just try to code it up from scratch without all the extra fluff your main project has. \r\n\r\nIf you cannot reproduce the Error, you will have found a HUGE clue as to what is going on with your error. \r\n\r\nIf you CAN reproduce the Error, that's also great, because that sets you up for...\r\n\r\n\r\n## 10. Asking for Help\r\n\r\nPost your error EVERYWHERE. Github, Stackoverflow, Reddit, Twitter, Slack/Discord communities, [Dev.to](https://dev.to/t/help) (ahem!), you name it. \r\n\r\nYour minimally reproducible sample, if you have it from step 9, will help go a long way for people to figure out what is going on. \r\n\r\nFurthermore, it will help FUTURE people who have YOUR exact error be able to Google to find YOU. If we are ever going to have Googlable error solutions, someone has to start the ball rolling!\r\n\r\n\r\n# More Resources\r\n\r\n- Michael Hoffman's presentation on [How To Be Stuck](http://code-worrier.com/how-to-be-stuck)\r\n- Qunicy Larson's answer on [How do I master the art of Googling as a programmer?](https://www.quora.com/How-do-I-master-the-art-of-Googling-as-a-programmer-I%E2%80%99m-currently-learning-bash-and-the-IT-seems-like-too-much-memorisation/answer/Quincy-Larson)\r\n\r\nGood luck!"
  },
  {
    "slug": "flutter-for-react-native-devs-in-30-seconds-78g",
    "data": {
      "title": "Flutter for React Native Devs in 30 Seconds",
      "description": "You may have heard of Flutter, Google's answer to React Native. What should you know?   It's in the D...",
      "tag_list": [
        "inthirtyseconds",
        "flutter",
        "reactnative"
      ]
    },
    "content": "\nYou may have heard of [Flutter](https://flutter.io), Google's answer to React Native. What should you know?\n\n- It's in the [Dart](https://www.dartlang.org/) language, which borrows heavily from Java. But Javascript fans will find it very easy to read (see below)\n- [IntelliJ or Android Studio](https://flutter.io/ide-setup/) are the recommended IDEs with Flutter plugins.\n- Unlike RN, Flutter doesn't use a Javascript bridge, it compiles straight to native iOS/Android files. (But in \"slow mode\" development, it operates as an interpreted language so you can still do [hot reloading](https://flutter.io/hot-reload/))\n- Unlike RN, Flutter comes \"batteries included\", with opinions on **[routing](https://flutter.io/routing-and-navigation/)**, **[animations](https://flutter.io/animations/)**, **[i18n](https://flutter.io/tutorials/internationalization/)** and **themes**!!! In particular it comes with a bunch of inbuilt [widget styles](https://flutter.io/widgets/) where you can use Material Design or \"Cupertino\" (aka Apple \"inspired\") designs right out of the box. Also, did I mention **routing?!?!?!** Here, let me do it justice:\n\n# FLUTTER COMES WITH ROUTING!!!1!!!\n\n- The animations are seriously good. Can you [do this](https://flutter.io/animations/hero-animations/) in React Native?\n\n{% youtube CEcFnqRDfgw %}\n\n- Like RN, Flutter uses a lot of familiar paradigms including `class` extensions, `setState` and event handlers. If you can read RN, you can read Flutter:\n\n```\nclass _MyHomePageState extends State<MyHomePage> {\n  int _counter = 0;\n\n  void _incrementCounter() {\n    setState(() {\n      _counter++;\n    });\n  }\n\n  @override\n  Widget build(BuildContext context) {\n    return new Scaffold(\n      appBar: new AppBar(\n        title: new Text(widget.title),\n      ),\n      body: new Center(\n        child: new Column(\n          mainAxisAlignment: MainAxisAlignment.center,\n          children: <Widget>[\n            new Text(\n              'You have pushed the button this many times:',\n            ),\n            new Text(\n              '$_counter',\n              style: Theme.of(context).textTheme.display1,\n            ),\n          ],\n        ),\n      ),\n      floatingActionButton: new FloatingActionButton(\n        onPressed: _incrementCounter,\n        tooltip: 'Increment',\n        child: new Icon(Icons.add),\n      ), // This trailing comma makes auto-formatting nicer for build methods.\n    );\n  }\n}\n```\n- The install experience is very smooth:\n\n```bash\ngit clone -b alpha https://github.com/flutter/flutter.git\nexport PATH=`pwd`/flutter/bin:$PATH\nflutter doctor\n```\n\nGive it a shot! <http://flutter.io> and the Google Codelab tutorial is [here](https://codelabs.developers.google.com/codelabs/flutter). or watch the Google I/O video!\n\n\n{% youtube w2TcYP8qiRI %}"
  },
  {
    "slug": "serverless-machine-learning-at-google-cp9",
    "data": {
      "title": "Serverless Machine Learning at Google",
      "description": "Serverless Machine Learning at Google",
      "tag_list": [
        "serverless",
        "machinelearning"
      ]
    },
    "content": "\r\n### Google can tell dogs from mops. Can you?\r\n\r\n![dog or mop](http://cin.h-cdn.co/assets/16/14/1460113736-screen-shot-2016-04-08-at-43127-pm.png)\r\n\r\nBret McGowen presented on Serverless machine learning at Google. You can watch his full talk [here](https://www.youtube.com/watch?v=Fu8Sdh_wkQM) but here are my notes.\r\n\r\n# Serverless\r\n\r\nFour principles:\r\n- no need to manage/think about servers\r\n- no upfront provisioning, scale as you go (can't be wrong about having enough capacity)\r\n- pay per use\r\n- stateless/ephemeral\r\n\r\nServerless at Google:\r\n- Background functions: Cloud Storage, Cloud Pub/Sub\r\n- HTTP functions: API, Webhooks, Browser\r\n\r\n# Machine Learning\r\n\r\nMachine learning is using many **examples** to **answer questions**.\r\n\r\nMachine Learning at Google:\r\n- Use your own data: TensorFlow and Cloud Machine Learning Engine\r\n- Pretrained ML models: Cloud (Vision, Speech, Natural Language, Translation) API, Cloud Video Intelligence\r\n\r\nSpecifics on capabilities of Cloud Vision API: \r\n- Label detection ([dog or mop](http://www.cosmopolitan.in/life/news/a5821/is-it-a-dog-or-mop-kitten-or-ice-cream-these-photographs-will-definitely-confuse-you/)?)\r\n- Face detection (within the photo, here is the location of the face)\r\n- OCR (read text from photos)\r\n- Explicit content detection (violence/adult)\r\n- Landmark detection (that's the Eiffel tower!)\r\n- Local Detection (not sure)\r\n\r\nOther Cloud Vision features:\r\n- crop hints - suggested crop dimensions\r\n- web annotations - suggested other metadata to search about your page - eg from a photo of an iconic car, it can tell you the model of car, what film it was from, where it probably is. And can give you other matching images to back it up.\r\n\r\n### Cloud event trigger walkthrough\r\n\r\nCloud storage -> Cloud Functions -> Cloud vision API\r\n\r\nNLP: extract entities from a sentence, sentiment analysis, syntax analysis (parse sentence to a lemma so you can see the parts of speech dependency graph)\r\n\r\n### Speech API\r\n\r\nSpeech to text transcription in 110 languages.\r\n\r\nAzar - uses cloud speech api and cloud translation api to talk\r\nAlso gives timestamp of each word on top of transcript.\r\n\r\n### Video Intelligence API\r\n\r\nLook through the whole video to label things.\r\n\r\n### Links\r\n\r\n- <http://github.com/bretmcg/functions-resize>\r\n- <http://cloud.google.com>\r\n"
  },
  {
    "slug": "are-we-human-or-are-we-recaptcha-blm",
    "data": {
      "title": "Are we human? Or are we reCAPTCHA?",
      "description": "\"The 2 squiggly word captcha that you know and hate will die by 3/31/2018.\"   The Web is dar...",
      "tag_list": [
        "todayilearned",
        "recaptcha",
        "machinelearning"
      ]
    },
    "content": "\r\n# \"The 2 squiggly word captcha that you know and hate will die by 3/31/2018.\"\r\n\r\nThe Web is dark and full of bots, and there is one undisputed leader in defending against them. You probably use reCAPTCHA every day but you don't even know it! [Aaron Malenfant](https://www.linkedin.com/in/aaronmalenfant/) is the lead software engineer for [reCAPTCHA](https://www.google.com/recaptcha) and he explained its past, present, and future at GDG DevFest NYC. reCAPTCHA is secretive by its very nature, so it is a rare look into how this essential piece of web technology works.\r\n\r\n# Part 1: High level details\r\n\r\n## What I Learned\r\n\r\nYou can sign up for reCAPTCHA at <https://www.google.com/recaptcha> and learn more with the CodeLab [here](http://g.co/recaptcha/codelab).\r\n\r\n### Volume\r\n\r\nReCAPTCHA\r\n- 2 million weekly active sites\r\n- 1 billion CAPTCHA solutions a week\r\n- Nocaptcha saves millions of hours a day\r\n\r\n### Difficulty levels\r\n\r\nThe reCAPTCHA Machine learning engine categorizes incoming requests on a spectrum of difficulty levels from \"just a checkbox\" to \"select all images with cars\" (image classification) to \"select all squares with vehicles\" (image localization) to \"ok you're definitely a bot\".\r\n\r\n### Integrating into -your- site\r\n\r\nHead to <https://www.google.com/recaptcha/admin#list> and answer a few simple questions!\r\n\r\nYou will have a few options:\r\n\r\n- Visible: Script tag and a div\r\n- Invisible: script tag and a button with a callback\r\n- Invisible: script tag with a div to have control when you execute\r\n\r\nYes, there is such a thing as Invisible reCAPTCHA! more below. Also look up more docs at the [DevGuide](http://g.co/recaptcha/devguide).\r\n\r\nDon't forget to integrate with serverside\r\n\r\n- make HTTP POST to <www.google.com/recaptcha/api/siteverify> with POST params of `secret` and `response` you get from reCAPTCHA\r\n\r\n# Part 2: Past, present and future\r\n\r\n### RIP 2 word Captcha (reCAPTCHA v1)\r\n\r\n**The 2 word captcha that you know and hate will die by 3/31/2018.** ([Source](https://www.programmableweb.com/news/google-recaptcha-v1-api-shutting-down-march-2018/brief/2017/10/24) and on the [FAQ](https://developers.google.com/recaptcha/docs/faq#what-happens-to-recaptcha-v1))\r\n\r\nAI has advanced to the point that [it can solve the hardest CAPTCHAs at 99.8% accuracy, but humans can only solve them 33% of the time](http://www.zdnet.com/article/google-algorithm-busts-captcha-with-99-8-percent-accuracy/). So it is time to put it to bed.\r\n\r\n### [reCAPTCHA v2](https://developers.google.com/recaptcha/docs/display)\r\n\r\nthe \"i am a human\" checkbox you've clicked dozens of times - this is actually called the \"[NoCAPTCHA](https://www.theverge.com/2014/12/3/7325925/google-is-killing-captcha-as-we-know-it)\" - for more details, see implementation options in Part 1 above.\r\n\r\n### [Invisible reCAPTCHA](https://developers.google.com/recaptcha/docs/invisible) - launched on 3/8/2017\r\n\r\nFor low risk traffic, no user interaction is required at all to detect if you are a bot!\r\n\r\n### reCAPTCHA Android API\r\n\r\nIncluded as part of [Google Play Services SafetyNet](https://developer.android.com/training/safetynet/recaptcha.html) - again, no user interaction required to verify you are human.\r\n\r\n### Future of reCAPTCHA (v3)\r\n\r\nv3 is in Closed Beta now:\r\n- puts you in control of when we show a challenge\r\n- integration siilar to V2 Invisible\r\n- In admin console, get a view into the riskiness of your traffic\r\n\r\nSignup for reCAPTCHA v3 beta announcements at <http://g.co/recaptcha/v3>!"
  },
  {
    "slug": "firebase-analytics-in-30-seconds-6pp",
    "data": {
      "title": "Firebase Analytics in 30 Seconds",
      "description": "Stacy Devino gave an excellent talk on Firebase Analytics today (slides here, github here, youtube in...",
      "tag_list": [
        "inthirtyseconds",
        "firebase",
        "analytics"
      ]
    },
    "content": "\r\n[Stacy Devino](https://twitter.com/doesitpew) gave an excellent talk on Firebase Analytics today ([slides here](https://docs.google.com/presentation/d/1akpEegWCXHkMjU-GBS-3yZ1wHvO7YATU_zHP0kuqDhU/edit#slide=id.p), [github here](https://github.com/childofthehorn/AndroidFirebaseAnalyticsWorkshop), [youtube intro here](https://www.youtube.com/watch?v=8iZpH7O6zXo)). There were a lot of aha moments:\r\n\r\n1. Integrating the Firebase SDK ([Android](https://firebase.google.com/docs/android/setup) and [iOS](https://firebase.google.com/docs/ios/setup) and [web](https://firebase.google.com/docs/web/setup)) is just a few lines of code!\r\n\r\nThere are **pre-configured** events (send standard stuff like login attempts):\r\n\r\n```\r\nBundle bundle = new Bundle();\r\nbundle.putString(FirebaseAnalytics.Param.ITEM_NAME, name);\r\nbundle.putString(FirebaseAnalytics.Param.CONTENT_TYPE, \"image\");\r\nmFirebaseAnalytics.logEvent(\r\n    FirebaseAnalytics.Event.SELECT_CONTENT, bundle);\r\n```\r\n\r\nand **custom** events (send whatever you want)\r\n\r\n```\r\nBundle bundle = new Bundle();\r\nbundle.putString(\"image_name\", name);\r\nmFirebaseAnalytics.logEvent(\"profile_image_select\", bundle);\r\n```\r\n\r\n2. User grouping (so you can study how different groups of your users behave differently) is similarly just a few lines:\r\n\r\n```\r\nmFirebaseAnalytics = FirebaseAnalytics.getInstance(this);\r\nmFirebaseAnalytics.setUserProperty(\"preferred_pet\", \r\n    petSelector);\r\nmFirebaseAnalytics.setUserId(\"userIdString\");\r\n```\r\n\r\n3. Once your app is out in the wild, head to the [Dashboard](https://console.firebase.google.com) to see insights!\r\n\r\n4. Within the Dashboard, [Streamview](https://support.google.com/firebase/answer/7229836?hl=en) is particularly awesome as you can see the \"clickstream\" of user actions in sequence and visualize what they are doing (or failing to do) on your app!\r\n\r\n5. Everything above is completely free and unlimited for you to use. To do -really big- number crunching, you'll have to export the data outside of [Firebase Analytics to Google BigQuery](https://cloud.google.com/solutions/mobile/mobile-firebase-analytics-big-query). There you will be able to query all your historical user data with plain SQL!\r\n\r\nBigQuery is a freemium product so you'll have to have a credit card attached but the financial risk is low for most use cases: \"[Realistically, most people wouldn’t burn through the free GCP $ in the first year. 20k users and normal use <$5/month](https://youtu.be/Ki_F6VCOtXU?t=86)\".\r\n\r\nHaving messed around with a few analytics products in my time as a product manager this looks extraordinarily easy to setup and get insights on, and I am looking forward to deploying this in ALL my [Firebase hosted](https://firebase.google.com/docs/hosting/) and [serverless function](https://firebase.google.com/docs/functions/) projects!"
  },
  {
    "slug": "medical-machine-learning-in-30-seconds-dh1",
    "data": {
      "title": "Medical Machine Learning in 30 Seconds",
      "description": "Today at DevFestNYC, Josh Gordon explained how Tensorflow is being used to detect cancer and diabetes...",
      "tag_list": [
        "inthirtyseconds",
        "machinelearning",
        "tensorflow",
        "keras"
      ]
    },
    "content": "\r\nToday at [DevFestNYC](https://devfestnyc.com/), [Josh Gordon](https://twitter.com/random_forests) explained how Tensorflow is being used to [detect cancer](https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html) and [diabetes](https://research.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html). What's amazing is that it is not much different from training a neural network for recognizing cats and dogs! Here is a brief summary of the talk:\r\n\r\n1. **Data** - Get a lot of data. You need millions of images of whatever you are training on.\r\n2. **MORE Data** - no really, you need more. Sometimes your data can be bad, for example if your doctor panel disagrees with each other, or worse still, disagrees with themselves. (humans, who needs them?)\r\n3. **Setup** - Tensorflow now comes with Keras (_awesome_), and Keras has inbuilt [applications](https://keras.io/applications/) of which *InceptionV3* is pretty good (_awesomer_), although Josh also shouted out to NasNet [(a type of AutoML)](https://research.googleblog.com/2017/11/automl-for-large-scale-image.html) as a neural network that trains itself (_galactic brain exploding_).\r\n4. **Fuzz** - Here is the art form and area of active research. Josh explained some key ideas, from using a [sliding window](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721) to applying rotation/contrast/other filters to milk all you can out of the image data so you can, for example, recognize the same thing it is trained to recognize, even if it was flipped upside down. The most interesting part here was how they looked at how real doctors look at slides in the microscope, zooming in and out to get different contexts, and achieved amazing results by replicating that behavior simply by copying and pasting their code 4 times and running their model at 4 different zoom levels on the same dataset!!\r\n5. **Train** - this takes on the order of 2 days (Google Cloud) to a week (local machine with 10 GPUs). If you are a researcher, Google offers 1000 TPUs FOR FREE to you to use if [you apply here](https://www.tensorflow.org/tfrc/).\r\n6. **Deploy** - This is actually the hardest thing, which is making your models useful for regular nontechnical people to use. \r\n\r\nThe goal is for Machine Learning should be so routine it is boring. Hopefully I've made that boringness interesting!\r\n\r\n"
  }
]